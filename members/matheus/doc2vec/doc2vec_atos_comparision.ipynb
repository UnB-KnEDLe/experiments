{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doc2vec_atos_comparision.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gu_f57fMogWN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd1LFgD4ogWB"
      },
      "source": [
        "# Experimento Doc2Vec - TFIDF - Embeddings\n",
        "\n",
        "O objetivo aqui é usar doc2vec para classificação multi-classe no mini-dataset de atos/classes gerado após teste no ezTag. O resultado será comparado com representações textuais TFIDF e embeddings diversas; todos os casos serão testados com SVM e Logistic Regression.\n",
        "\n",
        "Fontes principais:\n",
        "- https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d\n",
        "- https://github.com/UnB-KnEDLe/experiments/blob/master/members/matheus/BioC_XML_to_conll.ipynb\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "- http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_BLPEigFSz"
      },
      "source": [
        "Os testes foram feitos no Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJIpZo1lIdy",
        "outputId": "2f4ea0b6-0e23-4570-b55c-e457c0c7decc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXewSf15ogWE"
      },
      "source": [
        "### Import de packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMXkj4FnogWG"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import nltk\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_f57fMogWN"
      },
      "source": [
        "## Mini-Dataset de atos\n",
        "- Instanciação em memória\n",
        "- Retirada de classes com apenas 1 instância"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueMgCzGPogWN",
        "outputId": "9ec5ca3c-30e9-4bad-9ce9-96442eb929fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/atos_segs.csv')\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo_ato</th>\n",
              "      <th>ato</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>exoneracao</td>\n",
              "      <td>EXONERAR, a pedido, ANNA LUIZA BARCIA HALLEY d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR SANDRA TURCATO JORGE TOLENTINO para exe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR RAIMUNDO DA COSTA SANTOS NETO, Procurad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ato_tornado_sem_efeito</td>\n",
              "      <td>TORNAR SEM EFEITO no Decreto de 02 de julho de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>DESIGNAR\\nrespectivamente TEREZA CRISTINA DE A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>Designar FLÁVIO DE ARAÚJO ALMEIDA, matrícula n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>DESIGNAR OTAMÁ DANTAS BARRETO, matrícula\\n159....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>retificacao</td>\n",
              "      <td>RETIFICAR o Sexto Termo\\nAditivo ao Contrato, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>retificacao</td>\n",
              "      <td>RETIFICAR o Primeiro Termo Aditivo (45138948)\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>273 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   tipo_ato                                                ato\n",
              "0                  nomeacao  NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o ...\n",
              "1                exoneracao  EXONERAR, a pedido, ANNA LUIZA BARCIA HALLEY d...\n",
              "2                  nomeacao  NOMEAR SANDRA TURCATO JORGE TOLENTINO para exe...\n",
              "3                  nomeacao  NOMEAR RAIMUNDO DA COSTA SANTOS NETO, Procurad...\n",
              "4    ato_tornado_sem_efeito  TORNAR SEM EFEITO no Decreto de 02 de julho de...\n",
              "..                      ...                                                ...\n",
              "268            substituicao  DESIGNAR\\nrespectivamente TEREZA CRISTINA DE A...\n",
              "269            substituicao  Designar FLÁVIO DE ARAÚJO ALMEIDA, matrícula n...\n",
              "270            substituicao  DESIGNAR OTAMÁ DANTAS BARRETO, matrícula\\n159....\n",
              "271             retificacao  RETIFICAR o Sexto Termo\\nAditivo ao Contrato, ...\n",
              "272             retificacao  RETIFICAR o Primeiro Termo Aditivo (45138948)\\...\n",
              "\n",
              "[273 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQZ04vajogWS",
        "outputId": "88ab8812-604c-4242-fb44-87cfa0a2e9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.tipo_ato.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "aposentadoria             69\n",
              "nomeacao                  53\n",
              "exoneracao                49\n",
              "retificacao               38\n",
              "substituicao              37\n",
              "ato_tornado_sem_efeito    16\n",
              "cessao                     9\n",
              "abono_de_permanência       1\n",
              "reversao                   1\n",
              "Name: tipo_ato, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxLaHrbzogWX",
        "outputId": "dc4f0834-51f8-4674-c296-753deb011933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df[df.tipo_ato != 'abono_de_permanência']\n",
        "df = df[df.tipo_ato != 'reversao']\n",
        "df.tipo_ato.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "aposentadoria             69\n",
              "nomeacao                  53\n",
              "exoneracao                49\n",
              "retificacao               38\n",
              "substituicao              37\n",
              "ato_tornado_sem_efeito    16\n",
              "cessao                     9\n",
              "Name: tipo_ato, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlctNJRjogWc"
      },
      "source": [
        "### Criação de listas de atos e tipos de ato, em ordem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NKjFy6gogWd"
      },
      "source": [
        "atos = df.ato.to_list()\n",
        "tipos = df.tipo_ato.to_list()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVOV7iXogWh",
        "outputId": "2ba6b95a-d7d7-4550-a905-cf38e5e72837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(atos), len(tipos)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(271, 271)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfKcOaTbogWm"
      },
      "source": [
        "### Retira \\n das instâncias de atos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbXx6gQTogWo"
      },
      "source": [
        "import re\n",
        "\n",
        "at_contiguos = []\n",
        "for ato in atos:\n",
        "    at_contiguos.append(re.sub('\\n', ' ', ato))\n",
        "#at_contiguos"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xToZE3G3ogWt",
        "outputId": "ecc58255-6b12-4f4f-cc22-da1b22f8daa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(at_contiguos)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "271"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o1Wog0BogWy"
      },
      "source": [
        "### Cria lista de listas de referência\n",
        "\n",
        "infos é uma lista que contém duas outras listas:\n",
        "- tipos\n",
        "- at_contiguos\n",
        "\n",
        "Ela servirá de referência para mapear instâncias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QXERrHaogWy",
        "outputId": "289059bb-5082-4c16-951e-64a9093c404c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "infos = []\n",
        "infos.append(tipos)\n",
        "infos.append(at_contiguos)\n",
        "infos\n",
        "\n",
        "infos[0][1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'exoneracao'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEmKDbirogW3",
        "outputId": "eeefab6f-a5fe-46bb-bec1-4b52a2751775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "i = 0\n",
        "infos[0][i], infos[1][i]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('nomeacao',\n",
              " 'NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o Cargo em Comissão, Símbolo CC-04, código SIGRH B0001615, de Assessor Técnico, da Chefia de Gabinete, do Gabinete do Governador.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Hqxt-4ogW8"
      },
      "source": [
        "### Função para tokenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xU8XRuyogW9"
      },
      "source": [
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "# Function for tokenizing\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKNPWKa6ogXB"
      },
      "source": [
        "### Mapeia labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R_ifm_zogXC"
      },
      "source": [
        "possible_labels = df.tipo_ato.unique()\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJzwBixogXH",
        "outputId": "7e8d6442-a261-4da0-b79a-3774f4cb335e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df['label'] = df.tipo_ato.replace(label_dict)\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo_ato</th>\n",
              "      <th>ato</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>exoneracao</td>\n",
              "      <td>EXONERAR, a pedido, ANNA LUIZA BARCIA HALLEY d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR SANDRA TURCATO JORGE TOLENTINO para exe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nomeacao</td>\n",
              "      <td>NOMEAR RAIMUNDO DA COSTA SANTOS NETO, Procurad...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ato_tornado_sem_efeito</td>\n",
              "      <td>TORNAR SEM EFEITO no Decreto de 02 de julho de...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>DESIGNAR\\nrespectivamente TEREZA CRISTINA DE A...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>Designar FLÁVIO DE ARAÚJO ALMEIDA, matrícula n...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>substituicao</td>\n",
              "      <td>DESIGNAR OTAMÁ DANTAS BARRETO, matrícula\\n159....</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>retificacao</td>\n",
              "      <td>RETIFICAR o Sexto Termo\\nAditivo ao Contrato, ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>retificacao</td>\n",
              "      <td>RETIFICAR o Primeiro Termo Aditivo (45138948)\\...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>271 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   tipo_ato  ... label\n",
              "0                  nomeacao  ...     0\n",
              "1                exoneracao  ...     1\n",
              "2                  nomeacao  ...     0\n",
              "3                  nomeacao  ...     0\n",
              "4    ato_tornado_sem_efeito  ...     2\n",
              "..                      ...  ...   ...\n",
              "268            substituicao  ...     4\n",
              "269            substituicao  ...     4\n",
              "270            substituicao  ...     4\n",
              "271             retificacao  ...     3\n",
              "272             retificacao  ...     3\n",
              "\n",
              "[271 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4wJcchjogXM"
      },
      "source": [
        "### Divisão treino e teste para fins de indexação do modelo doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDiGlJz9ogXQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.ato.values, \n",
        "                                                  df.label.values, \n",
        "                                                  test_size=0.25, \n",
        "                                                  random_state=14, \n",
        "                                                  stratify=df.label.values)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6-UaJOVogXU"
      },
      "source": [
        "#X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syu7pNOmogXY",
        "outputId": "b687324f-a735-43a5-9bf8-8eff5b5291a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "infos[0][0], infos[1][0], len(infos[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('nomeacao',\n",
              " 'NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o Cargo em Comissão, Símbolo CC-04, código SIGRH B0001615, de Assessor Técnico, da Chefia de Gabinete, do Gabinete do Governador.',\n",
              " 271)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAhwCTHKogXd",
        "outputId": "805826ba-d7ec-4427-cd76-e1d3253d9a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "i = 0\n",
        "infos[0][i], infos[1][i]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('nomeacao',\n",
              " 'NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o Cargo em Comissão, Símbolo CC-04, código SIGRH B0001615, de Assessor Técnico, da Chefia de Gabinete, do Gabinete do Governador.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaJguewDmwmq",
        "outputId": "f6be8fbe-7e15-4049-bceb-b130482950e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMDnWOCCogXh"
      },
      "source": [
        "# Initializing the variables\n",
        "train_documents = []\n",
        "test_documents = []\n",
        "\n",
        "j = 0\n",
        "for i in X_train:\n",
        "    tag = y_train[j].item()\n",
        "    train_documents.append(TaggedDocument(words=tokenize_text(i), tags=[tag]))\n",
        "    j+=1\n",
        "\n",
        "j = 0\n",
        "for i in X_test:\n",
        "    tag = y_test[j].item()\n",
        "    test_documents.append(TaggedDocument(words=tokenize_text(i), tags=[tag]))\n",
        "    j+=1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVmVEpYyogXm",
        "scrolled": true,
        "outputId": "c484aeed-30ae-42a8-bf7f-f77746e23d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_documents[7]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=['exonerar', 'por', 'estar', 'sendo', 'nomeada', 'para', 'outro', 'cargo', 'karla', 'neres', 'de', 'laet', 'santana', 'do', 'cargo', 'em', 'comissão', 'símbolo', 'cc-06', 'código', 'sigrh', '07200232', 'de', 'assessor', 'do', 'gabinete', 'da', 'administração', 'regional', 'do', 'plano', 'piloto', 'do', 'distrito', 'federal'], tags=[1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwM9uBB7ogXp"
      },
      "source": [
        "## doc2vec\n",
        "Instanciação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDItxpYtogXq",
        "outputId": "b1bd5808-9439-4955-d2e5-921b625bdd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "model_dbow = Doc2Vec(dm=1, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_documents)])\n",
        "train_documents  = utils.shuffle(train_documents)\n",
        "model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n",
        "\n",
        "def vector_for_learning(model, input_docs):\n",
        "    sents = input_docs\n",
        "    targets, feature_vectors = zip(*[(doc.tags, model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, feature_vectors\n",
        "\n",
        "#model_dbow.save('./actModel.d2v')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 203/203 [00:00<00:00, 29627.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3GEiGBdogXv",
        "outputId": "7ec24808-87b5-4c91-fa00-4fa61139ffc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_documents)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgCxdUw9ZIkE"
      },
      "source": [
        "Divisão treino e teste para uso no modelo em si"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXj6KH5XogXz"
      },
      "source": [
        "y_train_model, X_train_model = vector_for_learning(model_dbow, train_documents)\n",
        "y_test_model, X_test_model = vector_for_learning(model_dbow, test_documents)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSfGc31XogX4"
      },
      "source": [
        "As duas células abaixo são usadas para prevenir o seguinte: `DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().`\n",
        "O método ravel() converte o shape do array para (n,). Ainda assim o warning persiste!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSln7VAmogX5"
      },
      "source": [
        "y_resampled = pd.DataFrame(y_test_model)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ5oGa8WogX9",
        "outputId": "90054991-714b-4568-fd76-24a6031bfe5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vai = y_resampled.values\n",
        "vai = vai.ravel()\n",
        "vai.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMe_DF4kZLPs"
      },
      "source": [
        "Utilização do modelo com LogisticRegression e SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbnLrQ_EogYC",
        "outputId": "efd72a1b-f8e8-43e7-e564-2de0cd98c780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logreg = LogisticRegression(max_iter=300000)\n",
        "logreg.fit(X_train_model, y_train_model)\n",
        "y_pred_d2v_reg = logreg.predict(X_test_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUZD9L06ogYG",
        "outputId": "d9e94cd1-d069-473a-9647-a371f6f07e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_model_d2v = LinearSVC(max_iter=10000)\n",
        "svm_model_d2v.fit(X_train_model, y_train_model)\n",
        "svm_prediction_d2v = svm_model_d2v.predict(X_test_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7R6evaNogYJ"
      },
      "source": [
        "## TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRuTYT_JogYK"
      },
      "source": [
        "O primeiro passo é instanciar um objeto Tfidfvectorizer e gerar os vetores para o nosso corpus. A lista infos, que geramos algumas células atrás, é uma lista de 2 listas, que indexam em ordem label/texto de ato. Para gerar os vetores, vamos usar a lista dos textos de atos, infos[1]. O método fit_transform gera os vetores tfidf dos textos e os retorna em uma matriz esparsa. Embora os resultados tenham se apresentado idênticos em um experimento anterior 'primeiro usando fit_transform em todo o conjunto de dados e depois divindo em treino/teste', vamos adotar o padrão 'primeiro dividir em treino/teste, fit_transform nos dados de treino e apenas fit nos dados de teste', apenas por desencargo de consciência.\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GYafobfogYQ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(infos[1], df['label'], test_size=0.25, random_state=14, stratify=df['label'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Ug8Cl9ogYU",
        "outputId": "5f2ee04f-7fdc-4f98-a91e-ffd7d08d1452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# fit_transform no treino\n",
        "X_csr_train = vectorizer.fit_transform(X_train)\n",
        "print(X_csr_train.shape)\n",
        "\n",
        "# apenas transform no teste\n",
        "X_csr_test = vectorizer.transform(X_test)\n",
        "print(X_csr_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(203, 1651)\n",
            "(68, 1651)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWi742qmXegT"
      },
      "source": [
        "Com treino e teste em mãos, basta treinar os modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_5lPQEFYhYL"
      },
      "source": [
        "TFIDF + SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixzvVx3nogYX"
      },
      "source": [
        "svm_model = LinearSVC(max_iter=1000)\n",
        "svm_model.fit(X_csr_train, y_train)\n",
        "svm_prediction_tfidf = svm_model.predict(X_csr_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OUfE8NdYmEp"
      },
      "source": [
        "TFIDF + regressão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON9GmFH5ogYa"
      },
      "source": [
        "tfidf_logreg = LogisticRegression()\n",
        "tfidf_logreg.fit(X_csr_train, y_train)\n",
        "y_pred_tfidf_reg = tfidf_logreg.predict(X_csr_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHg2D1VuogYd"
      },
      "source": [
        "Para os labels, vamos usar a coluna label do dataframe, gerada ao mapear os valores de infos[0], a lista de labels em ordem, para valores inteiros em um dict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfSBWhgqogYe",
        "outputId": "6f26735a-7955-4ee0-c657-645ccc06819f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "labels = df['label']\n",
        "labels.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(271,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJheKTFeogYt"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2HlDU1pbdf"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw-ZAbb-OAde"
      },
      "source": [
        "Variáveis de paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO3gu9OyogY8"
      },
      "source": [
        "path_w2v_cbow300 = '/content/drive/My Drive/pesquisa/embeddings_NILC_W2V/cbow_s300.txt'\n",
        "path_w2v_skip300 = '/content/drive/My Drive/pesquisa/embeddings_NILC_W2V/skip_s300.txt'\n",
        "path_glove300 = '/content/drive/My Drive/pesquisa/embeddings_NILC_GLOVE/glove_s300.txt'\n",
        "path_ft_cbow300 = '/content/drive/My Drive/pesquisa/embeddings_NILC_FT/cbow_s300.txt'\n",
        "path_ft_skip300 = '/content/drive/My Drive/pesquisa/embeddings_NILC_FT/skip_s300.txt'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQs35KraOGhz"
      },
      "source": [
        "Para cada tipo de embedding pré-treinado que temos, vamos instanciar um dicionário e fazer append dos 10000 primeiros (ou mais frequentes) termos como chaves e listas contendo as representações vetoriais desse termo. No caso, todas as embeddings que estamos usando tem 300 dimensões, então são 300 valores float em cada lista representando cada termo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfJ-eX31qycF",
        "outputId": "f5323519-1bd9-4439-aef9-25a040a2d131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dictionary = open(path_ft_skip300, 'r', encoding='utf-8',\n",
        "                  newline='\\n', errors='ignore')\n",
        "embeds = {}\n",
        "for line in dictionary:\n",
        "    tokens = line.rstrip().split(' ')\n",
        "    embeds[tokens[0]] = [float(x) for x in tokens[1:]]\n",
        "    \n",
        "    if len(embeds) == 10000:\n",
        "        break\n",
        "\n",
        "print(len(embeds))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZwIEMjzO6rk"
      },
      "source": [
        "Essa é uma exploração sobre a média de palavras em cada ato de nosso conjunto de dados. Com base nessa média vamos determinar o tamanho do slice na lista referente a cada ato já tokenizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFmAynEtuRBd",
        "outputId": "f7a3b4f7-391c-47c6-f310-29650a8173a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(infos[1][0])\n",
        "print(f'Qtd de palavras doc 4: {len(infos[1][4])}')\n",
        "soma = 0\n",
        "for i in range(len(infos[1])):\n",
        "  soma += len(infos[1][i])\n",
        "media = soma/len(infos[1])\n",
        "print(f'Média de palavras por ato: {media}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NOMEAR LETÍCIA MATOS MAGALHÃES para exercer o Cargo em Comissão, Símbolo CC-04, código SIGRH B0001615, de Assessor Técnico, da Chefia de Gabinete, do Gabinete do Governador.\n",
            "Qtd de palavras doc 4: 382\n",
            "Média de palavras por ato: 420.8228782287823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBYpjPn0QOs3"
      },
      "source": [
        "Cada ato tem, em média, 420 palavras. Vamos, então, usar um slice dos 100 primeiros termos de cada ato para montar a representação embedding do nosso conjunto de dados. O pedaço de código abaixo é o seguinte:\n",
        "1. Criamos um dataframe;\n",
        "2. Tokenizamos e salvamos os 100 primeiros termos de cada ato em uma variável words; words será da forma ['nomear', 'fulano', 'de', 'tal', 'para', ...];\n",
        "3. Para cada termo em words verificamos sua representação vetorial correspondente no dict que criamos mais cedo. Se o termo não existe no dict, apenas vamos para o próximo;\n",
        "4. Supondo que todos os 100 termos existam no dict, nossa linha terá um shape (1, 30000), já que cada um dos 100 termos tem 300 dimensões em sua representação vetorial. Nos casos onde temos menos de 100 termos adicionados - quando o termo em questão não existe no dict, por exemplo - teríamos um shape diferente, e isso nos traria problemas no treinamento. Para resolver essa questão, salvamos em uma variável array_length o tamanho esperado de cada linha do dataframe e adicionamos zeros às linhas que não adicionarem nativamente os 100 termos, de modo que, adicionando ou não 100 termos, o shape será sempre (1, 30000);\n",
        "5. Fazemos append de cada linha processada no dataframe criado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcYrBovdrwhg",
        "outputId": "1068fad2-9478-4098-c2de-388067df0268",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 100 tokens * 300 dimensões\n",
        "array_length = 100 * 300\n",
        "embedding_features = pd.DataFrame()\n",
        "for document in infos[1]:\n",
        "    # Saving the first 100 words of the document as a sequence\n",
        "    words = tokenize_text(document)[0:100]\n",
        "    \n",
        "    # Retrieving the vector representation of each word and \n",
        "    # appending it to the feature vector \n",
        "    feature_vector = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            feature_vector = np.append(feature_vector, \n",
        "                                       np.array(embeds[word]))\n",
        "        except KeyError:\n",
        "            # In the event that a word is not included in our \n",
        "            # dictionary skip that word\n",
        "            pass\n",
        "    # If the text has less then 100 words, fill remaining vector with\n",
        "    # zeros\n",
        "    zeroes_to_add = array_length - len(feature_vector)\n",
        "    if zeroes_to_add > 0:\n",
        "      feature_vector = np.append(feature_vector, \n",
        "                                np.zeros(zeroes_to_add)\n",
        "                                ).reshape((1,-1))\n",
        "    \n",
        "    feature_vector = feature_vector.reshape((1,-1))\n",
        "    \n",
        "    # Append the document feature vector to the feature table\n",
        "    embedding_features = embedding_features.append( \n",
        "                                     pd.DataFrame(feature_vector))\n",
        "print(embedding_features.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(271, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXfKxElZS66q"
      },
      "source": [
        "Aqui fazemos o split do dataframe de embeddings em treino e teste. Mantemos as mesmas configurações dos testes anteriores, para fins de comparação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOfhPxwnCAh2",
        "outputId": "1eb684fc-91fc-4a07-afee-910c0ec791f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "emb_train, emb_test = train_test_split(embedding_features, test_size=0.25, random_state=14, stratify=labels)\n",
        "\n",
        "print(emb_train.shape, emb_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(203, 30000) (68, 30000)\n",
            "(68,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53S3ceLYTGvZ"
      },
      "source": [
        "Com os conjuntos de treino e teste e respectivos rótulos já em mãos, basta rodar em seu modelo favorito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJylsJLibA0b"
      },
      "source": [
        "Word2vec CBOW 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHywJpSpC2bu"
      },
      "source": [
        "svm_w2v_cbow300 = LinearSVC(max_iter=40000)\n",
        "svm_w2v_cbow300.fit(emb_train, y_train)\n",
        "svm_pred_w2v_cbow300 = svm_w2v_cbow300.predict(emb_test)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN47IF7VDmT7"
      },
      "source": [
        "w2v_cbow300_logreg = LogisticRegression()\n",
        "w2v_cbow300_logreg.fit(emb_train, y_train)\n",
        "y_pred_w2v_cbow300_reg = w2v_cbow300_logreg.predict(emb_test)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KYK3rlgbE_x"
      },
      "source": [
        "Word2vec SkipGram 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPEPHAbabIqr"
      },
      "source": [
        "svm_w2v_skip300 = LinearSVC(max_iter=40000)\n",
        "svm_w2v_skip300.fit(emb_train, y_train)\n",
        "svm_pred_w2v_skip300 = svm_w2v_skip300.predict(emb_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnVhA-EUbQNR"
      },
      "source": [
        "w2v_skip300_logreg = LogisticRegression()\n",
        "w2v_skip300_logreg.fit(emb_train, y_train)\n",
        "y_pred_w2v_skip300_reg = w2v_skip300_logreg.predict(emb_test)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgj7Js77buWJ"
      },
      "source": [
        "Glove 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vrZvM7nbwOO"
      },
      "source": [
        "svm_glove300 = LinearSVC(max_iter=40000)\n",
        "svm_glove300.fit(emb_train, y_train)\n",
        "svm_pred_glove300 = svm_glove300.predict(emb_test)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnRRRzz3bxuf"
      },
      "source": [
        "glove300_logreg = LogisticRegression(max_iter=10000)\n",
        "glove300_logreg.fit(emb_train, y_train)\n",
        "y_pred_glove300_reg = glove300_logreg.predict(emb_test)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtqqzPuSc4Ha"
      },
      "source": [
        "Fast-text CBOW 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXYmheoQc6wu"
      },
      "source": [
        "svm_ft_cbow300 = LinearSVC(max_iter=40000)\n",
        "svm_ft_cbow300.fit(emb_train, y_train)\n",
        "svm_pred_ft_cbow300 = svm_ft_cbow300.predict(emb_test)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm3Iz8Hxc8VX"
      },
      "source": [
        "ft_cbow300_logreg = LogisticRegression(max_iter=10000)\n",
        "ft_cbow300_logreg.fit(emb_train, y_train)\n",
        "y_pred_ft_cbow300_reg = ft_cbow300_logreg.predict(emb_test)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIupWsw9dmJA"
      },
      "source": [
        "Fast-text SkipGram 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExkNKPj5dpDl"
      },
      "source": [
        "svm_ft_skip300 = LinearSVC(max_iter=40000)\n",
        "svm_ft_skip300.fit(emb_train, y_train)\n",
        "svm_pred_ft_skip300 = svm_ft_skip300.predict(emb_test)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plblGDtddqZ9"
      },
      "source": [
        "ft_skip300_logreg = LogisticRegression(max_iter=10000)\n",
        "ft_skip300_logreg.fit(emb_train, y_train)\n",
        "y_pred_ft_skip300_reg = ft_skip300_logreg.predict(emb_test)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FIZtmqBTSMO"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQlE_Ovao1U"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV-5nMlmDKsF"
      },
      "source": [
        "results = pd.DataFrame(columns = ['Precision', 'Recall', 'F1 score', 'support']\n",
        "          )\n",
        "results.loc['d2v + regressão'] = precision_recall_fscore_support(\n",
        "          vai, \n",
        "          y_pred_d2v_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['d2v + SVM'] = precision_recall_fscore_support(\n",
        "          vai, \n",
        "          svm_prediction_d2v, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['TFIDF + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_prediction_tfidf, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['TFIDF + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_tfidf_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['W2V CBOW 300 + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_pred_w2v_cbow300, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['W2V CBOW 300 + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_w2v_cbow300_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['W2V Skip 300 + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_pred_w2v_skip300, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['W2V Skip 300 + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_w2v_skip300_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Glove 300 + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_pred_glove300, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Glove 300 + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_glove300_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Fast-text CBOW 300 + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_pred_ft_cbow300, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Fast-text CBOW 300 + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_ft_cbow300_reg, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Fast-text Skip 300 + SVM'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          svm_pred_ft_skip300, \n",
        "          average = 'weighted'\n",
        "          )\n",
        "results.loc['Fast-text Skip 300 + regressão'] = precision_recall_fscore_support(\n",
        "          y_test, \n",
        "          y_pred_ft_skip300_reg, \n",
        "          average = 'weighted'\n",
        "          )"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVWYcjIQDcGl",
        "outputId": "bec170d4-68f0-4231-ac37-729b1af29f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "results"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>d2v + regressão</th>\n",
              "      <td>0.826824</td>\n",
              "      <td>0.838235</td>\n",
              "      <td>0.827509</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d2v + SVM</th>\n",
              "      <td>0.877245</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.878037</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TFIDF + SVM</th>\n",
              "      <td>0.986631</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.984594</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TFIDF + regressão</th>\n",
              "      <td>0.972976</td>\n",
              "      <td>0.970588</td>\n",
              "      <td>0.966309</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V CBOW 300 + SVM</th>\n",
              "      <td>0.986425</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.985156</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V CBOW 300 + regressão</th>\n",
              "      <td>0.959195</td>\n",
              "      <td>0.955882</td>\n",
              "      <td>0.956218</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V Skip 300 + SVM</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V Skip 300 + regressão</th>\n",
              "      <td>0.971639</td>\n",
              "      <td>0.970588</td>\n",
              "      <td>0.970428</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glove 300 + SVM</th>\n",
              "      <td>0.986425</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.985201</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glove 300 + regressão</th>\n",
              "      <td>0.943439</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.941765</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fast-text CBOW 300 + SVM</th>\n",
              "      <td>0.935294</td>\n",
              "      <td>0.926471</td>\n",
              "      <td>0.926993</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fast-text CBOW 300 + regressão</th>\n",
              "      <td>0.905638</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.898735</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fast-text Skip 300 + SVM</th>\n",
              "      <td>0.986425</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.985201</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fast-text Skip 300 + regressão</th>\n",
              "      <td>0.986425</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>0.985201</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                Precision    Recall  F1 score  support\n",
              "d2v + regressão                  0.826824  0.838235  0.827509      NaN\n",
              "d2v + SVM                        0.877245  0.882353  0.878037      NaN\n",
              "TFIDF + SVM                      0.986631  0.985294  0.984594      NaN\n",
              "TFIDF + regressão                0.972976  0.970588  0.966309      NaN\n",
              "W2V CBOW 300 + SVM               0.986425  0.985294  0.985156      NaN\n",
              "W2V CBOW 300 + regressão         0.959195  0.955882  0.956218      NaN\n",
              "W2V Skip 300 + SVM               1.000000  1.000000  1.000000      NaN\n",
              "W2V Skip 300 + regressão         0.971639  0.970588  0.970428      NaN\n",
              "Glove 300 + SVM                  0.986425  0.985294  0.985201      NaN\n",
              "Glove 300 + regressão            0.943439  0.941176  0.941765      NaN\n",
              "Fast-text CBOW 300 + SVM         0.935294  0.926471  0.926993      NaN\n",
              "Fast-text CBOW 300 + regressão   0.905638  0.897059  0.898735      NaN\n",
              "Fast-text Skip 300 + SVM         0.986425  0.985294  0.985201      NaN\n",
              "Fast-text Skip 300 + regressão   0.986425  0.985294  0.985201      NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC3Xc6mzTWUT"
      },
      "source": [
        "## Discussão\n",
        "\n",
        "O uso de embeddings pré-treinadas aqui se mostrou muito efetivo. A princípio, eu havia carregado um vocabulário de 100000 termos. Quando rodei o experimento com SVM, o modelo acertou tudo. Diminuí o vocabulário para 10000 termos e os modelos passaram a acertar algo acima de 95% dos casos. Eu havia feito um experimento parecido com os dados do trabalho do Pedro Luz, sobre textos de secretarias, e havia obtido um valor menor de F1-score usando embeddings em relação ao baseline TFIDF. A princípio, então, pensei que o que estava acontecendo aqui era um erro XD, mas depois de refletir um pouco sobre a natureza da minha base de dados, imagino o seguinte cenário:\n",
        "1. Os atos que constam no DODF tem uma estrutura muito mais rígida em relação aos textos de secretarias;\n",
        "2. Outro fator que conta bastante é o vocabulário diminuto da minha base em relação à base do Pedro: lá eu usei as mesmas embeddings, com 100000 termos carregados no dict, e ainda assim tive perfomance menor que o baseline; aqui, usei 10% do número de termos e obtive performance semelhante ou melhor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmI-b8ZEe7w4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}