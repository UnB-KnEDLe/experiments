{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, tokenizer=''):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def __call__(self, X, **kw_params):\n",
    "        return self.tokenizer(X, **kw_params)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X, **kw_params):\n",
    "        if not isinstance(X, pd.Series):\n",
    "            print(\"[preprocess.Tokenizer.transform] TYPE:\", type(X))\n",
    "            print('X:::: ', X)\n",
    "            X = pd.Series(X)\n",
    "        return X.map(self)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class IOBifyer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    @staticmethod\n",
    "    def find_entity(row, token, ignore_idx=0,\n",
    "        tokenizer=''):\n",
    "        # TODO: aceitar opção de offset, para não ter tennhum tipo de problema\n",
    "        for idx, column in enumerate(row.keys()):\n",
    "            if idx == ignore_idx:\n",
    "                continue\n",
    "            if isinstance(row[column], str) and \\\n",
    "                token == word_tokenize(row[column])[0]:\n",
    "                return column\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_IOB_labels(row, idx, tokenizer, dbg={}):\n",
    "        labels = []\n",
    "        entity_started = False\n",
    "        text = row.iloc[idx]\n",
    "        for token in word_tokenize(text):                         \n",
    "            if not entity_started:                               \n",
    "                entity = IOBifyer.find_entity(row, token, idx)                 \n",
    "                if entity is not None:                           \n",
    "                    entity_started = True\n",
    "                    token_index = 1\n",
    "                    labels.append('B-' + entity)\n",
    "                else:\n",
    "                    labels.append('O')\n",
    "            else:\n",
    "                if token_index < len(word_tokenize(row[entity])) and \\\n",
    "                    token == word_tokenize(row[entity])[token_index]:\n",
    "                    labels.append('I-' + entity)\n",
    "                    token_index += 1\n",
    "                    if token_index >= len(word_tokenize(row[entity])):\n",
    "                        entity_started = False\n",
    "                else:\n",
    "                    entity_started = False\n",
    "                    labels.append('O')\n",
    "        if labels[0] != 'O':\n",
    "            dbg['l'] = dbg.get('l', []) + [(row, idx)]\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_iob(tokens_mat, labels_mat, path='dump.txt',\n",
    "                            sep=' X X ', sent_sep='\\n',):\n",
    "        dbg_mat = []\n",
    "        if isinstance(path, Path):\n",
    "            path = path.as_posix()\n",
    "        if '/' in path:\n",
    "            os.makedirs('/'.join(path.split('/')[:-1]), exist_ok=True)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            for tokens_lis, labels_lis in zip(tokens_mat, labels_mat):\n",
    "                dbg_mat.append([])\n",
    "                for token, label in zip(tokens_lis, labels_lis):\n",
    "                    dbg_mat[-1].append((token, label))\n",
    "                    fp.write(f\"{token}{sep}{label}\\n\")\n",
    "                fp.write(sent_sep)\n",
    "        return dbg_mat\n",
    "\n",
    "\n",
    "    def __init__(self, column='act_column',\n",
    "        tokenizer=''):\n",
    "        self.column = column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dbg = {}\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"`df` expected to be a pd.DataFrame. Got {type(df)}\")\n",
    "        if df.empty:\n",
    "            print(\"[core.preprocess]Warning: empty DataFrame. There won't be ioblabels.\")\n",
    "            return pd.Series()\n",
    "\n",
    "        idx = self.column if isinstance(self.column, int) else  \\\n",
    "                df.columns.get_loc(self.column)\n",
    "        labels_row = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                labels_row.append(\n",
    "                    IOBifyer.generate_IOB_labels(\n",
    "                        row, idx, self.tokenizer, self.dbg\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"problem iobifyin row:\", row)\n",
    "                raise e\n",
    "        return pd.Series(labels_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob = IOBifyer(column='')\n",
    "r = iob.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IOB\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.loc[i, \"IOB\"] = ' '.join(r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a75d599c21bedaa008af7901534409cd119fdc00b97ec2fcbd6ba03e7c856c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
