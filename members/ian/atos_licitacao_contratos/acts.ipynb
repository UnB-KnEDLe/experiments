{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('parquet/all_acts_200x_2018_2020.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AVISO_LICITACAO', 'EXTRATO_CONTRATO_CONVENIO',\n",
       "       'EXTRATO_ADITAMENTO_CONTRATUAL', 'AVISO_SUSPENSAO_LICITACAO',\n",
       "       'AVISO_ANUL_REV_LICITACAO', 'EXTRATO_CONTRATO', 'EXTRATO_CONVENIO'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ato'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic = df[list(df.columns)].loc[(df['ato'] == 'AVISO_LICITACAO')]\n",
    "lic = lic.reset_index(drop=True)\n",
    "\n",
    "sus = df[list(df.columns)].loc[(df['ato'] == 'AVISO_SUSPENSAO_LICITACAO')]\n",
    "sus = sus.reset_index(drop=True)\n",
    "\n",
    "anr = df[list(df.columns)].loc[(df['ato'] == 'AVISO_ANUL_REV_LICITACAO')]\n",
    "anr = anr.reset_index(drop=True)\n",
    "\n",
    "adi = df[list(df.columns)].loc[(df['ato'] == 'EXTRATO_ADITAMENTO_CONTRATUAL')]\n",
    "adi = adi.reset_index(drop=True)\n",
    "\n",
    "con = df[list(df.columns)].loc[(df['ato'] == 'EXTRATO_CONTRATO') |\\\n",
    "                               (df['ato'] == 'EXTRATO_CONVENIO') |\\\n",
    "                               (df['ato'] == 'EXTRATO_CONTRATO_CONVENIO')]\n",
    "con = con.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licitação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text']\n",
    "\n",
    "for i in range(5, len(list(lic.isnull().sum()))):\n",
    "    if list(lic.isnull().sum())[i] != len(lic):\n",
    "        ent.append(list(lic.columns)[i])\n",
    "\n",
    "lic = lic[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspensão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text']\n",
    "\n",
    "for i in range(5, len(list(sus.isnull().sum()))):\n",
    "    if list(sus.isnull().sum())[i] != len(sus):\n",
    "        ent.append(list(sus.columns)[i])\n",
    "\n",
    "sus = sus[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anulação e Revogação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text']\n",
    "\n",
    "for i in range(5, len(list(anr.isnull().sum()))):\n",
    "    if list(anr.isnull().sum())[i] != len(anr):\n",
    "        ent.append(list(anr.columns)[i])\n",
    "\n",
    "anr = anr[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text']\n",
    "\n",
    "for i in range(5, len(list(adi.isnull().sum()))):\n",
    "    if list(adi.isnull().sum())[i] != len(adi):\n",
    "        ent.append(list(adi.columns)[i])\n",
    "\n",
    "adi = adi[ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrato e Convênio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text']\n",
    "\n",
    "for i in range(5, len(list(con.isnull().sum()))):\n",
    "    if list(con.isnull().sum())[i] != len(con):\n",
    "        ent.append(list(con.columns)[i])\n",
    "\n",
    "con = con[ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "con['NOME_RESPONSAVEL'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['arquivo_rast', 'text', 'ato', 'dodf', 'treated_text', 'PROCESSO',\n",
       "       'CONTRATANTE', 'CONTRATADA', 'OBJ_AJUSTE', 'VIGENCIA', 'VALOR', 'PT',\n",
       "       'DATA_ASSINATURA', 'CODIGO_UO', 'ND', 'NE', 'FUND_DISPENSA',\n",
       "       'ORGAO_LICITANTE', 'NUM_LICITACAO', 'NUM_CONTRATO', 'IDENT_DISPENSA',\n",
       "       'CONVENENTE', 'OG_ATA', 'FONTE_RECURSO', 'CNPJ_CONTRATADA',\n",
       "       'CONCEDENTE', 'CNPJ_CONVENENTE', 'CNPJ_CONTRATANTE', 'CNPJ_CONCEDENTE',\n",
       "       'NOME_RESPONSAVEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, tokenizer=''):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def __call__(self, X, **kw_params):\n",
    "        return self.tokenizer(X, **kw_params)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X, **kw_params):\n",
    "        if not isinstance(X, pd.Series):\n",
    "            print(\"[preprocess.Tokenizer.transform] TYPE:\", type(X))\n",
    "            print('X:::: ', X)\n",
    "            X = pd.Series(X)\n",
    "        return X.map(self)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class IOBifyer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    @staticmethod\n",
    "    def find_entity(row, token, ignore_idx=0,\n",
    "        tokenizer=''):\n",
    "        # TODO: aceitar opção de offset, para não ter tennhum tipo de problema\n",
    "        for idx, column in enumerate(row.keys()):\n",
    "            if idx == ignore_idx:\n",
    "                continue\n",
    "            if isinstance(row[column], str) and \\\n",
    "                token == word_tokenize(row[column])[0]:\n",
    "                return column\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_IOB_labels(row, idx, tokenizer, dbg={}):\n",
    "        labels = []\n",
    "        entity_started = False\n",
    "        text = row.iloc[idx]\n",
    "        for token in word_tokenize(text):                         \n",
    "            if not entity_started:                               \n",
    "                entity = IOBifyer.find_entity(row, token, idx)                 \n",
    "                if entity is not None:                           \n",
    "                    entity_started = True\n",
    "                    token_index = 1\n",
    "                    labels.append('B-' + entity)\n",
    "                else:\n",
    "                    labels.append('O')\n",
    "            else:\n",
    "                if token_index < len(word_tokenize(row[entity])) and \\\n",
    "                    token == word_tokenize(row[entity])[token_index]:\n",
    "                    labels.append('I-' + entity)\n",
    "                    token_index += 1\n",
    "                    if token_index >= len(word_tokenize(row[entity])):\n",
    "                        entity_started = False\n",
    "                else:\n",
    "                    entity_started = False\n",
    "                    labels.append('O')\n",
    "        if labels[0] != 'O':\n",
    "            dbg['l'] = dbg.get('l', []) + [(row, idx)]\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_iob(tokens_mat, labels_mat, path='dump.txt',\n",
    "                            sep=' X X ', sent_sep='\\n',):\n",
    "        dbg_mat = []\n",
    "        if isinstance(path, Path):\n",
    "            path = path.as_posix()\n",
    "        if '/' in path:\n",
    "            os.makedirs('/'.join(path.split('/')[:-1]), exist_ok=True)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            for tokens_lis, labels_lis in zip(tokens_mat, labels_mat):\n",
    "                dbg_mat.append([])\n",
    "                for token, label in zip(tokens_lis, labels_lis):\n",
    "                    dbg_mat[-1].append((token, label))\n",
    "                    fp.write(f\"{token}{sep}{label}\\n\")\n",
    "                fp.write(sent_sep)\n",
    "        return dbg_mat\n",
    "\n",
    "\n",
    "    def __init__(self, column='act_column',\n",
    "        tokenizer=''):\n",
    "        self.column = column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dbg = {}\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"`df` expected to be a pd.DataFrame. Got {type(df)}\")\n",
    "        if df.empty:\n",
    "            print(\"[core.preprocess]Warning: empty DataFrame. There won't be ioblabels.\")\n",
    "            return pd.Series()\n",
    "\n",
    "        idx = self.column if isinstance(self.column, int) else  \\\n",
    "                df.columns.get_loc(self.column)\n",
    "        labels_row = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                labels_row.append(\n",
    "                    IOBifyer.generate_IOB_labels(\n",
    "                        row, idx, self.tokenizer, self.dbg\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"problem iobifyin row:\", row)\n",
    "                raise e\n",
    "        return pd.Series(labels_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licitacao\n",
    "lic.drop(166, inplace=True)\n",
    "lic = lic.reset_index(drop=True)\n",
    "\n",
    "lic_iob = lic[list(lic.columns)[4:]]\n",
    "\n",
    "iob_lic= IOBifyer(column='treated_text')\n",
    "r_lic = iob_lic.transform(lic_iob)\n",
    "lic[\"IOB\"] = np.nan\n",
    "\n",
    "for i in range(len(lic)):\n",
    "    lic.loc[i, \"IOB\"] = ' '.join(r_lic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suspensao\n",
    "sus_iob = sus[list(sus.columns)[4:]]\n",
    "\n",
    "iob_sus = IOBifyer(column='treated_text')\n",
    "r_sus = iob_sus.transform(sus_iob)\n",
    "sus[\"IOB\"] = np.nan\n",
    "\n",
    "for i in range(len(sus)):\n",
    "    sus.loc[i, \"IOB\"] = ' '.join(r_sus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anulacao e revogacao\n",
    "anr_iob = anr[list(anr.columns)[4:]]\n",
    "\n",
    "iob_anr = IOBifyer(column='treated_text')\n",
    "r_anr = iob_anr.transform(anr_iob)\n",
    "anr[\"IOB\"] = np.nan\n",
    "\n",
    "for i in range(len(anr)):\n",
    "    anr.loc[i, \"IOB\"] = ' '.join(r_anr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aditamento\n",
    "adi_iob = adi[list(adi.columns)[4:]]\n",
    "\n",
    "iob_adi = IOBifyer(column='treated_text')\n",
    "r_adi = iob_adi.transform(adi_iob)\n",
    "adi[\"IOB\"] = np.nan\n",
    "\n",
    "for i in range(len(adi)):\n",
    "    adi.loc[i, \"IOB\"] = ' '.join(r_adi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrato e convenio\n",
    "con_iob = con[list(con.columns)[4:]]\n",
    "\n",
    "iob_con = IOBifyer(column='treated_text')\n",
    "r_con = iob_con.transform(con_iob)\n",
    "con[\"IOB\"] = np.nan\n",
    "\n",
    "for i in range(len(con)):\n",
    "    con.loc[i, \"IOB\"] = ' '.join(r_con[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-IDENT_REVOGACAO_ANULACAO I-IDENT_REVOGACAO_ANULACAO I-IDENT_REVOGACAO_ANULACAO B-MODALIDADE_LICITACAO I-MODALIDADE_LICITACAO O B-NUM_LICITACAO O O O O B-ORGAO_LICITANTE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n",
       " 'AVISO DE ANULAÇÃO PREGÃO PRESENCIAL Nº 03/2014. O PREGOEIRO da CPL comunica aos interessados a anulação da licitação em epígrafe, processo 001-000.601/2013, que tem por objeto a aquisição de canetas esferográficas para o Projeto Jovem Cidadão, da Câmara Legislativa do Distrito Federal, por reavaliação dos interesses da Administração (Escola do Legislativo ELEGIS). Maiores informações no local, pelo telefone (61) 3348.8650 ou 3348.8651 ou 3348.8652. Brasília/DF, 27 de janeiro de 2014. GUILHERME TAPAJÓS TÁVORA Pregoeiro')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anr['IOB'][0], anr['treated_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic.to_parquet('parquet/licitacao_435_acts_200x_2018_2020.parquet')\n",
    "sus.to_parquet('parquet/suspensao_25_acts_200x_2018_2020.parquet')\n",
    "anr.to_parquet('parquet/anulacao-revogacao_34_acts_200x_2018_2020.parquet')\n",
    "adi.to_parquet('parquet/aditamento_833_acts_200x_2018_2020.parquet')\n",
    "con.to_parquet('parquet/contrato-convenio_566_acts_200x_2018_2020.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 25, 34, 833, 566)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lic), len(sus), len(anr), len(adi), len(con)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "326a1a2f783c07b18559778bdda0b8194460ae1418816323f0f6f16e12b614e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
