{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score, classification_report, precision_score, recall_score, accuracy_score\n",
    "#! pip install plot_keras_history\n",
    "#from plot_keras_history import plot_history\n",
    "\n",
    "#!pip install scikit-learn==0.22.2\n",
    "\n",
    "#!pip install sklearn_crfsuite\n",
    "#from sklearn_crfsuite import CRF\n",
    "\n",
    "#!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./CSVs/V2/DODFCorpus_contratos_licitacoes_v2.csv', dtype=str)\n",
    "df = df.drop(['Unnamed: 0','Unnamed: 0.1'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Insere um espaço entre as entidade e insere um espaco apos :  : -> Processo:0...\n",
    "import re\n",
    "def correct_space_before_numeric_entities(string):\n",
    "    result = re.sub(r'([A-Za-z]:)[0-9]', r'\\1 ', string)\n",
    "    result = result.replace(\"\\n\", \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texto']= df['texto'].map(correct_space_before_numeric_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tipo_rel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatena o id_ato com o id_dodf\n",
    "df['id_ato'] = df['id_dodf'] + '-' + df['id_rel']\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista o tipo de relatório\n",
    "result =df.groupby('tipo_rel')['id_ato'].nunique()\n",
    "print(result.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_ato = df.query(\"tipo_rel == 'REL_EXTRATO_CONVENIO'\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista as entidades do tipo de ato\n",
    "result = tipo_ato.groupby('tipo_ent')['id_ato'].nunique()\n",
    "print(result)\n",
    "#print(result.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_name = 'EXTRATO_CONVENIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimetypes import init\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "class spacy_tokenizer():\n",
    "    def __init__(self):\n",
    "        #self.nlp = spacy.load('pt_core_news_sm', disable=[\"ner\", \"lemmatizer\"])\n",
    "        self.nlp = spacy.load('pt_core_news_sm', disable = ['parser','ner'])\n",
    "        \n",
    "\n",
    "    def tokenize(self, texto):\n",
    "                \n",
    "        doc = self.nlp(texto)\n",
    "        return[t.text.strip() for t in doc if t.text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iob_transformer():\n",
    "    \n",
    "    def __init__(self, coluna_id_ato: str, coluna_texto_entidade: str,\n",
    "                 coluna_tipo_entidade: str, keep_punctuation: bool = False,\n",
    "                 return_df: bool = False, tokenizer_tipo: str = \"Função\"):\n",
    "        self.coluna_id_ato = coluna_id_ato\n",
    "        self.coluna_texto_entidade = coluna_texto_entidade\n",
    "        self.coluna_tipo_entidade = coluna_tipo_entidade\n",
    "        self.tokenizer_tipo = tokenizer_tipo\n",
    "        if not keep_punctuation: #False\n",
    "            #self.tokenizer = RegexpTokenizer('\\w+')\n",
    "            self.tokenizer = spacy_tokenizer()\n",
    "        else:\n",
    "            self.tokenizer = False\n",
    "        self.return_df = return_df\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def gera_listas_atos_iobs(self, df):\n",
    "        \n",
    "        def _inclui_tags_vazias(texto_iob):\n",
    "            texto_ato_iob = texto_iob.copy()\n",
    "            for idx, token in enumerate(texto_ato_iob):\n",
    "                if token[0:2] == 'B-':# in token:\n",
    "                    pass\n",
    "                elif token[0:2] == 'I-':# in token:\n",
    "                    pass\n",
    "                else:\n",
    "                    texto_ato_iob[idx] = 'O'\n",
    "            \n",
    "            return texto_ato_iob\n",
    "        \n",
    "        def _constroi_iob(texto_ent, tipo_ent):\n",
    "            if self.tokenizer:\n",
    "                texto_ent_tok = self.tokenizer.tokenize(texto_ent)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                texto_ent_tok = word_tokenize(texto_ent)\n",
    "            iob_entidade = []\n",
    "            for index_token, token in enumerate(texto_ent_tok):\n",
    "                # primeiro token?\n",
    "                if index_token == 0:\n",
    "                    palavra = 'B-'+ tipo_ent\n",
    "                    iob_entidade.append(palavra)\n",
    "                # é o segundo token?\n",
    "                else:\n",
    "                    palavra = 'I-'+ tipo_ent\n",
    "                    iob_entidade.append(palavra)\n",
    "            # salva tupla contendo texto tokenizado e iob correspondente\n",
    "            if self.tokenizer:\n",
    "                tup_entidade = (self.tokenizer.tokenize(texto_ent), iob_entidade)\n",
    "            \n",
    "            else:\n",
    "                tup_entidade = (word_tokenize(texto_ent), iob_entidade)\n",
    "            return tup_entidade\n",
    "        \n",
    "        def _match_iob_texto_ato(texto_entidade_tok, iob_ato):\n",
    "            texto_ato_iob = texto_entidade_tok.copy()\n",
    "            #print(iob_ato)\n",
    "            for tupla in iob_ato:\n",
    "                 # checa se o texto de referência existe\n",
    "                if tupla[0]:\n",
    "                    for i in range(len(texto_entidade_tok)):\n",
    "                        # checa se a tag existe\n",
    "                        if tupla[0][0]:\n",
    "                            # match primeiro token\n",
    "                            if texto_entidade_tok[i] == tupla[0][0]:\n",
    "                                # a sequência de tokens de texto_entidade_token na\n",
    "                                # posição encontrada é igual aos tokens da entidade?\n",
    "                                if texto_entidade_tok[i:i+len(tupla[0])] == tupla[0]:\n",
    "                                    texto_ato_iob[i:i+len(tupla[0])] = tupla[1]\n",
    "            \n",
    "            return texto_ato_iob\n",
    "\n",
    "        atos = []\n",
    "        lista_labels = []\n",
    "        id_atos = set()\n",
    "        for row in df.iterrows():\n",
    "            id_ato = df.iloc[row[0]][self.coluna_id_ato]\n",
    "            texto_ato = []\n",
    "            texto_ato_iob = []\n",
    "            if id_ato not in id_atos:\n",
    "                id_atos.add(id_ato)\n",
    "                lista_ids = list(df.query(f'{self.coluna_id_ato} == \"{id_ato}\"').index)\n",
    "                \n",
    "                iob_ato = []\n",
    "                # todas as anotações que não são o ato inteiro\n",
    "                for index in lista_ids:\n",
    "                    texto_entidade = df.iloc[index][self.coluna_texto_entidade]\n",
    "                    tipo_entidade = df.iloc[index][self.coluna_tipo_entidade]\n",
    "                    #Lucelia if isinstance(self.tokenizer, RegexpTokenizer):\n",
    "                    if self.tokenizer_tipo == \"Função\":\n",
    "                        texto_entidade_tok = self.tokenizer.tokenize(texto_entidade)\n",
    "\n",
    "                    else:\n",
    "                        texto_entidade_tok = word_tokenize(texto_entidade)\n",
    "                    if not tipo_entidade.isupper():\n",
    "                        tup_entidade = _constroi_iob(texto_entidade, tipo_entidade)\n",
    "                        iob_ato.append(tup_entidade)\n",
    "                # anotação do ato inteiro\n",
    "                for index in lista_ids:\n",
    "                    texto_entidade = df.iloc[index][self.coluna_texto_entidade]\n",
    "                    tipo_entidade = df.iloc[index][self.coluna_tipo_entidade]\n",
    "                    \n",
    "                    if self.tokenizer_tipo == \"Função\":\n",
    "                        texto_entidade_tok = self.tokenizer.tokenize(texto_entidade)\n",
    "                    else:\n",
    "                        texto_entidade_tok = word_tokenize(texto_entidade)\n",
    "                    if tipo_entidade.isupper():\n",
    "                        texto_ato = texto_entidade_tok\n",
    "                        texto_ato_iob = _match_iob_texto_ato(texto_entidade_tok, iob_ato)\n",
    "                texto_ato_iob = _inclui_tags_vazias(texto_ato_iob)\n",
    "                atos.append(texto_ato)\n",
    "                lista_labels.append(texto_ato_iob)\n",
    "        \n",
    "        return atos, lista_labels\n",
    "\n",
    "    def create_iob_df(self, atos, lista_labels):\n",
    "        rows_list = []\n",
    "        dict1 = {\n",
    "                'Sentence_idx': -1,\n",
    "                'Word': 'UNK',\n",
    "                'Tag': 'O'\n",
    "            }\n",
    "        rows_list.append(dict1)\n",
    "        id_ato = 0\n",
    "        for ato, labels in zip(atos, lista_labels):\n",
    "            for word, label in zip(ato, labels):\n",
    "                dict1 = {\n",
    "                    'Sentence_idx': id_ato,\n",
    "                    'Word': word,\n",
    "                    'Tag': label\n",
    "                }\n",
    "                rows_list.append(dict1)\n",
    "                \n",
    "            id_ato += 1\n",
    "        new_df = pd.DataFrame(rows_list)\n",
    "\n",
    "        return new_df    \n",
    "    \n",
    "    def transform(self, df, **transform_params):\n",
    "        dataframe = df.copy()\n",
    "        dataframe = dataframe.reset_index(drop=True)\n",
    "        atos, lista_labels = self.gera_listas_atos_iobs(dataframe)\n",
    "        if self.return_df:\n",
    "            iob_df = self.create_iob_df(atos, lista_labels)\n",
    "            return iob_df\n",
    "        else:\n",
    "            return atos, lista_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob = iob_transformer('id_ato','texto','tipo_ent', keep_punctuation=False, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts, labels = iob.transform(tipo_ato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustes das labels e dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wrong_tags(label_list):\n",
    "      for label in label_list:\n",
    "    for idx,w in enumerate(label):\n",
    "      if w in ['B-11','B-12','B-50', 'B-60', 'I-2']:\n",
    "        label[idx] = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_wrong_tags(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in acts:\n",
    "    for word in act:\n",
    "        words.add(word)\n",
    "#convertendo o set em uma lista\n",
    "words = list(words)\n",
    "\n",
    "words.append(\"ENDPAD\")\n",
    "words.append(\"UNK\")\n",
    "\n",
    "words_amt = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "\n",
    "for label in labels:\n",
    "    for tag in label:\n",
    "        tags.add(tag)\n",
    "tags = list(tags)\n",
    "tags_amt = len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "for i in range(0,len(acts)):\n",
    "    for word in acts[i]:\n",
    "        if word.lower() not in vocab:\n",
    "            vocab[word.lower()] = 1\n",
    "        else:\n",
    "            vocab[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),        \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "  inputs = [sent2features(s) for s in acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "acc = []\n",
    "loss = []\n",
    "f1 = []\n",
    "reports = []\n",
    "fold = 0\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  x_train = np.array(inputs)[train.astype(int)]\n",
    "  x_test = np.array(inputs)[test.astype(int)]\n",
    "\n",
    "  y_train = np.array(targets)[train.astype(int)]\n",
    "  y_test = np.array(targets)[test.astype(int)]\n",
    " \n",
    "  with open(os.path.join('./CSVs/V2/split/CISTI/WS/teste_ws/'+act_name+'_'+str(fold)+'_x_test.txt'), 'wb') as file:\n",
    "        pickle.dump(x_test, file)\n",
    "  with open(os.path.join('./CSVs/V2/split/CISTI/WS/teste_ws/'+act_name+'_'+str(fold)+'_y_test.txt'), 'wb') as file:    \n",
    "        pickle.dump(y_test, file)\n",
    "\n",
    "\n",
    "#act type\t model\t c1\t c2\n",
    "#aviso de licitação\t crf\t 0.0629\t 0.0510\n",
    "#extrato de contrato/convênio\t crf\t 0.1094\t 0.0230\n",
    "#aviso de suspensão de licitação\t crf\t461\t 0.05\n",
    "\n",
    "#extrato de aditamento contratual\t crf\t 0.0773\t 0.1447\n",
    "#aviso de revogação/anulação de licitação\t crf\t 0.1181\t 0.0069\n",
    "\n",
    "  #valores c1:0.461, c2:0.039\n",
    "  crf = CRF(\n",
    "  algorithm='lbfgs', \n",
    "  c1=\t0.1094, \n",
    "  c2= 0.0230, \n",
    "  max_iterations=70, \n",
    "  all_possible_transitions=True, verbose=1)\n",
    "\n",
    "  crf.fit(X=x_train, y=y_train)\n",
    "\n",
    "  y_pred = crf.predict(x_test)\n",
    "\n",
    "  f1.append(f1_score(y_test, y_pred))\n",
    "\n",
    "  reports.append(classification_report(y_test,y_pred))\n",
    "\n",
    "  acc.append(accuracy_score(y_test,y_pred))\n",
    "\n",
    "  r = classification_report(y_test,y_pred, output_dict=True, mode='strict', scheme=IOB2)\n",
    "\n",
    "  name = act_name + '_f' + str(fold) +\".npy\"\n",
    "  np.save('./Results/V2/CISTI/27_06/CRF/'+name, r)\n",
    "\n",
    "  fold = fold + 1\n",
    "  reports.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reports[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
