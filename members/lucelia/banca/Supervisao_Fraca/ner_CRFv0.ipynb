{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER CRF - Atos de Contratos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estiver utilizanod o VSCode na sua máquina local ou no servidor do projeto, o primeio passo é criar o ambiente virtual.\n",
    "\n",
    "* Acesse a pasta onde vai colocar o seu projeto, no meu caso está em:\n",
    "\n",
    "        /home/lucelia_vieira/Experimentos\n",
    "\n",
    "\n",
    "* Crie o ambiente\n",
    "    \n",
    "        python3 -m venv \n",
    "    \n",
    "\n",
    "* Para ativar o ambiente virtual localmente:\n",
    "    \n",
    "        source /home/lucelia_vieira/Experimentos/venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira vez que carregar o projeto no VScode execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!virtualenv --python=python3.8 venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instale os pacotes abaixo direto no ambiente criado caso não queira ter que executar essa célula sempre que tiver que executar o projeto.\n",
    "\n",
    "Para instalar direto no ambiente, o ambiente precisa estar ativo, conforme instrução para ativar o ambiente.\n",
    "\n",
    "Se estiver utilizanodo o Collab, alguns desses pacotes já estarão disponíveis, outros não. Então, na dúvida, execute sempre..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "#!pip install seqeval\n",
    "#!pip install scikit-learn==0.24\n",
    "#!pip install sklearn==0.0\n",
    "#!pip install sklearn-crfsuite==0.3.6\n",
    "#!pip install python-crfsuite==0.9.7\n",
    "#!pip install tqdm\n",
    "#!pip install pytorch-crf==0.7.2\n",
    "#!pip install torch==1.8.1\n",
    "#!pip install torch==1.11.0\n",
    "#!pip install torch-summary\n",
    "#!pip install nltk-3.7\n",
    "#!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "#!pip install matplotlib-3.5.2\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports dos pacotes que serão utilizados no código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucelia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converte CSV para Padrão IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao carregar o csv pelo pandas especifique o data class das colunas. No nosso caso todas são String, então:\n",
    "\n",
    "     dtype=str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./CSVs/V2/DODFCorpus_contratos_licitacoes_v2.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43733"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insere um espaço entre as entidade e :\n",
    "import re\n",
    "def correct_space_before_numeric_entities(string):\n",
    "    result = re.sub(r'([A-Za-z]:)[0-9]', r'\\1 ', string)\n",
    "    result = result.replace(\"\\n\", \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessa o dataset para ajustar o casos de entidade e : sem espaço\n",
    "#df['a']=df['a'].map(func)\n",
    "# #data['texto']= data['texto'].apply(result)\n",
    "data['texto']= data['texto'].map(correct_space_before_numeric_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o iob_transformer fazer a correlação ato-entidades_correspondentes é necessário ter uma coluna id_ato, que junte o id do dodf e o id da relação específica.\n",
    "\n",
    "Mas para que funcione corretamente as colunas id_dodf e id_rel devem ser do tipo String, conforme abaixo: \n",
    "\n",
    " 1   id_dodf       30382 non-null  object\n",
    " \n",
    " 3   id_rel        30382 non-null  object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43733 entries, 0 to 43732\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  43733 non-null  object\n",
      " 1   Unnamed: 0    43733 non-null  object\n",
      " 2   id_dodf       43733 non-null  object\n",
      " 3   tipo_rel      43733 non-null  object\n",
      " 4   id_rel        43733 non-null  object\n",
      " 5   anotador_rel  43733 non-null  object\n",
      " 6   tipo_ent      43733 non-null  object\n",
      " 7   id_ent        43733 non-null  object\n",
      " 8   anotador_ent  43733 non-null  object\n",
      " 9   offset        43733 non-null  object\n",
      " 10  length        43733 non-null  object\n",
      " 11  texto         43733 non-null  object\n",
      " 12  id_ato        43733 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id_ato'] = data['id_dodf'] + '-' + data['id_rel']\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checando o tipo da coluna id_ato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#type(data.at[30379, 'id_ato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checando o formato do id_ato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data.at[30377, 'id_ato']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feito isso, podemos checar o número de labels por Ato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo_rel\n",
      "REL_EXTRATO_CONTRATO        1734\n",
      "REL_ADITAMENTO_CONTRATO     1551\n",
      "REL_AVISO_LICITACAO          639\n",
      "REL_SUSPENSAO_LICITACAO       82\n",
      "REL_ANUL_REVOG_LICITACAO      52\n",
      "REL_EXTRATO_CONVENIO          32\n",
      "Name: id_ato, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = data.groupby('tipo_rel')['id_ato'].nunique()\n",
    "print(result.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados certinhos, podemos fazer uso do iob_transformer normalmente, mas antes vamos relizar alguns filtros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro filtro a ser aplicado é pelo tipo de ATO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.tipo_rel.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É utilizados uns ilocs no código do transformer, então por via das dúvidas é bom dar um reset_index nos dataframes filtrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRATO DE CONTRATO\n",
    "#tipo_ato = data.loc[data['tipo_rel'] == 'REL_EXTRATO_CONTRATO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_EXTRATO_CONTRATO.csv\")\n",
    "\n",
    "#ADITAMENTO DE CONTRATO\n",
    "tipo_ato = data.loc[data['tipo_rel'] == 'REL_ADITAMENTO_CONTRATO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_ADITAMENTO_CONTRATO.csv\")\n",
    "\n",
    "#SUSPENSAO LICITACAO\n",
    "#tipo_ato = data.loc[data['tipo_rel'] == 'REL_SUSPENSAO_LICITACAO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_SUSPENSAO_LICITACAO.csv\")\n",
    "\n",
    "#ANULACAO E REVOCACAO LICITACAO\n",
    "#tipo_ato = data.loc[data['tipo_rel'] == 'REL_ANUL_REVOG_LICITACAO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_ANUL_REVOG_LICITACAO.csv\")\n",
    "\n",
    "#AVISO LICITACAO\n",
    "#tipo_ato = data.loc[data['tipo_rel'] == 'REL_AVISO_LICITACAO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_AVISO_LICITACAO.csv\")\n",
    "\n",
    "#CONVENIO\n",
    "#tipo_ato = data.loc[data['tipo_rel'] == 'REL_EXTRATO_CONVENIO'].reset_index(drop=True)\n",
    "#tipo_ato.to_csv(\"./CSVs/V1/REL_AVISO_LICITACAO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipo_ato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apenas para rodar o iob com spacy\n",
    "#csv_reader = pd.read_csv(\"/home/lucelia_vieira/Experimentos/ner/CSVs/REL_EXTRATO_CONTRATO_COMPLETO.csv\", nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista as entidades rotuladas no ato\n",
    "#tipo_ato.tipo_ent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODIGO RESPONSAVEL PELA LIMPEZA DO TEXTO DA BASE OURO\n",
    "tipo_ato_2 = tipo_ato.copy()\n",
    "#tipo_ato_2 = csv_reader.copy()\n",
    "tipo_ato_2.texto = tipo_ato_2.texto.str.replace(\"\\n\", \" \")\n",
    "tipo_ato_2.texto = tipo_ato_2.texto.str.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns atos possuem entidades rotuladas incorretamente, ou que, a quantide de rótulos não seja representativo. Nesse caso, sugere-se remover essas entidades do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = ['codigo_siggo','numero_convenio','nome_responsavel']\n",
    "#l = ['codigo_siggo','numero_convenio','nome_responsavel']\n",
    "\n",
    "#tipo_ato = tipo_ato.loc[~tipo_ato.tipo_ent.isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teste = tipo_ato.loc[tipo_ato['id_ato'] == \"7_2.8.2019-R11\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após ter aplicado todos os filtros no dataset, podemos realizar o transfomer e converter para o formato IOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucelia/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# return_df=False para retornar atos e labels, ou  return_df=True para retornar dataset\n",
    "from iob_transformer import iob_transformer\n",
    "iob = iob_transformer('id_ato','texto','tipo_ent', keep_punctuation=False, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "atos, labels = iob.transform(tipo_ato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conferir se o foi realizado corretamente o iob ao dataset, imprima o retorno do transformer em formato de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iob_dataset = iob_transformer('id_ato', 'texto',\n",
    "#                      'tipo_ent', keep_punctuation=True, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_iob = iob_dataset.transform(tipo_ato)\n",
    "#dataset_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./CSVs/atos.txt', 'wt') as fileout:\n",
    "#    for item in atos:\n",
    "#        for token in item:\n",
    "#            fileout.write(str(token)+\"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./CSVs/labels.txt', 'wt') as fileout:\n",
    "#    for item in labels:\n",
    "#        fileout.write(str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350042"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cria uma Lista \n",
    "atos_list = []\n",
    "for i in atos:\n",
    "   if not isinstance(i, list):\n",
    "      atos_list.append(i)\n",
    "   else:\n",
    "      for j in i:\n",
    "        atos_list.append(j)\n",
    "\n",
    "len(atos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(atos_list,columns=['Texto']) \n",
    "#df.to_csv('./CSVs/atos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350042"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cria uma Lista \n",
    "label_list = []\n",
    "for i in labels:\n",
    "   if not isinstance(i, list):\n",
    "      label_list.append(i)\n",
    "   else:\n",
    "      for j in i:\n",
    "        label_list.append(j)\n",
    "\n",
    "len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listando as tags após a conversão para IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "\n",
    "for label in labels:\n",
    "    for tag in label:\n",
    "        tags.add(tag)\n",
    "tags = list(tags)\n",
    "num_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-codigo_licitacao_sistema_compras',\n",
       " 'I-valor_estimado_contratacao',\n",
       " 'B-tipo_objeto',\n",
       " 'I-nome_responsavel',\n",
       " 'I-orgao_licitante',\n",
       " 'I-sistema_compras',\n",
       " 'B-valor_estimado_contratacao',\n",
       " 'I-codigo_licitacao_sistema_compras',\n",
       " 'B-data_abertura_licitacao',\n",
       " 'I-data_abertura_licitacao',\n",
       " 'I-processo_gdf',\n",
       " 'B-nome_responsavel',\n",
       " 'B-sistema_compras',\n",
       " 'B-objeto_licitacao',\n",
       " 'O',\n",
       " 'B-orgao_licitante',\n",
       " 'B-processo_gdf',\n",
       " 'B-modalidade_licitacao',\n",
       " 'I-numero_licitacao',\n",
       " 'I-tipo_objeto',\n",
       " 'B-numero_licitacao',\n",
       " 'I-modalidade_licitacao',\n",
       " 'I-objeto_licitacao']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=atos\n",
    "y=labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate train and test splits (in order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui usamos trains_test_split do sklearn para separar os conjuntos de treino e teste de forma randômica e sistematizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train 1387 \n",
      " x_test 347 \n",
      " total atos 1734\n"
     ]
    }
   ],
   "source": [
    "# 80% treino, 20% teste\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=None, shuffle=False)\n",
    "#random_state: the seed number to be passed to the shuffle operation, thus making the experiment reproducible.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(atos, labels, test_size=0.2, random_state=None, shuffle=True)\n",
    "print( 'x_train',len(x_train),'\\n','x_test',len(x_test),'\\n','total atos',len(atos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83919"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cria uma Lista com o x_test\n",
    "x_test_list = []\n",
    "for i in x_test:\n",
    "   if not isinstance(i, list):\n",
    "      x_test_list.append(i)\n",
    "   else:\n",
    "      for j in i:\n",
    "        x_test_list.append(j)\n",
    "\n",
    "len(x_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83919"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_list = []\n",
    "for i in y_test:\n",
    "   if not isinstance(i, list):\n",
    "      y_test_list.append(i)\n",
    "   else:\n",
    "      for j in i:\n",
    "       y_test_list.append(j)\n",
    "\n",
    "len(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insere um espaço entre as entidade e :\n",
    "import re\n",
    "def correct_space_before_numeric_entities(string):\n",
    "    result = re.sub(r'[0-9].[\\s](?=[0-9])', r'[0-9].(?=[0-9])', string) \n",
    "    result = result.replace(\"\\n\", \" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test_texto =  \" \".join(\" \".join(l) for l in x_test_s)   \n",
    "#x_test_texto =  \"\".join(str(l) for l in x_test_s)\n",
    "x_test_texto = list(map(' '.join, x_test))         \n",
    "#x_test_texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODIGO RESPONSAVEL PELA LIMPEZA DO TEXTO DA BASE OURO\n",
    "import re\n",
    "\n",
    "df1 =[]\n",
    "\n",
    "for i in x_test_texto:\n",
    "    i = i.replace(\"\\n\", \" \")\n",
    "    i = re.sub('xx[a-z]{1,10}', '', i)\n",
    "    #i = i.replace(r\"xx[a-z]{1,10}\",\"\")\n",
    "    aux = ' '.join(i.split())\n",
    "    #print(aux)\n",
    "    df1.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset do x_test em texto para rodar o supervisao fraca\n",
    "df = pd.DataFrame(x_test_texto,columns=['Texto']) \n",
    "df.to_csv('./CSVs/V2/Extrato_Contrato_Split_4_x_test_texto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Split Test to CSV file\n",
    "# labels para validação do x_test\n",
    "#df = pd.DataFrame(x_test_list, columns=['Texto']) \n",
    "#df.to_csv('./CSVs/Extrato_Contrato_Split_1_x_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./CSVs/Extrato_Contrato_Split_1_x_test.txt', 'wt') as fileout:\n",
    "#    for item in x_test_s:\n",
    "#        fileout.write(str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./CSVs/V2/Extrato_Contrato_Split_4_x_test.txt', 'wb') as file:\n",
    "    pickle.dump(x_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./CSVs/V2/Extrato_Contrato_Split_4_y_test.txt', 'wb') as file:\n",
    "    pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Split Test to CSV file\n",
    "# labels para validação do x_test\n",
    "#df = pd.DataFrame(y_test_list, columns=['Texto']) \n",
    "#df.to_csv('./CSVs/Extrato_Contrato_Split_1_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./CSVs/Extrato_Contrato_Split_1_y_test.txt', 'wt') as fileout:\n",
    "#    for item in y_test_s:\n",
    "#        fileout.write(str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train 1240 \n",
      " x_test 311 \n",
      " total atos 1551\n"
     ]
    }
   ],
   "source": [
    "#Gambiarra para recuperar o result do crf\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(atos, labels, test_size=0.2, random_state=None, shuffle=True)\n",
    "print( 'x_train',len(x_train),'\\n','x_test',len(x_test),'\\n','total atos',len(atos))\n",
    "\n",
    "import pickle\n",
    "with open(\"./CSVs/V2/Aditamento_Contrato_Split_4_x_test.txt\", \"rb\") as file:\n",
    "    x_test = pickle.load(file)\n",
    "    \n",
    "with open(\"./CSVs/V2/Aditamento_Contrato_Split_4_y_test.txt\", \"rb\") as file:    \n",
    "    y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary feature for each word in each sequence in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train\n",
    "\n",
    "def get_features(sentence):\n",
    "        \"\"\"Create features for each word in act.\n",
    "        Create a list of dict of words features to be used in the predictor module.\n",
    "        Args:\n",
    "            act (list): List of words in an act.\n",
    "        Returns:\n",
    "            A list with a dictionary of features for each of the words.\n",
    "        \"\"\"\n",
    "        sent_features = []\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            word_feat = {\n",
    "                'word': sentence[i].lower(),\n",
    "                'capital_letter': sentence[i][0].isupper(),\n",
    "                'all_capital': sentence[i].isupper(),\n",
    "                'isdigit': sentence[i].isdigit(),\n",
    "                'word_before': sentence[i].lower() if i == 0 else sentence[i-1].lower(),\n",
    "                'word_after:': sentence[i].lower() if i+1 >= len(sentence) else sentence[i+1].lower(),\n",
    "                'BOS': i == 0,\n",
    "                'EOS': i == len(sentence)-1\n",
    "            }\n",
    "            sent_features.append(word_feat)\n",
    "        return sent_features\n",
    "    \n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = get_features(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test\n",
    "def get_features(sentence):\n",
    "        \"\"\"Create features for each word in act.\n",
    "        Create a list of dict of words features to be used in the predictor module.\n",
    "        Args:\n",
    "            act (list): List of words in an act.\n",
    "        Returns:\n",
    "            A list with a dictionary of features for each of the words.\n",
    "        \"\"\"\n",
    "        x_test = []\n",
    "        sent_features = []\n",
    "        for i in range(len(sentence)):\n",
    "            word_feat = {\n",
    "                'word': sentence[i].lower(),\n",
    "                'capital_letter': sentence[i][0].isupper(),\n",
    "                'all_capital': sentence[i].isupper(),\n",
    "                'isdigit': sentence[i].isdigit(),\n",
    "                'word_before': sentence[i].lower() if i == 0 else sentence[i-1].lower(),\n",
    "                'word_after:': sentence[i].lower() if i+1 >= len(sentence) else sentence[i+1].lower(),\n",
    "                'BOS': i == 0,\n",
    "                'EOS': i == len(sentence)-1\n",
    "            }\n",
    "            sent_features.append(word_feat)\n",
    "        return sent_features\n",
    "    \n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = get_features(x_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CRF Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=10,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    #max_iterations=50,\n",
    "    \n",
    "    all_possible_transitions=False,\n",
    "    all_possible_states=True\n",
    ")\n",
    "\n",
    "#crf.fit(x_train, y_train)\n",
    "\n",
    "try:\n",
    "    #crf.fit(x_train[:200], y_train[:200])\n",
    "    crf.fit(x_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crf_model_extrato_contrato.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(crf, 'crf_model_extrato_contrato.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucelia/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8817771695107018"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = list(crf.classes_)\n",
    "classes.remove('O')\n",
    "\n",
    "y_pred = crf.predict(x_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                codigo_siggo       0.00      0.00      0.00        32\n",
      "                data_escrito       0.67      0.66      0.67       301\n",
      "            nome_responsavel       1.00      0.43      0.60        14\n",
      "             numero_contrato       0.79      0.63      0.70       380\n",
      "        numero_termo_aditivo       0.94      0.91      0.92       332\n",
      "objeto_aditamento_contratual       0.54      0.46      0.50       305\n",
      "           orgao_contratante       0.84      0.61      0.71       374\n",
      "                processo_gdf       0.84      0.43      0.57       289\n",
      "\n",
      "                   micro avg       0.77      0.61      0.68      2027\n",
      "                   macro avg       0.70      0.52      0.58      2027\n",
      "                weighted avg       0.76      0.61      0.67      2027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucelia/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.scheme import IOB1\n",
    "from seqeval.scheme import IOBES\n",
    "\n",
    "#classification_report(labels_skweak, gold_y_test,  mode='strict', scheme=IOB2)\n",
    "#\n",
    "#for item in classes:\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=False, mode='strict', scheme=IOB2)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'codigo_siggo': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 30}, 'data_escrito': {'precision': 0.6602564102564102, 'recall': 0.7152777777777778, 'f1-score': 0.6866666666666666, 'support': 288}, 'nome_responsavel': {'precision': 1.0, 'recall': 0.7, 'f1-score': 0.8235294117647058, 'support': 20}, 'numero_contrato': {'precision': 0.7745098039215687, 'recall': 0.6638655462184874, 'f1-score': 0.7149321266968326, 'support': 357}, 'numero_termo_aditivo': {'precision': 0.9487179487179487, 'recall': 0.9051987767584098, 'f1-score': 0.9264475743348982, 'support': 327}, 'objeto_aditamento_contratual': {'precision': 0.5652173913043478, 'recall': 0.47039473684210525, 'f1-score': 0.5134649910233393, 'support': 304}, 'orgao_contratante': {'precision': 0.8876811594202898, 'recall': 0.6234096692111959, 'f1-score': 0.7324364723467862, 'support': 393}, 'processo_gdf': {'precision': 0.8695652173913043, 'recall': 0.4844290657439446, 'f1-score': 0.6222222222222222, 'support': 289}, 'micro avg': {'precision': 0.7839657282741738, 'recall': 0.6379482071713147, 'f1-score': 0.7034596375617791, 'support': 2008}, 'macro avg': {'precision': 0.7132434913764837, 'recall': 0.5703219465689902, 'f1-score': 0.6274624331319314, 'support': 2008}, 'weighted avg': {'precision': 0.7813116264040826, 'recall': 0.6379482071713147, 'f1-score': 0.6953051929278782, 'support': 2008}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucelia/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.scheme import IOB1\n",
    "from seqeval.scheme import IOBES\n",
    "\n",
    "#classification_report(labels_skweak, gold_y_test,  mode='strict', scheme=IOB2)\n",
    "#\n",
    "#for item in classes:\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True, mode='strict', scheme=IOB2)\n",
    "print(report)\n",
    "np.save(\"./Results/V2/METRICA_ADITAMENTO_CONTRATO_4.npy\", report)\n",
    "#np.save(\"./Results/V2/METRICA_ANUL_REVOG_LICITACAO_4.npy\", report)\n",
    "#np.save(\"./Results/V2/METRICA_AVISO_LICITACAO_4.npy\", report)\n",
    "#np.save(\"./Results/V2/METRICA_EXTRATO_CONTRATO_4.npy\", report)\n",
    "#np.save(\"./Results/V2/METRICA_EXTRATO_CONVENIO_4.npy\", report)\n",
    "#np.save(\"./Results/V2/METRICA_SUSPENSAO_LICITACAO_4.npy\", report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' report = metrics.flat_classification_report(\\n    #y_test[:200], y_pred[:200], labels=classes, digits=3))\\n   y_test, y_pred, labels=classes, digits=3, output_dict=False)\\nprint(report) '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' report = metrics.flat_classification_report(\n",
    "    #y_test[:200], y_pred[:200], labels=classes, digits=3))\n",
    "   y_test, y_pred, labels=classes, digits=3, output_dict=False)\n",
    "print(report) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' report = metrics.flat_classification_report(\\n#report = sklearn.metrics.classification_report(\\n    #y_test[:200], y_pred[:200], labels=classes, digits=3))\\n    y_test_s, y_pred, labels=classes, digits=3, output_dict=True)\\nprint(report)\\nnp.save(\"./Results/METRICA_EXTRATO_CONTRATO_V1_0.npy\", report) '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' report = metrics.flat_classification_report(\n",
    "#report = sklearn.metrics.classification_report(\n",
    "    #y_test[:200], y_pred[:200], labels=classes, digits=3))\n",
    "    y_test_s, y_pred, labels=classes, digits=3, output_dict=True)\n",
    "print(report)\n",
    "np.save(\"./Results/METRICA_EXTRATO_CONTRATO_V1_0.npy\", report) '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricas = (np.load(f\"./Results/V1/METRICA_ADITAMENTO_CONTRATO_0.npy\",allow_pickle=True)).tolist()\n",
    "#metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "##confusion_matrix(y_test, y_pred, labels=classes)\n",
    "#classes = list(crf.classes_)\n",
    "#classes.remove('I-numero_contrato')\n",
    "#classes.remove('I-processo_gdf')\n",
    "#classes.remove('B-11')    \n",
    "#classes.remove('B-12')\n",
    "#classes.remove('I-valor_estimado_contratacao')\n",
    "#cm = confusion_matrix(y_test_list, y_pred_list, labels=classes)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "#disp.plot() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'{classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CRF' object has no attribute 'keep_tempfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [66], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# search\u001b[39;00m\n\u001b[1;32m     19\u001b[0m rs \u001b[39m=\u001b[39m RandomizedSearchCV(crf, params_space,\n\u001b[1;32m     20\u001b[0m                         cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     21\u001b[0m                         verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     22\u001b[0m                         n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     23\u001b[0m                         n_iter\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     24\u001b[0m                         scoring\u001b[39m=\u001b[39mf1_scorer)\n\u001b[0;32m---> 25\u001b[0m rs\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n",
      "File \u001b[0;32m~/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:789\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    786\u001b[0m cv_orig \u001b[39m=\u001b[39m check_cv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    787\u001b[0m n_splits \u001b[39m=\u001b[39m cv_orig\u001b[39m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 789\u001b[0m base_estimator \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator)\n\u001b[1;32m    791\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs, pre_dispatch\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_dispatch)\n\u001b[1;32m    793\u001b[0m fit_and_score_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    794\u001b[0m     scorer\u001b[39m=\u001b[39mscorers,\n\u001b[1;32m    795\u001b[0m     fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    802\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/sklearn/base.py:85\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[1;32m     82\u001b[0m             )\n\u001b[1;32m     84\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m---> 85\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/experiments/members/lucelia/venv/lib/python3.9/site-packages/sklearn/base.py:211\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    209\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    210\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 211\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, key)\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n\u001b[1;32m    213\u001b[0m         deep_items \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mget_params()\u001b[39m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'keep_tempfiles'"
     ]
    }
   ],
   "source": [
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=15.0),\n",
    "    'c2': scipy.stats.expon(scale=1.0),\n",
    "}\n",
    "\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=classes)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbest params:\u001b[39m\u001b[39m'\u001b[39m, rs\u001b[39m.\u001b[39;49mbest_params_)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbest CV score:\u001b[39m\u001b[39m'\u001b[39m, rs\u001b[39m.\u001b[39mbest_score_)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel size: \u001b[39m\u001b[39m{:0.2f}\u001b[39;00m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(rs\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39msize_ \u001b[39m/\u001b[39m \u001b[39m1000000\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best estimator on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_classes = sorted(\n",
    "    classes,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = list(crf.classes_)\n",
    "classes.remove('O')\n",
    "\n",
    "y_pred = crf.predict(x_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucelia_vieira/experiments/members/lucelia/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['B-cnpj_entidade_contratada', 'I-cnpj_entidade_contratada', 'B-cnpj_orgao_contratante', 'I-cnpj_orgao_contratante', 'B-codigo_siggo', 'I-codigo_siggo', 'B-data_assinatura_contrato', 'I-data_assinatura_contrato', 'B-entidade_contratada', 'I-entidade_contratada', 'B-fonte_recurso', 'I-fonte_recurso', 'B-natureza_despesa', 'I-natureza_despesa', 'B-nome_responsavel', 'I-nome_responsavel', 'B-nota_empenho', 'I-nota_empenho', 'B-numero_contrato', 'I-numero_contrato', 'B-objeto_contrato', 'I-objeto_contrato', 'B-orgao_contratante', 'I-orgao_contratante', 'B-processo_gdf', 'I-processo_gdf', 'B-programa_trabalho', 'I-programa_trabalho', 'B-unidade_orcamentaria', 'I-unidade_orcamentaria', 'B-valor_contrato', 'I-valor_contrato', 'B-vigencia_contrato', 'I-vigencia_contrato'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "B-cnpj_entidade_contratada      0.982     0.775     0.866        71\n",
      "I-cnpj_entidade_contratada      0.982     0.764     0.859        72\n",
      "  B-cnpj_orgao_contratante      1.000     0.750     0.857        16\n",
      "  I-cnpj_orgao_contratante      1.000     0.750     0.857        16\n",
      "            B-codigo_siggo      0.750     0.667     0.706        27\n",
      "            I-codigo_siggo      0.000     0.000     0.000         2\n",
      "B-data_assinatura_contrato      0.671     0.818     0.737       192\n",
      "I-data_assinatura_contrato      1.000     0.755     0.860        53\n",
      "     B-entidade_contratada      0.926     0.841     0.881       314\n",
      "     I-entidade_contratada      0.918     0.849     0.882      1458\n",
      "           B-fonte_recurso      0.959     0.913     0.935       253\n",
      "           I-fonte_recurso      1.000     1.000     1.000         5\n",
      "        B-natureza_despesa      0.988     0.816     0.894       207\n",
      "        I-natureza_despesa      0.000     0.000     0.000         1\n",
      "        B-nome_responsavel      0.944     0.919     0.932        37\n",
      "        I-nome_responsavel      0.951     0.918     0.934        85\n",
      "            B-nota_empenho      0.938     0.794     0.860       247\n",
      "            I-nota_empenho      0.000     0.000     0.000         2\n",
      "         B-numero_contrato      0.863     0.884     0.874       336\n",
      "         I-numero_contrato      1.000     0.643     0.783        14\n",
      "         B-objeto_contrato      0.953     0.950     0.952       300\n",
      "         I-objeto_contrato      0.919     0.977     0.947     13803\n",
      "       B-orgao_contratante      0.885     0.862     0.873       456\n",
      "       I-orgao_contratante      0.936     0.946     0.941      2213\n",
      "            B-processo_gdf      0.931     0.900     0.915       300\n",
      "            I-processo_gdf      0.922     0.915     0.919       260\n",
      "       B-programa_trabalho      0.778     0.771     0.775       214\n",
      "       I-programa_trabalho      1.000     0.500     0.667         2\n",
      "    B-unidade_orcamentaria      0.928     0.902     0.915       143\n",
      "    I-unidade_orcamentaria      0.000     0.000     0.000         4\n",
      "          B-valor_contrato      0.818     0.897     0.856       370\n",
      "          I-valor_contrato      0.000     0.000     0.000         0\n",
      "       B-vigencia_contrato      0.972     0.949     0.961       295\n",
      "       I-vigencia_contrato      0.977     0.958     0.968      4104\n",
      "\n",
      "                 micro avg      0.926     0.946     0.936     25872\n",
      "                 macro avg      0.791     0.717     0.747     25872\n",
      "              weighted avg      0.926     0.946     0.935     25872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(x_test_s)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_s, y_pred, labels=sorted_classes, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-cnpj_entidade_contratada -> I-cnpj_entidade_contratada 5.844630\n",
      "I-objeto_contrato -> I-objeto_contrato 5.793420\n",
      "I-vigencia_contrato -> I-vigencia_contrato 5.734659\n",
      "B-cnpj_orgao_contratante -> I-cnpj_orgao_contratante 5.503859\n",
      "O      -> O       5.465237\n",
      "B-processo_gdf -> I-processo_gdf 5.402388\n",
      "I-nome_responsavel -> I-nome_responsavel 4.835842\n",
      "I-entidade_contratada -> I-entidade_contratada 4.820523\n",
      "I-valor_contrato -> I-valor_contrato 4.789618\n",
      "I-orgao_contratante -> I-orgao_contratante 4.489032\n",
      "I-processo_gdf -> I-processo_gdf 4.356173\n",
      "I-data_assinatura_contrato -> I-data_assinatura_contrato 4.312161\n",
      "B-nome_responsavel -> I-nome_responsavel 4.257360\n",
      "I-numero_contrato -> I-numero_contrato 3.983543\n",
      "B-entidade_contratada -> I-entidade_contratada 3.815881\n",
      "B-programa_trabalho -> I-programa_trabalho 3.759227\n",
      "B-vigencia_contrato -> I-vigencia_contrato 3.665918\n",
      "B-data_assinatura_contrato -> I-data_assinatura_contrato 3.640761\n",
      "I-fonte_recurso -> I-fonte_recurso 3.491157\n",
      "B-fonte_recurso -> I-fonte_recurso 3.239678\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-entidade_contratada -> O       -2.018321\n",
      "B-objeto_contrato -> I-vigencia_contrato -2.062336\n",
      "B-vigencia_contrato -> I-objeto_contrato -2.070239\n",
      "B-objeto_contrato -> O       -2.138108\n",
      "B-vigencia_contrato -> O       -2.175822\n",
      "B-entidade_contratada -> I-orgao_contratante -2.281935\n",
      "O      -> I-nome_responsavel -2.308491\n",
      "I-vigencia_contrato -> I-objeto_contrato -2.345984\n",
      "O      -> I-processo_gdf -2.395983\n",
      "I-objeto_contrato -> I-vigencia_contrato -2.404485\n",
      "I-entidade_contratada -> I-objeto_contrato -2.420813\n",
      "O      -> I-data_assinatura_contrato -2.490981\n",
      "I-entidade_contratada -> I-orgao_contratante -2.556087\n",
      "O      -> I-cnpj_entidade_contratada -2.606924\n",
      "I-objeto_contrato -> I-orgao_contratante -2.692590\n",
      "I-orgao_contratante -> I-entidade_contratada -3.300981\n",
      "O      -> I-entidade_contratada -3.552701\n",
      "O      -> I-vigencia_contrato -3.818855\n",
      "O      -> I-orgao_contratante -4.447185\n",
      "O      -> I-objeto_contrato -4.846350\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "6.620938 B-processo_gdf word_before:processo\n",
      "6.571771 O        EOS\n",
      "5.481333 B-nota_empenho all_capital\n",
      "5.433425 B-programa_trabalho word_before:trabalho\n",
      "5.192110 O        word:14202\n",
      "5.157273 O        word:140202\n",
      "5.063219 B-natureza_despesa word:33.90.39\n",
      "5.030145 B-fonte_recurso word:100\n",
      "4.751505 O        word::\n",
      "4.706548 B-valor_contrato word_before:r$\n",
      "4.641312 B-natureza_despesa word:33.90.30\n",
      "4.606614 O        word:400091\n",
      "4.582759 B-natureza_despesa word:44.90.52\n",
      "4.558941 B-fonte_recurso word:220000000\n",
      "4.500370 B-codigo_siggo isdigit\n",
      "4.488277 B-natureza_despesa word:339039\n",
      "4.371495 O        word:x\n",
      "4.311440 B-entidade_contratada word_before:x\n",
      "4.305492 B-orgao_contratante word:brb\n",
      "4.295000 B-unidade_orcamentaria word:44.101\n",
      "4.285224 B-natureza_despesa word:3.3.90.30\n",
      "4.149354 B-natureza_despesa word_before:despesa\n",
      "4.041723 B-fonte_recurso isdigit\n",
      "3.981875 B-programa_trabalho word:10122620361957\n",
      "3.978987 B-natureza_despesa word:3.3.90.39\n",
      "3.970536 B-unidade_orcamentaria word:34.101\n",
      "3.945643 B-nota_empenho word_before:empenho\n",
      "3.909396 B-numero_contrato word_before:no\n",
      "3.899204 B-nome_responsavel word_before:.\n",
      "3.869008 B-programa_trabalho word:28.845.0903.00nr.0053\n",
      "\n",
      "Top negative:\n",
      "-2.225095 O        word:33903302\n",
      "-2.250713 B-unidade_orcamentaria capital_letter\n",
      "-2.285258 B-processo_gdf word:nº\n",
      "-2.300773 I-cnpj_entidade_contratada all_capital\n",
      "-2.336295 B-programa_trabalho word::\n",
      "-2.351183 I-objeto_contrato word_before:.\n",
      "-2.353090 B-codigo_siggo word_after::,\n",
      "-2.356512 I-cnpj_entidade_contratada word:)\n",
      "-2.461067 I-entidade_contratada word_before::\n",
      "-2.495780 O        word:caesb\n",
      "-2.498939 I-cnpj_entidade_contratada word:.\n",
      "-2.533966 I-processo_gdf word:-\n",
      "-2.548124 B-nota_empenho word:28845090300fm0053\n",
      "-2.591957 B-processo_gdf word:no\n",
      "-2.617366 O        word:brb\n",
      "-2.621137 B-nota_empenho word:28.845.0903.00nr.0053\n",
      "-2.631431 O        word_before:x\n",
      "-2.689485 B-natureza_despesa capital_letter\n",
      "-2.698511 B-numero_contrato word:23.287/2002\n",
      "-2.711627 I-vigencia_contrato word_before:.\n",
      "-2.758247 O        word_after::.\n",
      "-2.852111 O        word_after::;\n",
      "-2.923364 O        word_before::\n",
      "-2.949363 I-entidade_contratada word:,\n",
      "-3.476487 B-processo_gdf capital_letter\n",
      "-3.529142 B-data_assinatura_contrato capital_letter\n",
      "-3.576788 B-processo_gdf word::\n",
      "-3.852196 B-programa_trabalho capital_letter\n",
      "-4.323682 I-processo_gdf word:.\n",
      "-6.176913 B-nota_empenho capital_letter\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crf_model.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(crf, 'crf_model_aditamento.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.pklcrf_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pklcrf_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiments/members/lucelia/.venv/lib/python3.8/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.pklcrf_model'"
     ]
    }
   ],
   "source": [
    "model = joblib.load('.pklcrf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96baed6db0003f61f4a930eafeae8ace7abdbffdf58b8c787c6463efd11f449a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
