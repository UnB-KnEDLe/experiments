{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use CoNLL 2003 data to build a NER system\n",
    "\n",
    "We use English data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 1 - Treina o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conll2003\n",
    "with open('/home/82068895153/POS/skweak/data/conll2003_dataset/train.txt', 'r') as file:\n",
    "  sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sentences[5])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in sentences[5:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quebra a sentença em lista\n",
    "sentences_1=preprocess(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    #print ('word', word)\n",
    "    postag = sent[i][1]\n",
    "    #print ('postag', postag)\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],        \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]        \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, __, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, __, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in sentences_1]\n",
    "\n",
    "y_train = [sent2labels(s) for s in sentences_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 2 - Prepara o Y_test a partir do dataset do Ontonotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 - Abre o Ontonotes para aplicar o tratamento \n",
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train.txt', 'r') as file:\n",
    "with open('/Users/lucelia/POS/experimentos/Dataset/Ontonotes/ner_train.txt', 'r') as file:\n",
    "#dataset com 1000 linhas para teste\n",
    "#with open('/Users/lucelia/POS/experimentos/Dataset/Ontonotes/ner_train_lu.txt', 'r') as file:\n",
    "   sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160787"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.2 - Retira os espaços em branco e as words maiores que duas posições\n",
    "for i in range(len(sentences) - 1):\n",
    "    #print(sentences[116:122])\n",
    "    atual = sentences[i].split()\n",
    "    proximo = sentences[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        sentences[i] = '\\t'.join(atual)+'\\n'\n",
    "        proximo = proximo[1:]\n",
    "        sentences[i+1] = '\\t'.join(proximo)+'\\n'\n",
    "        #print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160787"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.3 - Verifica as linhas com mais de duas words e concatena\n",
    "for i in range(len(sentences) - 1):\n",
    "        atual = sentences[i].split()\n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]))+' '+atual[2]+'\\n' \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] +'\\n'  \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4] +'\\n' \n",
    "            #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[121:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160787"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7fe4510365e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4 - carrega a função para detecção da língua\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "#na primeira execução descomentar essa linha\n",
    "@Language.factory(\"language_detector\")\n",
    "def get_lang_detector(nlp, name):\n",
    "   return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "#print(nlp('场')._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 - cria a função que detecta linha a linha a língua e remove tudo que não for Inglês\n",
    "def limpaOntonotes(linhas):\n",
    "    #count=0\n",
    "    lista = []\n",
    "    for line in linhas:\n",
    "        line1=line.split('\\t')\n",
    "        if len(line1)>1:\n",
    "            result = nlp(line1[0])\n",
    "            #print(line1[0])\n",
    "            #print(line1[1])\n",
    "            result1 = nlp(line1[1])\n",
    "        \n",
    "            #print(result)\n",
    "            if((result._.language) or (result1._.language)):\n",
    "                #print(result._.language)\n",
    "                result=(result._.language['language'])\n",
    "                result1=(result1._.language['language'])\n",
    "                #print('line', line)\n",
    "                #print('result', result)\n",
    "                if (result or result1) not in ['ko','zh-cn','ar','fa','zh-tw','fa', 'es','sw','id']:\n",
    "                #if result not in lang:\n",
    "                    #print('lista', lista) \n",
    "                    lista.append(line)\n",
    "                    #print(lista)\n",
    "        if (len(line1)==1):\n",
    "            #print('entrou no if')\n",
    "            lista.append('\\n')\n",
    "        #count+=1\n",
    "        #print(count)    \n",
    "    return lista  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang = ['af' , 'ar' , 'bg' , 'bn' , 'ca' , 'cs' , 'cy' , 'da' , 'de' , 'el' , 'es' , 'et' , 'fa' , 'fi' , 'fr' , 'gu' , 'he' , 'hi' , 'hr' , 'hu' , 'id' , 'it' , 'ja' , 'kn' , 'ko' , 'lt' , 'lv' , 'mk' , 'ml' , 'mr' , 'ne' , 'nl' , 'no' , 'pa' , 'pl' , 'pt' , 'ro' , 'ru' , 'sk' , 'sl' , ' sq' , 'sv' , 'sw' , 'ta' , 'te' , 'th' , 'tl' , 'tr' , 'uk' , 'ur' , 'vi' , 'zh-cn' , 'zh-tw']\n",
    "#lang = ['ar', 'fa', 'es' ,'id',  'ko' , 'sw' , 'zh-cn', 'zh-tw']\n",
    "\n",
    "\n",
    "['af'  , 'bg' , 'bn' , 'ca' , 'cs' , 'cy' , 'da' , 'de' , 'el', 'et' , 'fi' , 'fr' , 'gu' , 'he' , 'hi' , 'hr' , 'hu' , 'it' , 'ja' , 'kn' , 'lt' , 'lv' , 'mk' , 'ml' , 'mr' , 'ne' , 'nl' , 'no' , 'pa' , 'pl' , 'pt' , 'ro' , 'ru' , 'sk' , 'sl' , ' sq' , 'sv' , 'ta' , 'te' , 'th' , 'tl' , 'tr' , 'uk' , 'ur' , 'vi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#lista = limpaOntonotes(['مسيرت','并不','高子平','告诉','记者','这些','收入','对','مريم','العذراء','预报','uh-huh','Good', 'school'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#lista_1 = limpaOntonotes(['他','还','强调','现在'])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#lista_1 = limpaOntonotes(['他','还','强调','，','这些','并不','意味','着','将','有','一','场','战争'])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#2.6 Chama a função de detecção da língua\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m lista_1\u001b[38;5;241m=\u001b[39m\u001b[43mlimpaOntonotes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mlimpaOntonotes\u001b[0;34m(linhas)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m((result\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mlanguage) \u001b[38;5;129;01mor\u001b[39;00m (result1\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mlanguage)):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#print(result._.language)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     result\u001b[38;5;241m=\u001b[39m(result\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mlanguage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m     result1\u001b[38;5;241m=\u001b[39m(\u001b[43mresult1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print('line', line)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#print('result', result)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (result \u001b[38;5;129;01mor\u001b[39;00m result1) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mko\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh-cn\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mar\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfa\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh-tw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#if result not in lang:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m#print('lista', lista) \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/spacy/tokens/underscore.py:50\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     48\u001b[0m default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     method_partial \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/spacy_langdetect/spacy_langdetect.py:8\u001b[0m, in \u001b[0;36m_detect_language\u001b[0;34m(spacy_object)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_detect_language\u001b[39m(spacy_object):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m         detected_language \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_langs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspacy_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(detected_language\u001b[38;5;241m.\u001b[39mlang), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(detected_language\u001b[38;5;241m.\u001b[39mprob)}\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m LangDetectException:\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/langdetect/detector_factory.py:137\u001b[0m, in \u001b[0;36mdetect_langs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    135\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m    136\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/langdetect/detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob)\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/langdetect/detector.py:161\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_lang_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_prob(prob) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONV_THRESHOLD \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mITERATION_LIMIT:\n",
      "File \u001b[0;32m~/anaconda3/envs/pos/lib/python3.9/site-packages/langdetect/detector.py:212\u001b[0m, in \u001b[0;36mDetector._update_lang_prob\u001b[0;34m(self, prob, word, alpha)\u001b[0m\n\u001b[1;32m    210\u001b[0m weight \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_FREQ\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m xrange(\u001b[38;5;28mlen\u001b[39m(prob)):\n\u001b[0;32m--> 212\u001b[0m     prob[i] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m+\u001b[39m \u001b[43mlang_prob_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#lista = limpaOntonotes(['مسيرت','并不','高子平','告诉','记者','这些','收入','对','مريم','العذراء','预报','uh-huh','Good', 'school'])\n",
    "#lista_1 = limpaOntonotes(['他','还','强调','现在'])\n",
    "#lista_1 = limpaOntonotes(['他','还','强调','，','这些','并不','意味','着','将','有','一','场','战争'])\n",
    "\n",
    "#2.6 Chama a função de detecção da língua\n",
    "lista_1=limpaOntonotes(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160787"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['还有\\tO\\n', '那些\\tO\\n', '鼓噪\\tO\\n', '回汉冲突\\tGPE\\n', '的\\tO\\n', '人\\tO\\n', '是片面的\\tO\\n', '，\\tO\\n', '他们\\tO\\n']\n"
     ]
    }
   ],
   "source": [
    "print (txt[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [124]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences, labels\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#print('Sentenças e labels separados')\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m sentences, labels \u001b[38;5;241m=\u001b[39m \u001b[43mlines_to_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#print(sentences)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#print(labels)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#         self.l = not self.l\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#         return language\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_sentences\u001b[39m(sentences, labels):\n",
      "Input \u001b[0;32mIn [124]\u001b[0m, in \u001b[0;36mlines_to_sentences\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     22\u001b[0m     label \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#print('entrou no for --> sentences, labels', sentences, labels)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     word, word_label \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     sentence\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m     27\u001b[0m     label\u001b[38;5;241m.\u001b[39mappend(word_label)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#encoding: utf-8\n",
    "#txt = ['A\\tO\\n', 'House\\tO\\n', '.\\tO\\n', '\\n', 'Pedro\\tPER\\n', '.\\tO\\n', '\\n', 'Doing\\tVERB\\n', 'something\\tO\\n', '\\n']\n",
    "\n",
    "def list_of_lists_to_list_of_strings(ls):\n",
    "    return [' '.join(l) for l in ls]\n",
    "\n",
    "def lines_to_sentences(lines):\n",
    "    \"\"\"\n",
    "    Só funciona se último elemento de lines for '\\n',\n",
    "    se não for assim no seu caso adicione um no final\n",
    "    caso contrário vai pular a última sentença\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            sentence = []\n",
    "            label = []\n",
    "            print('entrou no for --> sentences, labels', sentences, labels)\n",
    "        else:\n",
    "            word, word_label = line.strip('\\n').split('\\t')\n",
    "            sentence.append(word)\n",
    "            label.append(word_label)\n",
    "    \n",
    "    sentences = list_of_lists_to_list_of_strings(sentences)\n",
    "    labels = list_of_lists_to_list_of_strings(labels)\n",
    "    return sentences, labels\n",
    "\n",
    "#print('Sentenças e labels separados')\n",
    "sentences, labels = lines_to_sentences(txt)\n",
    "#print(sentences)\n",
    "#print(labels)\n",
    "\n",
    "# class LanguageDetector:\n",
    "#     \"\"\"Alterna a linguagem detectada só pra teste\"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.l = True\n",
    "\n",
    "#     def lang(self, sentence):\n",
    "#         if self.l:\n",
    "#             language = 'en'\n",
    "            \n",
    "#         else:\n",
    "#             language = 'ko'\n",
    "#         self.l = not self.l\n",
    "#         return language\n",
    "\n",
    "def filter_sentences(sentences, labels):\n",
    "    \"\"\"\n",
    "    filtra pela linguagem\n",
    "    \"\"\"\n",
    "    filtered_sentences = []\n",
    "    filtered_labels = []\n",
    "    #ld = LanguageDetector()\n",
    "\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        #if ld.lang(sentence) == 'en':\n",
    "        result = nlp(sentence)\n",
    "        \n",
    "        if result._.language['language'] == 'en':\n",
    "            print('sentenca', sentence, 'label',label, 'result',  result._.language['language'])\n",
    "            filtered_sentences.append(sentence)\n",
    "            filtered_labels.append(label)\n",
    "        \n",
    "    return filtered_sentences, filtered_labels\n",
    "\n",
    "\n",
    "def sentences_to_lines(sentences, sentence_labels):\n",
    "    \"\"\"\n",
    "    Contrário de lines_to_sentences\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for sentence, labels in zip(sentences, sentence_labels):\n",
    "        words = sentence.split(' ')\n",
    "        word_labels = labels.split(' ')\n",
    "        for word, label in zip(words, word_labels):\n",
    "            # line = f'{word}\\t{label}\\n'\n",
    "            line = word + '\\t' + label + '\\n'\n",
    "            lines.append(line)\n",
    "        lines.append('\\n')\n",
    "    return lines\n",
    "\n",
    "#print('Sentenças e labels filtrados pela linguagem')\n",
    "filtered_sentences, filtered_labels = filter_sentences(sentences, labels)\n",
    "#print(filtered_sentences)\n",
    "#print(filtered_labels)\n",
    "\n",
    "#print('Linhas após filtragem')\n",
    "filtered_lines = sentences_to_lines(filtered_sentences, filtered_labels)\n",
    "#print(filtered_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outra fução para detecçõ da lingua, usei a anterior. \n",
    "\n",
    "#detect(\"War doesn't show who's right, just who's left.\")\n",
    "\n",
    "def limpaOntonotes_1(linhas):\n",
    "    from langdetect import detect\n",
    "    lista = []\n",
    "    for line in linhas[0:500]:\n",
    "# the try except blook because there is some tweets contain links\n",
    "        if not line: #verifica se a linha está vazia\n",
    "            #print('line', line)\n",
    "            result = detect(line)\n",
    "            #print(result)\n",
    "            if result not in lang:\n",
    "                #print('lista', lista)\n",
    "                lista.append(line)\n",
    "            #print(lista)\n",
    "        else:\n",
    "            lista.append(line)\n",
    "    return lista  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpaOntonotes(['他','还','强调','现在','Good Morning'])\n",
    "#lista_1=limpaOntonotes_1(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gravação de teste\n",
    "#with open('/Users/lucelia/POS/experimentos/Dataset/Ontonotes/ner_train_lista.txt', 'wt') as fileout:\n",
    "#    fileout.writelines(lista_1)\n",
    "\n",
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_setences.txt', 'wt') as fileout:\n",
    "\n",
    "with open('/Users/lucelia/POS/experimentos/Dataset/Ontonotes/ner_train_setences_test.txt', 'wt') as fileout:\n",
    "    fileout.writelines(filtered_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_setences.txt', 'r') as file:\n",
    "with open('/Users/lucelia/POS/experimentos/Dataset/Ontonotes/ner_train_setences.txt', 'r') as file:\n",
    "    sentences_trat = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troca(linhas, termo, termo_cabeca, termo_corpo):\n",
    "    newlines = []\n",
    "    is_in_body = False\n",
    "    for line in linhas:\n",
    "        if termo in line:\n",
    "            if not is_in_body:\n",
    "                #retira \\n e insere termo depois\n",
    "                word = line.split()\n",
    "                newLine = word[0] + ' ' + termo_cabeca + '\\n' \n",
    "                #print('PRIMEIRA OCORRENCIA')\n",
    "                #print('line',line)\n",
    "                #print('newLine',newLine)\n",
    "            else:\n",
    "                word = line.split()\n",
    "                newLine = word[0] + ' ' + termo_corpo + '\\n'\n",
    "                #print('SEGUNDA OCORRENCIA')\n",
    "                #print('line',line)\n",
    "                #print('newLine',newLine)\n",
    "            is_in_body = True\n",
    "        else:\n",
    "            is_in_body = False\n",
    "            newLine = line\n",
    "        newlines.append(newLine)\n",
    "    return newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines = troca(sentences_trat, 'GPE', 'B-LOC', 'I-LOC')\n",
    "lines1 = troca(lines, 'PERSON', 'B-PER', 'I-PER')\n",
    "lines2 = troca(lines1, 'ORG', 'B-ORG', 'I-ORG')\n",
    "lines3 = troca(lines2, 'LOC', 'B-LOC', 'I-LOC')\n",
    "lines4 = troca(lines3, 'PERCENT', 'MISC','MISC')\n",
    "lines5 = troca(lines4, 'FAC', 'MISC','MISC')\n",
    "lines6 = troca(lines5, 'CARDINAL', 'MISC','MISC') \n",
    "lines7 = troca(lines6, 'QUANTITY', 'MISC','MISC') \n",
    "lines8 = troca(lines7, 'DATE', 'MISC','MISC') \n",
    "lines9 = troca(lines8, 'EVENT', 'MISC','MISC') \n",
    "lines10 = troca(lines9, 'MONEY', 'MISC','MISC') \n",
    "lines11 = troca(lines10, 'NORP', 'MISC','MISC') \n",
    "lines12 = troca(lines11, 'PRODUCT', 'MISC','MISC') \n",
    "lines13 = troca(lines12, 'TIME', 'MISC','MISC') \n",
    "lines14 = troca(lines13, 'FAC', 'MISC','MISC') \n",
    "lines15 = troca(lines14, 'LAW', 'MISC','MISC') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'wt') as fileout:\n",
    "    fileout.writelines(lines15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 - Abre o Ontonotes após aplicar ajuste do label\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'r') as file:\n",
    "   sentences_trat_label = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(sentences_trat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sentences_trat_label[0:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Insere linha entre as sentenças\n",
    "arq1 = ''\n",
    "for linha in lines15:\n",
    "\n",
    "#for linha in sentences_trat:\n",
    "    p=linha.find('.')\n",
    "    #print('linha', linha)\n",
    "    #print ('p == ',p)\n",
    "    if p==0:\n",
    "        arq1=arq1+linha+'\\n'\n",
    "        #print ('arq de p0 == ',arq)\n",
    "    else:\n",
    "        arq1=arq1+linha\n",
    "        #print ('arq de p = . == ',arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Grava o arquivo após o tratamento\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq1.txt', 'wt') as fileout:\n",
    "    fileout.writelines(sentences_trat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o y_test\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq1.txt', 'r') as file:\n",
    "   arq_sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 - cria a função para inserir tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_b(arq_sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in arq_sentences[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.9  Aplica a função de tokens ao arquivo aberto\n",
    "arq_sentences_1 = preprocess_b(sentences_trat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arq_sentences_1[0:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 - Função para extrair os rotulos reais\n",
    "def sent2labelsO(sent):\n",
    "    return [label for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 - Verifica se o dataset tem mais elementos que a chamado do métoddo sent2labels\n",
    "for sentences in arq_sentences_1:\n",
    "    try: \n",
    "        _ = sent2labelsO(sentences)\n",
    "    except ValueError:\n",
    "        for word in sentences:\n",
    "            if len(word)!= 2:\n",
    "                print(f' {word} possui {len(word)} elementos.')\n",
    "                 \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Extrai os rótulos reais y_test\n",
    "y_test = [sent2labelsO(sentences) for sentences in arq_sentences_1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo 3 - Carrega o X_test a partir do dataset do Ontonotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.1 - Abre o Ontonotes para aplicar o tratamento \n",
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train.txt', 'r') as file:\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq1.txt', 'r') as file:\n",
    "   sentences_x = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 - Retira os espaços em branco e as words maiores que duas posições\n",
    "for i in range(len(sentences_x) - 1):\n",
    "    #print(sentences[116:122])\n",
    "    atual = sentences_x[i].split()\n",
    "    proximo = sentences_x[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        sentences_x[i] = '\\t'.join(atual)+'\\n'\n",
    "        proximo = proximo[1:]\n",
    "        sentences_x[i+1] = '\\t'.join(proximo)+'\\n'\n",
    "        #print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 - Verifica as linhas com mais de duas words e concatena\n",
    "for i in range(len(sentences_x) - 1):\n",
    "        atual = sentences_x[i].split()\n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]))+' '+atual[2]+'\\n' \n",
    "            #print('sentences', sentences_x[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] +'\\n'  \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4] +'\\n' \n",
    "            #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.4 - Remove o label original\n",
    "def removeLabel(linhas):\n",
    "    #novoTexto = []\n",
    "    newLines=[]\n",
    "    for line in linhas: \n",
    "        if len(line)!=1:\n",
    "            word = line.split()\n",
    "            newLine = word[0]+ '\\n'\n",
    "            #print('entrou no if')\n",
    "            #print(newLine)\n",
    "            \n",
    "        else:\n",
    "            newLine = line \n",
    "            #print('entrou no else')\n",
    "        newLines.append(newLine)\n",
    "        #print(newLines)\n",
    "    return newLines\n",
    "     \n",
    "                \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_sem_label = removeLabel(sentences_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(texto_sem_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Grava o arquivo após o tratamento\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_2.txt', 'wt') as fileout:\n",
    "    fileout.writelines(texto_sem_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o X_text\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_2.txt', 'r') as file:\n",
    "   arq_sentences_2 = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Grava o arquivo após o tratamento\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_3.txt', 'wt') as fileout:\n",
    "    fileout.write(arq_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o X_text\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_3.txt', 'r') as file:\n",
    "   arq_sentences_3 = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 - Cria os tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_O(arq_sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in arq_sentences[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.9 - Transforma a sentença em lista\n",
    "test_sentences_2=preprocess_O(arq_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(test_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 - Função para extrair as features do texto a ser rotulado\n",
    "\n",
    "def word2featuresO(sent, i):\n",
    "    #word = sent[i][0]\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit()  \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]       \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 - Função para chamada da Função para extrair as features do texto a ser rotulado\n",
    "def sent2featuresO(sent):\n",
    "    return [word2featuresO(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.2 - Extrai as fetatures de X_test \n",
    "X_test = [[sent2featuresO(s) for s in text] for text in test_sentences_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 - Retira o X_teste de dentro da lista aninhada para execução do y_pred\n",
    "for i in range(len(X_test)):\n",
    "        X_test[i] = [i[0] for i in X_test[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Passo: Gerar o y_pred = rotulos preditos para o texto não rotulado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 - Aplica o modelo treinado no dataset sem rotulos\n",
    "y_pred = crf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Checa se as word do Y_test com o Y_pred estão batendo\n",
    " #len(sentences_trat) , len(arq_sentences_2) \n",
    " \n",
    "for i in range(len(arq_sentences_2)):\n",
    "    predicao = arq_sentences_2[i].split()\n",
    "    real =  sentences_trat[i].split()\n",
    "    #print('predicao',predicao[0] )\n",
    "    #print('real', real[0])\n",
    "    if predicao[0] != real[0]:\n",
    "        print(i)\n",
    "        print('real', real, i)\n",
    "        print('predicao', predicao, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('predicao', arq_sentences_2[141346:141349].split())\n",
    "print ('real' ,  sentences_trat[141346:141349].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('word do y_test   ==>', arq_sentences_1[2000],'\\n')\n",
    "print('word do y_pred   ==>', test_sentences_2[2000],'\\n')\n",
    "print('label do y_test  ==>',y_test[2000],'\\n')\n",
    "print('label do y_pred  ==>',y_pred[2000],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho y_pred\", len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação\n",
    "\n",
    "There is much more O entities in data set, but we’re more interested in other entities. To account for this we’ll use averaged F1 score computed for all labels except for O. sklearn-crfsuite.metrics package provides some useful metrics for sequence classification task, including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERSON == 'B-PER' 'I-PER' (CONLL), ORG == 'B-ORG', GPE == 'B-LOC' 'I-LOC' (CONLL), MISC == TUDO QUE NAO AS OUTRAS 3 NO CONLL\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imprime o tamanho do y_test e y_pred e qtde de label predita\n",
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho em sentencas y_test\", len(y_test))\n",
    "print(\"tamanho em sentencas y_pred\", len(y_pred))\n",
    "print('tamanho em linhas y_test  ==>', len(arq_sentences))\n",
    "print('tamanho em linhas y_pred  ==>', len(arq_sentences_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization\n",
    "\n",
    "To improve quality try to select regularization parameters using randomized search and 3-fold cross-validation.\n",
    "\n",
    "I takes quite a lot of CPU time and RAM (we’re fitting a model 50 * 3 = 150 times), so grab a tea and be patient, or reduce n_iter in RandomizedSearchCV, or fit model only on a subset of training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check parameter space\n",
    "\n",
    "A chart which shows which c1 and c2 values have RandomizedSearchCV checked. Red color means better results, blue means worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check best estimator on our test data\n",
    "\n",
    "As you can see, quality is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check what classifier learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s check what classifier learned\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized.\n",
    "\n",
    "Check the state features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
