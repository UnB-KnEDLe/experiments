{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "source": [
    "Let's use CoNLL 2003 data to build a NER system\n",
    "\n",
    "We use English data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Passo 1 - Treina o modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conll2003\n",
    "with open('/home/82068895153/POS/skweak/data/conll2003_dataset/train.txt', 'r') as file:\n",
    "  sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print (sentences[5])\n",
    " len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in sentences[5:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quebra a sentença em lista\n",
    "sentences_1=preprocess(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    #print ('word', word)\n",
    "    postag = sent[i][1]\n",
    "    #print ('postag', postag)\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],        \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]        \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, __, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, __, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in sentences_1]\n",
    "\n",
    "y_train = [sent2labels(s) for s in sentences_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Passo 2 - Prepara o Y_test a partir do dataset do Ontonotes "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 - Abre o Ontonotes para aplicar o tratamento \n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train.txt', 'r') as file:\n",
    "   sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo (['عارضا', 'O']) e (['ً', 'المساهمات', 'O']) para (['عارضاً', 'O']) e (['المساهمات', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'your', 'O']) para ([\"'s\", 'O']) e (['your', 'O'])\n",
      "Convertendo (['معددا', 'O']) e (['ً', 'المشاريع', 'O']) para (['معدداً', 'O']) e (['المشاريع', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'very', 'O']) para ([\"'s\", 'O']) e (['very', 'O'])\n",
      "Convertendo (['و', 'O']) e (['2.9', 'مليار', 'MONEY']) para (['و2.9', 'O']) e (['مليار', 'MONEY'])\n",
      "Convertendo (['جدا', 'O']) e (['ً', '\"', 'O']) para (['جداً', 'O']) e (['\"', 'O'])\n",
      "Convertendo (['سباقا', 'O']) e (['ً', 'في', 'O']) para (['سباقاً', 'O']) e (['في', 'O'])\n",
      "Convertendo (['تتوهج', 'O']) e (['نوراً', 'و', 'O']) para (['تتوهجنوراً', 'O']) e (['و', 'O'])\n",
      "Convertendo (['ايضا', 'O']) e (['ً', 'ان', 'O']) para (['ايضاً', 'O']) e (['ان', 'O'])\n",
      "Convertendo (['عددا', 'O']) e (['ً', 'من', 'O']) para (['عدداً', 'O']) e (['من', 'O'])\n",
      "Convertendo (['محاميا', 'O']) e (['ً', 'ل', 'O']) para (['محامياً', 'O']) e (['ل', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'really', 'O']) para ([\"'s\", 'O']) e (['really', 'O'])\n",
      "Convertendo (['مأخوذا', 'O']) e (['ً', 'ب', 'O']) para (['مأخوذاً', 'O']) e (['ب', 'O'])\n",
      "Convertendo (['مثيرا', 'O']) e (['ً', 'لا', 'O']) para (['مثيراً', 'O']) e (['لا', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'say', 'O']) para ([\"n't\", 'O']) e (['say', 'O'])\n",
      "Convertendo (['عم', 'O']) e (['ّت', 'البلاد', 'O']) para (['عمّت', 'O']) e (['البلاد', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'also', 'O']) para ([\"'s\", 'O']) e (['also', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'they', 'O']) para ([\"n't\", 'O']) e (['they', 'O'])\n",
      "Convertendo (['ناصحا', 'O']) e (['ً', 'و', 'O']) para (['ناصحاً', 'O']) e (['و', 'O'])\n",
      "Convertendo (['محذرا', 'O']) e (['ً', 'من', 'O']) para (['محذراً', 'O']) e (['من', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo (['42', 'TIME']) e (['.94', 'ثانية', 'TIME']) para (['42.94', 'TIME']) e (['ثانية', 'TIME'])\n",
      "Convertendo (['1-', 'O']) e (['1', 'و', 'تأهل', 'O']) para (['1-1', 'O']) e (['و', 'تأهل', 'O'])\n",
      "Convertendo (['1-1', 'O']) e (['و', 'تأهل', 'O']) para (['1-1و', 'O']) e (['تأهل', 'O'])\n",
      "Convertendo (['0', 'O']) e (['-0', 'في', 'O']) para (['0-0', 'O']) e (['في', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'have', 'O']) para ([\"n't\", 'O']) e (['have', 'O'])\n",
      "Convertendo (['خبيرا', 'O']) e (['ً', 'في', 'O']) para (['خبيراً', 'O']) e (['في', 'O'])\n",
      "Convertendo (['يحصل', 'O']) e (['تحسّن', 'يذكر', 'O']) para (['يحصلتحسّن', 'O']) e (['يذكر', 'O'])\n",
      "Convertendo (['الظهور', 'O']) e (['علناً', 'بعد', 'O']) para (['الظهورعلناً', 'O']) e (['بعد', 'O'])\n",
      "Convertendo (['المانيا', 'GPE']) e (['1-0', 'الشوط', 'CARDINAL']) para (['المانيا1-0', 'GPE']) e (['الشوط', 'CARDINAL'])\n",
      "Convertendo (['0', 'CARDINAL']) e (['-0', 'سجل', 'O']) para (['0-0', 'CARDINAL']) e (['سجل', 'O'])\n",
      "Convertendo (['after', 'O']) e (['Mr.', 'Palmeiro', 'PERSON']) para (['afterMr.', 'O']) e (['Palmeiro', 'PERSON'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'read', 'O']) para ([\"n't\", 'O']) e (['read', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'hard', 'O']) para ([\"'s\", 'O']) e (['hard', 'O'])\n",
      "Convertendo (['امورا', 'O']) e (['ً', 'عدة', 'O']) para (['اموراً', 'O']) e (['عدة', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'say', 'O']) para ([\"'s\", 'O']) e (['say', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'not', 'O']) para ([\"'m\", 'O']) e (['not', 'O'])\n",
      "Convertendo (['28', 'PERSON']) e (['عاماً', 'و', 'O']) para (['28عاماً', 'PERSON']) e (['و', 'O'])\n",
      "Convertendo (['عاما', 'PERSON']) e (['ً', '.', 'O']) para (['عاماً', 'PERSON']) e (['.', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'easy', 'O']) para ([\"'s\", 'O']) e (['easy', 'O'])\n",
      "Convertendo (['الشانفيل', 'ORG']) e (['5.58.27', 'دقائق', 'O']) para (['الشانفيل5.58.27', 'ORG']) e (['دقائق', 'O'])\n",
      "Convertendo (['شيئا', 'O']) e (['ً', 'في', 'O']) para (['شيئاً', 'O']) e (['في', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'actually', 'O']) para ([\"'s\", 'O']) e (['actually', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'powerful', 'O']) para ([\"'s\", 'O']) e (['powerful', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'watch', 'O']) para ([\"'s\", 'O']) e (['watch', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'why', 'O']) para ([\"'s\", 'O']) e (['why', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'very', 'O']) para ([\"'s\", 'O']) e (['very', 'O'])\n",
      "Convertendo (['طبيعيا', 'O']) e (['ً', 'بعد', 'O']) para (['طبيعياً', 'O']) e (['بعد', 'O'])\n",
      "Convertendo (['اشارة', 'O']) e (['ً', 'من', 'O']) para (['اشارةً', 'O']) e (['من', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'been', 'O']) para ([\"n't\", 'O']) e (['been', 'O'])\n",
      "Convertendo (['الذي', 'O']) e (['أسّ', 'سه', 'O']) para (['الذيأسّ', 'O']) e (['سه', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'like', 'O']) para ([\"n't\", 'O']) e (['like', 'O'])\n",
      "Convertendo (['تطر', 'O']) e (['ّ', 'فها', 'O']) para (['تطرّ', 'O']) e (['فها', 'O'])\n",
      "Convertendo (['ظلالا', 'O']) e (['ً', 'ل', 'O']) para (['ظلالاً', 'O']) e (['ل', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'not', 'O']) para ([\"'s\", 'O']) e (['not', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'about', 'O']) para ([\"'s\", 'O']) e (['about', 'O'])\n",
      "Convertendo (['George', 'PERSON']) e (['W.', 'Bush', 'PERSON']) para (['GeorgeW.', 'PERSON']) e (['Bush', 'PERSON'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'what', 'O']) para ([\"'s\", 'O']) e (['what', 'O'])\n",
      "Convertendo (['اجتماعا', 'O']) e (['ً', 'في', 'O']) para (['اجتماعاً', 'O']) e (['في', 'O'])\n",
      "Convertendo (['بيانا', 'O']) e (['ً', 'دعت', 'O']) para (['بياناً', 'O']) e (['دعت', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'not', 'O']) para ([\"'m\", 'O']) e (['not', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'exactly', 'O']) para ([\"'s\", 'O']) e (['exactly', 'O'])\n",
      "Convertendo (['فاعلا', 'O']) e (['ً', 'او', 'O']) para (['فاعلاً', 'O']) e (['او', 'O'])\n",
      "Convertendo (['متدخلا', 'O']) e (['ً', 'او', 'O']) para (['متدخلاً', 'O']) e (['او', 'O'])\n",
      "Convertendo (['بعدا', 'O']) e (['ً', 'مهما', 'O']) para (['بعداً', 'O']) e (['مهما', 'O'])\n",
      "Convertendo (['مهما', 'O']) e (['ً', 'ل', 'O']) para (['مهماً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['ل', 'O']) e (['لمحصِّلة', 'التاريخية', 'O']) para (['للمحصِّلة', 'O']) e (['التاريخية', 'O'])\n",
      "Convertendo (['مث', 'O']) e (['ّل', 'هذا', 'O']) para (['مثّل', 'O']) e (['هذا', 'O'])\n",
      "Convertendo (['و', 'O']) e (['المسوّغ', 'الرئيسي', 'O']) para (['والمسوّغ', 'O']) e (['الرئيسي', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'see', 'O']) para ([\"n't\", 'O']) e (['see', 'O'])\n",
      "Convertendo (['أيضا', 'O']) e (['ً', 'وقوف', 'O']) para (['أيضاً', 'O']) e (['وقوف', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'help', 'O']) para ([\"n't\", 'O']) e (['help', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['d', 'also', 'O']) para ([\"'d\", 'O']) e (['also', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'do', 'O']) para ([\"n't\", 'O']) e (['do', 'O'])\n",
      "Convertendo (['رح', 'O']) e (['ّب', 'النائب', 'O']) para (['رحّب', 'O']) e (['النائب', 'O'])\n",
      "Convertendo (['و', 'O']) e (['تحديداً', 'تحت', 'O']) para (['وتحديداً', 'O']) e (['تحت', 'O'])\n",
      "Convertendo (['خصوصا', 'O']) e (['ً', 'ل', 'O']) para (['خصوصاً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['ها', 'O']) e (['حقاً', 'صفة', 'O']) para (['هاحقاً', 'O']) e (['صفة', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'afraid', 'O']) para ([\"'m\", 'O']) e (['afraid', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'going', 'O']) para ([\"'s\", 'O']) e (['going', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'enough', 'O']) para ([\"n't\", 'O']) e (['enough', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'our', 'O']) para ([\"n't\", 'O']) e (['our', 'O'])\n",
      "Convertendo (['عن', 'O']) e (['ستّ', 'محطات', 'CARDINAL']) para (['عنستّ', 'O']) e (['محطات', 'CARDINAL'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'had', 'O']) para ([\"'s\", 'O']) e (['had', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'make', 'O']) para ([\"n't\", 'O']) e (['make', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'correct', 'O']) para ([\"'s\", 'O']) e (['correct', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'want', 'O']) para ([\"n't\", 'O']) e (['want', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo (['دولارا', 'MONEY']) e (['ً', 'اميركيا', 'MONEY']) para (['دولاراً', 'MONEY']) e (['اميركيا', 'MONEY'])\n",
      "Convertendo (['اميركيا', 'MONEY']) e (['ً', 'ل', 'O']) para (['اميركياً', 'MONEY']) e (['ل', 'O'])\n",
      "Convertendo (['could', 'O']) e ([\"n't\", 'really', 'O']) para ([\"couldn't\", 'O']) e (['really', 'O'])\n",
      "Convertendo (['تبيض', 'O']) e (['ابداً', 'عند', 'O']) para (['تبيضابداً', 'O']) e (['عند', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'know', 'O']) para ([\"n't\", 'O']) e (['know', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'unlikely', 'O']) para ([\"'s\", 'O']) e (['unlikely', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo (['يكن', 'O']) e (['موفقاً', 'أيضا', 'O']) para (['يكنموفقاً', 'O']) e (['أيضا', 'O'])\n",
      "Convertendo (['رسميا', 'O']) e (['ً', 'عن', 'O']) para (['رسمياً', 'O']) e (['عن', 'O'])\n",
      "Convertendo (['ايضا', 'O']) e (['ً', 'معرفة', 'O']) para (['ايضاً', 'O']) e (['معرفة', 'O'])\n",
      "Convertendo (['مصرفيا', 'O']) e (['ً', 'سابقا', 'O']) para (['مصرفياً', 'O']) e (['سابقا', 'O'])\n",
      "Convertendo (['سابقا', 'O']) e (['ً', 'و', 'O']) para (['سابقاً', 'O']) e (['و', 'O'])\n",
      "Convertendo (['متنبها', 'O']) e (['ً', 'ل', 'O']) para (['متنبهاً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['much', 'O']) e (['Mr.', 'Hale', 'PERSON']) para (['muchMr.', 'O']) e (['Hale', 'PERSON'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'keeping', 'O']) para ([\"'s\", 'O']) e (['keeping', 'O'])\n",
      "Convertendo (['do', 'O']) e ([\"n't\", 'always', 'O']) para ([\"don't\", 'O']) e (['always', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'going', 'O']) para ([\"'m\", 'O']) e (['going', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'let', 'O']) para ([\"n't\", 'O']) e (['let', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'forgive', 'O']) para ([\"n't\", 'O']) e (['forgive', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'get', 'O']) para ([\"n't\", 'O']) e (['get', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'let', 'O']) para ([\"n't\", 'O']) e (['let', 'O'])\n",
      "Convertendo (['مي', 'O']) e (['ّزت', 'الحملة', 'O']) para (['ميّزت', 'O']) e (['الحملة', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'keep', 'O']) para ([\"'s\", 'O']) e (['keep', 'O'])\n",
      "Convertendo (['anti', 'O']) e (['-drug', 'policy', 'O']) para (['anti-drug', 'O']) e (['policy', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo (['تقد', 'O']) e (['ّ', 'منا', 'O']) para (['تقدّ', 'O']) e (['منا', 'O'])\n",
      "Convertendo (['مسل', 'O']) e (['ّم', 'ب', 'O']) para (['مسلّم', 'O']) e (['ب', 'O'])\n",
      "Convertendo (['مرشدا', 'O']) e (['ً', 'عاما', 'O']) para (['مرشداً', 'O']) e (['عاما', 'O'])\n",
      "Convertendo (['عاما', 'O']) e (['ً', 'ل', 'O']) para (['عاماً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['ملما', 'O']) e (['ً', 'ب', 'O']) para (['ملماً', 'O']) e (['ب', 'O'])\n",
      "Convertendo (['\"', 'FAC']) e (['حالياً', 'انتاج', 'O']) para (['\"حالياً', 'FAC']) e (['انتاج', 'O'])\n",
      "Convertendo (['المسر', 'O']) e (['ّحين', 'ل', 'O']) para (['المسرّحين', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['معتبرا', 'O']) e (['ً', 'أن', 'O']) para (['معتبراً', 'O']) e (['أن', 'O'])\n",
      "Convertendo (['ميدانيا', 'O']) e (['ً', 'في', 'O']) para (['ميدانياً', 'O']) e (['في', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'recess', 'O']) para ([\"'s\", 'O']) e (['recess', 'O'])\n",
      "Convertendo (['مسن', 'O']) e (['ّة', 'و', 'O']) para (['مسنّة', 'O']) e (['و', 'O'])\n",
      "Convertendo (['و', 'O']) e (['يتقوّتان', 'الفضلات', 'O']) para (['ويتقوّتان', 'O']) e (['الفضلات', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'have', 'O']) para ([\"n't\", 'O']) e (['have', 'O'])\n",
      "Convertendo (['اثينا', 'GPE']) e (['سلباً', '0-.0', 'CARDINAL']) para (['اثيناسلباً', 'GPE']) e (['0-.0', 'CARDINAL'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'the', 'O']) para ([\"n't\", 'O']) e (['the', 'O'])\n",
      "Convertendo (['واصفا', 'O']) e (['ً', 'المؤسسة', 'O']) para (['واصفاً', 'O']) e (['المؤسسة', 'O'])\n",
      "Convertendo (['ها', 'O']) e (['تجسّد', 'تزاوج', 'O']) para (['هاتجسّد', 'O']) e (['تزاوج', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'think', 'O']) para ([\"n't\", 'O']) e (['think', 'O'])\n",
      "Convertendo (['باريكيللو', 'PERSON']) e (['تصدّر', 'لفات', 'O']) para (['باريكيللوتصدّر', 'PERSON']) e (['لفات', 'O'])\n",
      "Convertendo (['anti', 'O']) e (['-war', 'Democrat', 'NORP']) para (['anti-war', 'O']) e (['Democrat', 'NORP'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'why', 'O']) para ([\"'s\", 'O']) e (['why', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'nothing', 'O']) para ([\"'s\", 'O']) e (['nothing', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['d', 'like', 'O']) para ([\"'d\", 'O']) e (['like', 'O'])\n",
      "Convertendo (['هدفا', 'O']) e (['ًُ', 'محتملا', 'O']) para (['هدفاًُ', 'O']) e (['محتملا', 'O'])\n",
      "Convertendo (['محتملا', 'O']) e (['ً', 'ل', 'O']) para (['محتملاً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['ايضا', 'O']) e (['ً', '\"', 'O']) para (['ايضاً', 'O']) e (['\"', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'all', 'O']) para ([\"'s\", 'O']) e (['all', 'O'])\n",
      "Convertendo (['افتتح', 'O']) e (['مستوصفاً', 'و', 'O']) para (['افتتحمستوصفاً', 'O']) e (['و', 'O'])\n",
      "Convertendo (['مسحا', 'O']) e (['ً', 'سياحيا', 'O']) para (['مسحاً', 'O']) e (['سياحيا', 'O'])\n",
      "Convertendo (['سياحيا', 'O']) e (['ً', 'حماده', 'PERSON']) para (['سياحياً', 'O']) e (['حماده', 'PERSON'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'any', 'O']) para ([\"n't\", 'O']) e (['any', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'going', 'O']) para ([\"'s\", 'O']) e (['going', 'O'])\n",
      "Convertendo (['تقد', 'O']) e (['ّر', 'ب_', 'O']) para (['تقدّر', 'O']) e (['ب_', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'want', 'O']) para ([\"n't\", 'O']) e (['want', 'O'])\n",
      "Convertendo (['دليلا', 'O']) e (['ً', 'قاطعا', 'O']) para (['دليلاً', 'O']) e (['قاطعا', 'O'])\n",
      "Convertendo (['قاطعا', 'O']) e (['ً', 'على', 'O']) para (['قاطعاً', 'O']) e (['على', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'the', 'O']) para ([\"'s\", 'O']) e (['the', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'buy', 'O']) para ([\"n't\", 'O']) e (['buy', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'drug', 'O']) para ([\"'s\", 'O']) e (['drug', 'O'])\n",
      "Convertendo (['\"', 'O']) e (['غلّة', 'الجوائز', 'O']) para (['\"غلّة', 'O']) e (['الجوائز', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'afford', 'O']) para ([\"n't\", 'O']) e (['afford', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'so', 'O']) para ([\"'s\", 'O']) e (['so', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'notice', 'O']) para ([\"n't\", 'O']) e (['notice', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'been', 'O']) para ([\"'s\", 'O']) e (['been', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'David', 'O']) para ([\"'s\", 'O']) e (['David', 'O'])\n",
      "Convertendo (['سوف', 'O']) e (['تغيّر', 'معالم', 'O']) para (['سوفتغيّر', 'O']) e (['معالم', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'thrown', 'O']) para ([\"'m\", 'O']) e (['thrown', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'with', 'O']) para ([\"'s\", 'O']) e (['with', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'take', 'O']) para ([\"'s\", 'O']) e (['take', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'happened', 'O']) para ([\"'s\", 'O']) e (['happened', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'power', 'O']) para ([\"'s\", 'O']) e (['power', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'they', 'O']) para ([\"n't\", 'O']) e (['they', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'moving', 'O']) para ([\"'s\", 'O']) e (['moving', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'just', 'O']) para ([\"'s\", 'O']) e (['just', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'possible', 'O']) para ([\"'s\", 'O']) e (['possible', 'O'])\n",
      "Convertendo (['n', 'O']) e ([\"'t\", 'think', 'O']) para ([\"n't\", 'O']) e (['think', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo (['خصوصا', 'O']) e (['ً', 'في', 'O']) para (['خصوصاً', 'O']) e (['في', 'O'])\n",
      "Convertendo (['رد', 'O']) e (['ّ', 'عنيف', 'O']) para (['ردّ', 'O']) e (['عنيف', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'that', 'O']) para ([\"n't\", 'O']) e (['that', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'coming', 'O']) para ([\"'s\", 'O']) e (['coming', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'think', 'O']) para ([\"n't\", 'O']) e (['think', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'optomistic', 'O']) para ([\"'s\", 'O']) e (['optomistic', 'O'])\n",
      "Convertendo (['ايضا', 'O']) e (['ً', 'الى', 'O']) para (['ايضاً', 'O']) e (['الى', 'O'])\n",
      "Convertendo (['ب', 'O']) e (['ُن', 'اها', 'O']) para (['بُن', 'O']) e (['اها', 'O'])\n",
      "Convertendo (['ان', 'O']) e (['بلداً', 'مثل', 'O']) para (['انبلداً', 'O']) e (['مثل', 'O'])\n",
      "Convertendo (['تحالفا', 'O']) e (['ً', 'من', 'O']) para (['تحالفاً', 'O']) e (['من', 'O'])\n",
      "Convertendo (['تأييدا', 'O']) e (['ً', 'واسعا', 'O']) para (['تأييداً', 'O']) e (['واسعا', 'O'])\n",
      "Convertendo (['واسعا', 'O']) e (['ً', 'ل', 'O']) para (['واسعاً', 'O']) e (['ل', 'O'])\n",
      "Convertendo (['سباقا', 'O']) e (['ً', 'على', 'O']) para (['سباقاً', 'O']) e (['على', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'also', 'O']) para ([\"'s\", 'O']) e (['also', 'O'])\n",
      "Convertendo (['6', 'O']) e (['-2', 'في', 'O']) para (['6-2', 'O']) e (['في', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'not', 'O']) para ([\"'m\", 'O']) e (['not', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'leave', 'O']) para ([\"n't\", 'O']) e (['leave', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'have', 'O']) para ([\"n't\", 'O']) e (['have', 'O'])\n",
      "Convertendo (['Imette', 'PERSON']) e (['St.', 'Guillen', 'PERSON']) para (['ImetteSt.', 'PERSON']) e (['Guillen', 'PERSON'])\n",
      "Convertendo ([\"'\", 'PERSON']) e (['s', 'rape', 'O']) para ([\"'s\", 'PERSON']) e (['rape', 'O'])\n",
      "Convertendo (['رئيسا', 'O']) e (['ً', 'جديداً', 'O']) para (['رئيساً', 'O']) e (['جديداً', 'O'])\n",
      "Convertendo (['الجنوبية', 'ORG']) e (['1-0', 'و', 'خروج', 'O']) para (['الجنوبية1-0', 'ORG']) e (['و', 'خروج', 'O'])\n",
      "Convertendo (['الجنوبية1-0', 'ORG']) e (['و', 'خروج', 'O']) para (['الجنوبية1-0و', 'ORG']) e (['خروج', 'O'])\n",
      "Convertendo (['70', 'CARDINAL']) e (['مواطناً', 'عند', 'O']) para (['70مواطناً', 'CARDINAL']) e (['عند', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo (['توترا', 'O']) e (['ً', 'في', 'O']) para (['توتراً', 'O']) e (['في', 'O'])\n",
      "Convertendo (['anti', 'O']) e (['-war', 'candidate', 'O']) para (['anti-war', 'O']) e (['candidate', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'give', 'O']) para ([\"n't\", 'O']) e (['give', 'O'])\n",
      "Convertendo (['اجراميا', 'O']) e (['ً', 'ب', 'O']) para (['اجرامياً', 'O']) e (['ب', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'a', 'O']) para ([\"'s\", 'O']) e (['a', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'going', 'O']) para ([\"'s\", 'O']) e (['going', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['m', 'thirty', 'DATE']) para ([\"'m\", 'O']) e (['thirty', 'DATE'])\n",
      "Convertendo (['الجيش', 'ORG']) e (['16.09.80', 'دقيقة', 'O']) para (['الجيش16.09.80', 'ORG']) e (['دقيقة', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'this', 'O']) para ([\"n't\", 'O']) e (['this', 'O'])\n",
      "Convertendo ([\"n'\", 'O']) e (['t', 'imagine', 'O']) para ([\"n't\", 'O']) e (['imagine', 'O'])\n",
      "Convertendo (['نظاما', 'O']) e (['ً', 'غذائياً', 'مختلفا', 'O']) para (['نظاماً', 'O']) e (['غذائياً', 'مختلفا', 'O'])\n",
      "Convertendo (['نظاماً', 'O']) e (['غذائياً', 'مختلفا', 'O']) para (['نظاماًغذائياً', 'O']) e (['مختلفا', 'O'])\n",
      "Convertendo (['مختلفا', 'O']) e (['ً', 'كليا', 'O']) para (['مختلفاً', 'O']) e (['كليا', 'O'])\n",
      "Convertendo (['كليا', 'O']) e (['ً', 'عن', 'O']) para (['كلياً', 'O']) e (['عن', 'O'])\n",
      "Convertendo (['على', 'O']) e (['أسرّة', 'تخمت', 'O']) para (['علىأسرّة', 'O']) e (['تخمت', 'O'])\n",
      "Convertendo ([\"'\", 'O']) e (['s', 'worked', 'O']) para ([\"'s\", 'O']) e (['worked', 'O'])\n"
     ]
    }
   ],
   "source": [
    "#2.2 - Retira os espaços em branco e as words maiores que duas posições\n",
    "for i in range(len(sentences) - 1):\n",
    "    atual = sentences[i].split()\n",
    "    proximo = sentences[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        sentences[i] = '\\t'.join(atual)\n",
    "        proximo = proximo[1:]\n",
    "        sentences[i+1] = '\\t'.join(proximo)\n",
    "        print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 - Verifica as linhas com mais de duas words e concatena\n",
    "for i in range(len(sentences) - 1):\n",
    "        atual = sentences[i].split()\n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]))+' '+atual[2] \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] \n",
    "            #print('test_sentences_label', test_sentences_label[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4]\n",
    "            #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['\\n', 'He\\tO\\n', 'does\\tO\\n', \"n't\\tO\", 'seem\\tO', 'to\\tO\\n', 'be\\tO\\n', 'backing\\tO\\n', 'away\\tO\\n', 'from\\tO\\n', 'those\\tO\\n', 'controversial\\tO\\n', 'comments\\tO\\n', 'the\\tO\\n', 'last\\tO\\n', 'throes\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'insurgency\\tO\\n', '.\\tO\\n', '\\n', 'Last\\tDATE\\n', 'year\\tDATE\\n', ',\\tO\\n', 'Dentsu\\tORG\\n', \"'s\\tO\\n\", 'foreign\\tO\\n', 'business\\tO\\n', 'accounted\\tO\\n', 'for\\tO\\n', 'less\\tO\\n', 'than\\tO\\n', '10\\tO\\n', '%\\tO\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[116:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3160678"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 - Faz a troca dos labels\n",
    "def processarLinha(sentences):\n",
    "    #print(sentences)\n",
    "    #print(sentences[1])\n",
    "    #count = 0\n",
    "    # Write the file out again\n",
    "    with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'wt') as fileout:\n",
    "        for linha in sentences:\n",
    "            #count = count +1\n",
    "            #print (linha)\n",
    "            lista = linha.split(\"\\t\")\n",
    "            \n",
    "            #print (len(lista))\n",
    "            #print (lista)     \n",
    "            if len(lista)==2 and lista[1] == 'PERSON\\n':\n",
    "                lista[1]= 'B-PER\\n'\n",
    "                #print (lista)\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])\n",
    "            elif len(lista)==2 and lista[1] == 'GPE\\n': \n",
    "                lista[1]= 'B-LOC\\n'\n",
    "                #print (lista)\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])         \n",
    "            elif len(lista)==2:\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])\n",
    "            #if count == 15:\n",
    "              #  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processarLinha(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 - Abre o Ontonotes após aplicar ajuste do label\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'r') as file:\n",
    "   sentences_trat = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2993507"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(sentences_trat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Insere linha entre as sentenças\n",
    "arq = ''\n",
    "for linha in sentences_trat:\n",
    "    p=linha.find('.')\n",
    "    #print('linha', linha)\n",
    "    #print ('p == ',p)\n",
    "    if p==0:\n",
    "        arq=arq+linha+'\\n'\n",
    "        #print ('arq de p0 == ',arq)\n",
    "    else:\n",
    "        arq=arq+linha\n",
    "        #print ('arq de p = . == ',arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq.txt', 'wt') as fileout:\n",
    "    fileout.write(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 - Abre o Ontonotes após aplicar ajuste do label\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq.txt', 'r') as file:\n",
    "   arq_sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria os tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_b(sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in sentences[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences_1 = preprocess_b(arq_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The', 'O'], ['school', 'O'], ['is', 'O'], ['going', 'O'], ['to', 'O'], ['be', 'O'], ['closed', 'O'], ['for', 'O'], ['the', 'DATE'], ['rest', 'DATE'], ['of', 'DATE'], ['the', 'DATE'], ['week', 'DATE'], ['Anderson', 'B-PER'], ['.', 'O']], [['Well', 'O'], [',', 'O'], ['actually', 'O'], [',', 'O'], ['regarding', 'O'], [',', 'O'], ['um', 'O'], [',', 'O'], ['Koizumi', 'B-PER'], [\"'s\", 'O'], ['attendance', 'O'], ['at', 'O'], ['this', 'O'], ['APEC', 'ORG'], ['summit', 'O'], [',', 'O'], ['people', 'O'], ['are', 'O'], ['more', 'O'], ['interested', 'O'], ['in', 'O'], ['the', 'O'], ['relations', 'O'], ['between', 'O'], ['Japan', 'B-LOC'], ['and', 'O'], ['its', 'O'], ['Asian', 'NORP'], ['neighboring', 'O'], ['countries', 'O'], [',', 'O'], ['including', 'O'], ['the', 'O'], ['kind', 'O'], ['of', 'O'], ['alliance', 'O'], ['relationship', 'O'], ['between', 'O'], ['the', 'O'], ['US', 'B-LOC'], ['and', 'O'], ['Japan', 'B-LOC'], [',', 'O'], ['and', 'O'], ['so', 'O'], ['on', 'O'], ['.', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "print(arq_sentences_1[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " [\"n't\", 'Oseem', 'Oto', 'O'] possui 4 elementos.\n"
     ]
    }
   ],
   "source": [
    "#Verifica se o dataset tem mais elementos que a chamado do métoddo sent2labels\n",
    "\n",
    "for sentences in arq_sentences_1:\n",
    "    try: \n",
    "        _ = sent2labelsO(sentences)\n",
    "    except ValueError:\n",
    "        for word in sentences:\n",
    "            if len(word)!= 2:\n",
    "                print(f' {word} possui {len(word)} elementos.')\n",
    "                 \n",
    "        break"
   ]
  },
  {
   "source": [
    "Carrega o X_test --> dataset do Ontonotes sem rótulos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Abre o Ontonotes sem rotulos\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_lu_1.txt', 'r') as file:\n",
    "    test_sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_sentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insereLinha(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retira os espaços em branco e as sentencas maiores que duas posições\n",
    "for i in range(len(test_sentences) - 1):\n",
    "    atual = test_sentences[i].split()\n",
    "    proximo = test_sentences[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        test_sentences[i] = '\\t'.join(atual)\n",
    "        proximo = proximo[1:]\n",
    "        test_sentences[i+1] = '\\t'.join(proximo)\n",
    "        #print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - retira espaço entre palavras quebradas indevidamente\n",
    "def retirarEspaco(sentences):\n",
    "#Retira os espaços em branco e as words maiores que duas posições\n",
    "    #with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_quebra.txt', 'wt') as fileout:\n",
    "      \n",
    "        for i in range(len(sentences) - 1):\n",
    "            atual = sentences[i].split()\n",
    "            proximo = sentences[i+1].split()\n",
    "            if len(atual) == 0:\n",
    "                continue\n",
    "            while len(proximo) > 2:\n",
    "            #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "                atual[0] += proximo[0]\n",
    "                sentences[i] = ' '.join(atual)\n",
    "                proximo = proximo[1:]\n",
    "                sentences[i+1] = ' '.join(proximo)\n",
    "            #print(f'({atual}) e ({proximo})')\n",
    "        return sentences\n",
    "        #fileout.write(str(sentences))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenarPalavra(sentences):\n",
    "    for i in range(len(sentences) - 1):\n",
    "        atual = sentences[i].split()\n",
    "    #print('atual', atual[0])\n",
    "    #proximo = test_sentences_label[i+1].split()\n",
    "    #print('proximo', proximo[0])\n",
    "    #print(i) \n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]))+' '+atual[2] \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] \n",
    "            #print('test_sentences_label', test_sentences_label[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4]\n",
    "            #print(i)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria os tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_b(sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in sentences[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transforma a sentença em lista\n",
    "test_sentences_1=preprocess_b(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(test_sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences_1)"
   ]
  },
  {
   "source": [
    "X_test = features so texto a ser rotulado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2featuresO(sent, i):\n",
    "    #word = sent[i][0]\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit()  \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]       \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2featuresO(sent):\n",
    "    #print('sent ==', sent)\n",
    "    #teste = [word2featuresO(sent, i) for i in range(len(sent))]\n",
    "    #print('teste ==', teste)\n",
    "    return [word2featuresO(sent, i) for i in range(len(sent))]\n",
    "    #return teste\n",
    "\n",
    "\n",
    "def sent2labelsO(sent):\n",
    "    return [label for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extrai as fetatures de X_test \n",
    "X_test = [[sent2featuresO(s) for s in text] for text in test_sentences_1]\n",
    "\n",
    "#https://stackoverflow.com/questions/41829323/attributeerror-list-object-has-no-attribute-lower-gensim\n",
    "#data = [line.strip() for line in open(\"C:\\corpus\\TermList.txt\", 'r')]\n",
    "#texts = [[word.lower() for word in text.split()] for text in data]\n",
    "#X_test=test_sentences_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "#print(X_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_text = [i for i in X_test_1] \n",
    "#import copy\n",
    "#X_text = [copy.copy(X_test_1)]\n",
    "#nova_lista is lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retira de dentro da lista aninhada (o dataset está com uma lista a mais)\n",
    "for i in range(len(X_test)):\n",
    "        X_test[i] = [i[0] for i in X_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "source": [
    "y_pred = rotulos preditos para o texto não rotulado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica o modelo treinado no dataset sem rotulos\n",
    "y_pred = crf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "source": [
    "Dataset Ontonotes com rótulos reais para gerar o y_test\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Abre o Ontonotes para carregar o y_test = DATASET com rótulos reais\n",
    "#with open('/home/82068895153/POS/skweak/data/BERT/train_out_trat.txt', 'r') as file:\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_trat_1.txt', 'r') as file:\n",
    "    test_sentences_label = list(file.readlines())\n",
    "    print (test_sentences_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_sentences_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentences_label[120]\n",
    "len(test_sentences_label)\n",
    "\n",
    "\n",
    "#Ontonotes sem rotulos ==> 1639268 (linhas)\n",
    "#                          153956  (sentencas)\n",
    "\n",
    "#Ontonotes com rotulos ==> 3103580 / 3160787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Concatena as sentencas com mais de 2 words\n",
    "# 560908\n",
    "# 3103580 \n",
    "for i in range(len(test_sentences_label) - 1):\n",
    "    atual = test_sentences_label[i].split()\n",
    "    #print('atual', atual[0])\n",
    "    #proximo = test_sentences_label[i+1].split()\n",
    "    #print('proximo', proximo[0])\n",
    "    #print(i) \n",
    "    if ((len(atual)>2) and (len(atual)<=3)):\n",
    "        print('atual', atual)\n",
    "        test_sentences_label[i]=(''.join(atual[0]+atual[1]))+' '+atual[2] \n",
    "        print('test_sentences_label', test_sentences_label[i])\n",
    "        print(i)\n",
    "    elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "        #print('atual', atual)\n",
    "        test_sentences_label[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] \n",
    "        #print('test_sentences_label', test_sentences_label[i])\n",
    "        print(i) \n",
    "    elif (len(atual)>4):\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4]\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quebra em lista o Dataset Ontonotes com rótulos reais para gerar o y_test\n",
    "\n",
    "test_sentences_label_1=preprocess_b(test_sentences_label)         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(test_sentences_label_1[120])\n",
    "len(test_sentences_label_1)\n",
    "\n",
    "#Ontonotes sem rotulos ==> 1639268 (linhas)\n",
    "#                           153956 (sentencas)\n",
    "\n",
    "#Ontonotes com rotulos ==> 3103580 (linhas)\n",
    "#                           98836 (sentencas)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica se o dataset tem mais elementos que a chamado do métoddo sent2labels\n",
    "\n",
    "for sentences in test_sentences_label_1:\n",
    "    try: \n",
    "        _ = sent2labelsO(sentences)\n",
    "    except ValueError:\n",
    "        for word in sentences:\n",
    "            if len(word)!= 2:\n",
    "                print(f' {word} possui {len(word)} elementos.')\n",
    "                 \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retira elementos de uma lista aninhada\n",
    "#for i in range(len(test_sentences_label_1)):\n",
    "#    test_sentences_label_1[i] = [i[0] for i in test_sentences_label_1[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retira elementos de uma lista aninhada\n",
    "#matrix = test_sentences_label_1\n",
    "#sentencesOL = [] \n",
    "#for sublist in matrix: \n",
    "#    for val in sublist: \n",
    "#        sentencesOL.append(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(sentencesOL[0]) "
   ]
  },
  {
   "source": [
    "Extrai os rótulos reais y_test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [sent2labelsO(sentences) for sentences in test_sentences_label_1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('word do y_test   ==>', test_sentences_label_1[140],'\\n')\n",
    "print('word do y_pred   ==>', test_sentences_1[140],'\\n')\n",
    "#print('label do y_test  ==>',y_test[0],'\\n')\n",
    "#print('label do y_test  ==>',y_pred[0],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho y_pred\", len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##está dentro de duas uma lista = test_sentences_1 = texto sem rotulo\n",
    "test_sentences_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##está dentro de apenas uma lista = y_pred = rotulos preditos de test_sentences_1\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Muda o label na lista para o formato do Ontonotes  \n",
    "lista = []\n",
    "for p in y_pred:\n",
    "  for x in t:\n",
    "    #print(x)\n",
    "    if (x=='B-LOC'):\n",
    "        print(x)\n",
    "        lista.append('LOC')\n",
    "    if (x=='B-PER'):\n",
    "        print(x)\n",
    "        lista.append('PER')\n",
    "        print(lista) \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retira os espaços em branco da lista test_sentences_1\n",
    "lista=[]\n",
    "for t in test_sentences_1:\n",
    "    if len(t)!=0:\n",
    "        lista.append([i[0] for i in t])\n",
    "    #else:\n",
    "    #    lista.append('\\n')\n",
    "#lista[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retira os espaços em branco da lista test_sentences_1\n",
    "lista_y=[]\n",
    "for t in test_sentences_label_1:\n",
    "    if len(t)!=0:\n",
    "        lista_y.append([i[0] for i in t])\n",
    "    #else:\n",
    "    #    lista.append('\\n')\n",
    "#lista[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setenca=(zip(t[0],t[1])) for t in zip(test_sentences_1, y_pred)\n",
    "#zip(test_sentences_1[],y_pred[])\n",
    "#substantivo = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "#print(substantivo)\n",
    "#adjetivo = ['be', 'closed', 'for', 'the', 'rest', 'of', 'the', 'week', 'Anderson', '.']\n",
    "#print(adjetivo)\n",
    "#['%s %s' % (s, a) for s in substantivo for a in adjetivo ]\n",
    "#['pão pequeno', 'pé pequeno', 'carro caro', 'bolo bonito', 'bolo bom']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #t=list('%s %s' % (s, a) for s in lista[0] for a in y_pred[0])\n",
    " #t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Concatena a palavra e o rotulo predito\n",
    "l = [list(zip(x, y)) for x, y in zip(lista, lista_y)]\n",
    "#print(l[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in y_pred:\n",
    "    for r in p:\n",
    "#        #print(r)\n",
    "        if (r=='LOC' or r=='PER'):\n",
    "            #n=[i for i,x in enumerate(r)]\n",
    "            a=[list(zip(lista, lista_y))]\n",
    "            print(a)\n",
    "            #Concatena a palavra e o rotulo predito\n",
    "\n",
    "#print(l[0:20]\n",
    "#transforma a lista em sentenca\n",
    "texto_1 = ''\n",
    "for linha in a:\n",
    "    for z in linha:\n",
    "        texto_1='sentenca'+ (texto_1) +(z) + '\\n'\n",
    "# Write the file out again\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/train_labeled_comp.txt', 'wt') as fileout:\n",
    "  fileout.write(texto_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforma a lista em sentenca\n",
    "texto = ''\n",
    "for linha in l:\n",
    "    for z,t in linha:\n",
    "        texto=texto + str(z)+' '+ t + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file out again\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/train_labeled.txt', 'wt') as fileout:\n",
    "  fileout.write(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = texto.replace('I-','').replace('O-','')\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texto = 'who was tagged for a pair of homers by Mike Devereaux and Brady Anderson and three runs in the ninth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [Texto.split()]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percorre a letra dentro da word dentro da sentenca\n",
    "X_t1 = [[sent2featuresO(s) for s in text] for text in tokenized]\n",
    "X_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percorre a word dentro da sentença, mas sem criar uma nova lista\n",
    "X_t1 =  [sent2featuresO(s) for s in tokenized]\n",
    "X_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = crf.predict(X_t1)\n",
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pega apenas a word da\n",
    "for i in range(len(X_t1)):\n",
    "        X_t1[i] = [i[0] for i in X_t1[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('predicao', l[0] )\n",
    "print ('rotulos reais', y_test[0], test_sentences_label[0])"
   ]
  },
  {
   "source": [
    "Avaliação\n",
    "\n",
    "There is much more O entities in data set, but we’re more interested in other entities. To account for this we’ll use averaged F1 score computed for all labels except for O. sklearn-crfsuite.metrics package provides some useful metrics for sequence classification task, including this one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERSON == 'B-PER' 'I-PER' (CONLL), ORG == 'B-ORG', GPE == 'B-LOC' 'I-LOC' (CONLL), MISC == TUDO QUE NAO AS OUTRAS 3 NO CONLL\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Imprime os Labels do Y_pred diferentes de 'O'\n",
    "\n",
    "#for i in range(len(y_pred)):\n",
    "#    if y_pred[i]!= ('O'): \n",
    "#        print(y_pred[i])    \n",
    "\n",
    "for linha in y_pred:        \n",
    "    #print(list(linha))\n",
    "    for palavra in linha:\n",
    "          if palavra!= ('O'): \n",
    "                print(palavra)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprime o tamanho do y_test e y_pred e qtde de label predita\n",
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho y_test\", len(y_test))\n",
    "print(\"tamanho y_pred\", len(y_pred))\n",
    "\n",
    "#Ontonotes sem rotulos ==> 1639268 (linhas)\n",
    "#                           153956 (sentencas)\n",
    "\n",
    "#Ontonotes com rotulos y_ test==> 3160787 (linhas)\n",
    "#                            98836 (sentencas) \n",
    "\n",
    "#ValueError: Found input variables with inconsistent numbers of samples: [3004715, 1485302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "source": [
    "Inspect per-class results in more detail:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "source": [
    "Hyperparameter Optimization\n",
    "\n",
    "To improve quality try to select regularization parameters using randomized search and 3-fold cross-validation.\n",
    "\n",
    "I takes quite a lot of CPU time and RAM (we’re fitting a model 50 * 3 = 150 times), so grab a tea and be patient, or reduce n_iter in RandomizedSearchCV, or fit model only on a subset of training data.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Best result:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check parameter space\n",
    "\n",
    "A chart which shows which c1 and c2 values have RandomizedSearchCV checked. Red color means better results, blue means worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "source": [
    "Check best estimator on our test data\n",
    "\n",
    "As you can see, quality is improved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n"
   ]
  },
  {
   "source": [
    "Let’s check what classifier learned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s check what classifier learned\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized.\n",
    "\n",
    "Check the state features:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## carregar a base ontonotes - ok\n",
    "## fazer mapeamento das labels - ok\n",
    "## fazer a predição - ok\n",
    "## fazer a comparação entre y_predito e y_real - ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Manipulando lista\n",
    "l = [1,2,3,4,5]\n",
    "\n",
    "#criando listas\n",
    "l = [[w] for w in \"We dont like it\".split()]\n",
    "l\n",
    "\n",
    "def sum(i):\n",
    "        return i+20\n",
    "[sum(x) for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(test_sentences_label_1[0]).strip('[]')\n",
    "#t=','.join(test_sentences_label_1)\n",
    "cars = (['rav4'], ['td5'], ['yaris'], ['land rover tdi']) \n",
    "\n",
    "print(\"I like the \"+cars[0][0]+\" ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}