{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "source": [
    "Let's use CoNLL 2003 data to build a NER system\n",
    "\n",
    "We use English data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Passo 1 - Treina o modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conll2003\n",
    "with open('/home/82068895153/POS/skweak/data/conll2003_dataset/train.txt', 'r') as file:\n",
    "  sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print (sentences[5])\n",
    " len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in sentences[5:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quebra a sentença em lista\n",
    "sentences_1=preprocess(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \n",
    "    word = sent[i][0]\n",
    "    #print ('word', word)\n",
    "    postag = sent[i][1]\n",
    "    #print ('postag', postag)\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],        \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]        \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, __, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, __, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in sentences_1]\n",
    "\n",
    "y_train = [sent2labels(s) for s in sentences_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Passo 2 - Prepara o Y_test a partir do dataset do Ontonotes "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 - Abre o Ontonotes para aplicar o tratamento \n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train.txt', 'r') as file:\n",
    "   sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.2 - Retira os espaços em branco e as words maiores que duas posições\n",
    "for i in range(len(sentences) - 1):\n",
    "    #print(sentences[116:122])\n",
    "    atual = sentences[i].split()\n",
    "    proximo = sentences[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        sentences[i] = '\\t'.join(atual)+'\\n'\n",
    "        proximo = proximo[1:]\n",
    "        sentences[i+1] = '\\t'.join(proximo)+'\\n'\n",
    "        #print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[116:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.3 - Verifica as linhas com mais de duas words e concatena\n",
    "for i in range(len(sentences) - 1):\n",
    "        atual = sentences[i].split()\n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]))+' '+atual[2]+'\\n' \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] +'\\n'  \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4] +'\\n' \n",
    "            #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[116:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 - Faz a troca dos labels\n",
    "def processarLinha(sentences):\n",
    "    #print(sentences)\n",
    "    #print(sentences[1])\n",
    "    #count = 0\n",
    "    # Write the file out again\n",
    "    with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'wt') as fileout:\n",
    "        for linha in sentences:\n",
    "            #count = count +1\n",
    "            #print (linha)\n",
    "            lista = linha.split(\"\\t\")\n",
    "            \n",
    "            #print (len(lista))\n",
    "            #print (lista)     \n",
    "            if len(lista)==2 and lista[1] == 'PERSON\\n':\n",
    "                lista[1]= 'B-PER\\n'\n",
    "                #print (lista)\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])\n",
    "            elif len(lista)==2 and lista[1] == 'GPE\\n': \n",
    "                lista[1]= 'B-LOC\\n'\n",
    "                #print (lista)\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])         \n",
    "            elif len(lista)==2:\n",
    "                fileout.write(lista[0]+'\\t'+lista[1])\n",
    "            #if count == 15:\n",
    "              #  break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processarLinha(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 - Abre o Ontonotes após aplicar ajuste do label\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_label.txt', 'r') as file:\n",
    "   sentences_trat = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(sentences_trat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Insere linha entre as sentenças\n",
    "arq1 = ''\n",
    "for linha in sentences_trat:\n",
    "    p=linha.find('.')\n",
    "    #print('linha', linha)\n",
    "    #print ('p == ',p)\n",
    "    if p==0:\n",
    "        arq1=arq1+linha+'\\n'\n",
    "        #print ('arq de p0 == ',arq)\n",
    "    else:\n",
    "        arq1=arq1+linha\n",
    "        #print ('arq de p = . == ',arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Grava o arquivo após o tratamento\n",
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq1.txt', 'wt') as fileout:\n",
    "#    fileout.write(arq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o y_test\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq1.txt', 'r') as file:\n",
    "   arq_sentences = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 - cria a função para inserir tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_b(arq_sentences):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in arq_sentences[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.9  Aplica a função de tokens ao arquivo aberto\n",
    "arq_sentences_1 = preprocess_b(arq_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arq_sentences_1[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 - Função para extrair os rotulos reais\n",
    "def sent2labelsO(sent):\n",
    "    return [label for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 - Verifica se o dataset tem mais elementos que a chamado do métoddo sent2labels\n",
    "for sentences in arq_sentences_1:\n",
    "    try: \n",
    "        _ = sent2labelsO(sentences)\n",
    "    except ValueError:\n",
    "        for word in sentences:\n",
    "            if len(word)!= 2:\n",
    "                print(f' {word} possui {len(word)} elementos.')\n",
    "                 \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 Extrai os rótulos reais y_test\n",
    "y_test = [sent2labelsO(sentences) for sentences in arq_sentences_1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "source": [
    "Passo 3 - Carrega o X_test a partir do dataset do Ontonotes "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.1 - Abre o Ontonotes para aplicar o tratamento \n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train.txt', 'r') as file:\n",
    "   sentences_x = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 - Retira os espaços em branco e as words maiores que duas posições\n",
    "for i in range(len(sentences_x) - 1):\n",
    "    #print(sentences[116:122])\n",
    "    atual = sentences_x[i].split()\n",
    "    proximo = sentences_x[i+1].split()\n",
    "    if len(atual) == 0:\n",
    "        continue\n",
    "    while len(proximo) > 2:\n",
    "        #print(f'Convertendo ({atual}) e ({proximo}) para ', end = '')\n",
    "        atual[0] += proximo[0]\n",
    "        sentences_x[i] = '\\t'.join(atual)+'\\n'\n",
    "        proximo = proximo[1:]\n",
    "        sentences_x[i+1] = '\\t'.join(proximo)+'\\n'\n",
    "        #print(f'({atual}) e ({proximo})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 - Verifica as linhas com mais de duas words e concatena\n",
    "for i in range(len(sentences_x) - 1):\n",
    "        atual = sentences_x[i].split()\n",
    "        if ((len(atual)>2) and (len(atual)<=3)):\n",
    "            #print('atual', atual)\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]))+' '+atual[2]+'\\n' \n",
    "            #print('sentences', sentences_x[i])\n",
    "            #print(i)\n",
    "        elif ((len(atual)>3) and (len(atual)<=4)):\n",
    "            #print('atual', atual)\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]+atual[2]))+' '+atual[3] +'\\n'  \n",
    "            #print('sentences', sentences[i])\n",
    "            #print(i)\n",
    "        elif (len(atual)>4):\n",
    "            sentences_x[i]=(''.join(atual[0]+atual[1]+atual[2]+atual[3]))+' '+atual[4] +'\\n' \n",
    "            #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.4 - Remove o label original\n",
    "texto=''\n",
    "#count=0\n",
    "for linha in sentences_x:\n",
    "    if len(linha) != 1:\n",
    "        x=linha.split()[0]              \n",
    "        #print('x-->', x)\n",
    "        texto=texto+x+'\\n'\n",
    "        continue\n",
    "                \n",
    "                #verifica se a linha tem mais de 2 palavras\n",
    "                # if len(x) > 2:\n",
    "                #     #print('split', x)\n",
    "                #     print(f' {x} possui {len(x)} elementos.') \n",
    "                # if len(x) == 1:\n",
    "                #     #print('split', x)\n",
    "                #     print(f' {x} possui {len(x)} elementos.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texto[0:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Grava o arquivo após o tratamento\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_2.txt', 'wt') as fileout:\n",
    "    fileout.write(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o X_text\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_2.txt', 'r') as file:\n",
    "   arq_sentences_2 = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arq_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5 - Insere linha entre as sentenças\n",
    "arq_2 = ''\n",
    "for linha in arq_sentences_2:\n",
    "    p=linha.find('.')\n",
    "    #print('linha', linha)\n",
    "    #print ('p == ',p)\n",
    "    if p==0:\n",
    "        arq_2=arq_2+linha+'\\n'\n",
    "        #print ('arq de p0 == ',arq)\n",
    "    else:\n",
    "        arq_2=arq_2+linha\n",
    "        #print ('arq de p = . == ',arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 - Grava o arquivo após o tratamento\n",
    "#with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_3.txt', 'wt') as fileout:\n",
    "#    fileout.write(arq_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 - Abre o Ontonotes após concluir o tratamento para carregar o X_text\n",
    "with open('/home/82068895153/POS/skweak/data/Ontonotes/ner_train_arq_3.txt', 'r') as file:\n",
    "   arq_sentences_3 = list(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_sentences_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 - Cria os tokens dentro das sentencas -- quebra a setenca em uma lista\n",
    "def preprocess_O(arq_sentences_3):\n",
    "    l_sentences = []\n",
    "    l1_ = []\n",
    "    for token in arq_sentences_3[0:]: #a partir da quinta posicao\n",
    "    #for token in sentences:\n",
    "        #print('token==>', token)\n",
    "        cls = token.split()    \n",
    "        #print('token.split==>', cls)\n",
    "        if len(cls) != 0:\n",
    "            l1_.append(cls)\n",
    "            #print('apos o append==>', l1_)\n",
    "        else:\n",
    "            l_sentences.append(l1_)\n",
    "            l1_ = []\n",
    "    return l_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.9 - Transforma a sentença em lista\n",
    "test_sentences_2=preprocess_O(arq_sentences_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print(test_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.0 - Função para extrair as features do texto a ser rotulado\n",
    "\n",
    "def word2featuresO(sent, i):\n",
    "    #word = sent[i][0]\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit()  \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]       \n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1 - Função para chamada da Função para extrair as features do texto a ser rotulado\n",
    "def sent2featuresO(sent):\n",
    "    return [word2featuresO(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.2 - Extrai as fetatures de X_test \n",
    "X_test = [[sent2featuresO(s) for s in text] for text in test_sentences_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'bias': 1.0,\n",
       "   'word.lower()': 'the',\n",
       "   'word[-3:]': 'The',\n",
       "   'word[-2:]': 'he',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'school',\n",
       "   'word[-3:]': 'ool',\n",
       "   'word[-2:]': 'ol',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'is',\n",
       "   'word[-3:]': 'is',\n",
       "   'word[-2:]': 'is',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'going',\n",
       "   'word[-3:]': 'ing',\n",
       "   'word[-2:]': 'ng',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'to',\n",
       "   'word[-3:]': 'to',\n",
       "   'word[-2:]': 'to',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'be',\n",
       "   'word[-3:]': 'be',\n",
       "   'word[-2:]': 'be',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'closed',\n",
       "   'word[-3:]': 'sed',\n",
       "   'word[-2:]': 'ed',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'for',\n",
       "   'word[-3:]': 'for',\n",
       "   'word[-2:]': 'or',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'the',\n",
       "   'word[-3:]': 'the',\n",
       "   'word[-2:]': 'he',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'rest',\n",
       "   'word[-3:]': 'est',\n",
       "   'word[-2:]': 'st',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'of',\n",
       "   'word[-3:]': 'of',\n",
       "   'word[-2:]': 'of',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'the',\n",
       "   'word[-3:]': 'the',\n",
       "   'word[-2:]': 'he',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'week',\n",
       "   'word[-3:]': 'eek',\n",
       "   'word[-2:]': 'ek',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': 'anderson',\n",
       "   'word[-3:]': 'son',\n",
       "   'word[-2:]': 'on',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': True,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}],\n",
       " [{'bias': 1.0,\n",
       "   'word.lower()': '.',\n",
       "   'word[-3:]': '.',\n",
       "   'word[-2:]': '.',\n",
       "   'word.isupper()': False,\n",
       "   'word.istitle()': False,\n",
       "   'word.isdigit()': False,\n",
       "   'BOS': True,\n",
       "   'EOS': True}]]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 - Retira o X_teste de dentro da lista aninhada para execução do y_pred\n",
    "for i in range(len(X_test)):\n",
    "        X_test[i] = [i[0] for i in X_test[i]]"
   ]
  },
  {
   "source": [
    "4 - Passo: Gerar o y_pred = rotulos preditos para o texto não rotulado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 - Aplica o modelo treinado no dataset sem rotulos\n",
    "y_pred = crf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "98807"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'arq_sentences_2' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10933/1273381583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m  \u001b[0;31m#len(sentences_trat) , len(arq_sentences_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marq_sentences_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpredicao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marq_sentences_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msentences_trat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arq_sentences_2' is not defined"
     ]
    }
   ],
   "source": [
    "#Checa se as word do Y_test com o Y_pred estão batendo\n",
    " #len(sentences_trat) , len(arq_sentences_2) \n",
    " \n",
    "for i in range(len(arq_sentences_2)):\n",
    "    predicao = arq_sentences_2[i].split()\n",
    "    real =  sentences_trat[i].split()\n",
    "    #print('predicao',predicao[0] )\n",
    "    #print('real', real[0])\n",
    "    if predicao[0] != real[0]:\n",
    "        print(i)\n",
    "        print('real', real, count)\n",
    "        print('predicao', predicao, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('predicao', arq_sentences_2[141346:141349].split())\n",
    "print ('real' ,  sentences_trat[141346:141349].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word do y_test   ==> [['angstrom', 'B-PER'], ['Larry', 'B-PER'], ['Drury', 'B-PER'], [',', 'O'], ['attorney', 'O'], ['for', 'O'], ['the', 'O'], ['plaintiffs', 'O'], [',', 'O'], ['valued', 'O'], ['the', 'O'], ['settlement', 'O'], ['at', 'O'], ['between', 'MONEY'], ['$', 'MONEY'], ['6', 'MONEY'], ['million', 'MONEY'], ['and', 'MONEY'], ['$', 'MONEY'], ['8', 'MONEY'], ['million', 'MONEY'], ['.', 'O']] \n\nword do y_pred   ==> [['angstrom'], ['Larry'], ['Drury'], [','], ['attorney'], ['for'], ['the'], ['plaintiffs'], [','], ['valued'], ['the'], ['settlement'], ['at'], ['between'], ['$'], ['6'], ['million'], ['and'], ['$'], ['8'], ['million'], ['.']] \n\nlabel do y_test  ==> ['B-PER', 'B-PER', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'MONEY', 'MONEY', 'MONEY', 'MONEY', 'MONEY', 'MONEY', 'MONEY', 'MONEY', 'O'] \n\nlabel do y_pred  ==> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] \n\n"
     ]
    }
   ],
   "source": [
    "print('word do y_test   ==>', arq_sentences_1[140],'\\n')\n",
    "print('word do y_pred   ==>', test_sentences_2[140],'\\n')\n",
    "print('label do y_test  ==>',y_test[140],'\\n')\n",
    "print('label do y_pred  ==>',y_pred[140],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Qtde labels preditos 4238\ntamanho y_pred 98807\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho y_pred\", len(y_pred))"
   ]
  },
  {
   "source": [
    "Avaliação\n",
    "\n",
    "There is much more O entities in data set, but we’re more interested in other entities. To account for this we’ll use averaged F1 score computed for all labels except for O. sklearn-crfsuite.metrics package provides some useful metrics for sequence classification task, including this one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'B-ORG', 'I-ORG', 'I-MISC', 'I-LOC']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "#PERSON == 'B-PER' 'I-PER' (CONLL), ORG == 'B-ORG', GPE == 'B-LOC' 'I-LOC' (CONLL), MISC == TUDO QUE NAO AS OUTRAS 3 NO CONLL\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Qtde labels preditos 4238\ntamanho em sentencas y_test 98807\ntamanho em sentencas y_pred 98807\ntamanho em linhas y_test  ==> 3103448\ntamanho em linhas y_pred  ==> 3103448\n"
     ]
    }
   ],
   "source": [
    "#Imprime o tamanho do y_test e y_pred e qtde de label predita\n",
    "count = 0\n",
    "for i,x in enumerate(y_pred):\n",
    "        ss=set(x)\n",
    "        if len(ss) > 1:\n",
    "            count+=1\n",
    "print(\"Qtde labels preditos\", count)\n",
    "print(\"tamanho em sentencas y_test\", len(y_test))\n",
    "print(\"tamanho em sentencas y_pred\", len(y_pred))\n",
    "print('tamanho em linhas y_test  ==>', len(arq_sentences))\n",
    "print('tamanho em linhas y_pred  ==>', len(arq_sentences_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.06900161764121217"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "source": [
    "Inspect per-class results in more detail:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n       B-LOC      0.817     0.071     0.130     46165\n       I-LOC      0.000     0.000     0.000         0\n      B-MISC      0.000     0.000     0.000         0\n      I-MISC      0.000     0.000     0.000         0\n       B-ORG      0.000     0.000     0.000         0\n       I-ORG      0.000     0.000     0.000         0\n       B-PER      0.517     0.006     0.012     49382\n       I-PER      0.000     0.000     0.000         0\n\n   micro avg      0.754     0.037     0.071     95547\n   macro avg      0.167     0.010     0.018     95547\nweighted avg      0.662     0.037     0.069     95547\n\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "source": [
    "Hyperparameter Optimization\n",
    "\n",
    "To improve quality try to select regularization parameters using randomized search and 3-fold cross-validation.\n",
    "\n",
    "I takes quite a lot of CPU time and RAM (we’re fitting a model 50 * 3 = 150 times), so grab a tea and be patient, or reduce n_iter in RandomizedSearchCV, or fit model only on a subset of training data.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Best result:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check parameter space\n",
    "\n",
    "A chart which shows which c1 and c2 values have RandomizedSearchCV checked. Red color means better results, blue means worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = [s.parameters['c1'] for s in rs.grid_scores_]\n",
    "_y = [s.parameters['c2'] for s in rs.grid_scores_]\n",
    "_c = [s.mean_validation_score for s in rs.grid_scores_]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "source": [
    "Check best estimator on our test data\n",
    "\n",
    "As you can see, quality is improved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n"
   ]
  },
  {
   "source": [
    "Let’s check what classifier learned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s check what classifier learned\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "source": [
    "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized.\n",
    "\n",
    "Check the state features:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}