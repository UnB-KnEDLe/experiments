{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cnn_cnn_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpgrC8PIHeXq",
        "outputId": "a21aaa39-715f-4b15-d86d-a15266cf558a"
      },
      "source": [
        "!pip install seqval\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement seqval (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for seqval\u001b[0m\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 127kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig3nqBkZOQGq",
        "outputId": "2c6c5401-2f06-4abf-d194-ed1c5db9f010"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "P = 'drive/MyDrive/knedle-data/NER_dataset'\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbt3cCQBC5kx",
        "outputId": "60c38314-8593-4d0d-ce32-e8975a00ea41"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJYnQMjEszNX",
        "outputId": "4aac019e-7783-4a87-e038-8d20beb3db26"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 26.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 26.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=f9d19dd6cea4ef6543c74ca6810e96f0de3706f91920f9ef2a6f48a889380222\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb8xS8XLsrfi",
        "outputId": "d20ca21d-80d1-4ae4-e1e0-fbb81933cf6a"
      },
      "source": [
        "# Basic packages\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from gensim.models import KeyedVectors\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "import joblib\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import math\n",
        "from random import sample\n",
        "# NER open packages\n",
        "from seqeval.scheme import IOBES\n",
        "from seqeval.metrics import f1_score\n",
        "# my NER packages\n",
        "from data3 import active_dataset\n",
        "from utils import create_char2idx_dict, create_tag2idx_dict, create_word2idx_dict, new_custom_collate_fn, budget_limit, find_iobes_entities, find_iobes_entities2\n",
        "from metrics import exact_f1_score, preprocess_pred_targ, IOBES_tags\n",
        "from CNN_biLSTM_CRF import cnn_bilstm_crf\n",
        "from CNN_CNN_LSTM2 import CNN_CNN_LSTM\n",
        "\n",
        "import utils\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6dLVjfps_PS",
        "outputId": "55f2e3f0-344b-4e96-f8a5-25f003a0715d"
      },
      "source": [
        "\n",
        "DATA_PATH=Path('drive/MyDrive/knedle_data/NER_dataset')\n",
        "SAVE_PATH=Path('drive/MyDrive/knedle_data/NER_results')\n",
        "\n",
        "args = dict(\n",
        "    save_path=SAVE_PATH,\n",
        "    epochs=50,\n",
        "    dataset='aposentadoria',\n",
        "    model='CNN-CNN-LSTM',\n",
        "    lstm_hidden_size=128,\n",
        "    batch_size=16,\n",
        "    use_dev_set=False\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--save_path', \n",
        "    action='store', dest='save_path', \n",
        "    default=SAVE_PATH, type=str,\n",
        "    # default='experiments/Supervised/', type=str\n",
        "    )\n",
        "parser.add_argument(\n",
        "    '--epochs', \n",
        "    action='store', \n",
        "    dest='epochs', default=50, type=int)\n",
        "parser.add_argument(\n",
        "    '--dataset', \n",
        "    action='store', \n",
        "    dest='dataset', default='aposentadoria', type=str)\n",
        "parser.add_argument(\n",
        "    '--model', \n",
        "    action='store', \n",
        "    dest='model', default = 'CNN-CNN-LSTM', type=str)\n",
        "parser.add_argument(\n",
        "    '--lstm_hidden_size', \n",
        "    action='store', \n",
        "    dest='lstm_hidden_size', default=128, type=int)\n",
        "parser.add_argument(\n",
        "    '--batch_size', \n",
        "    action='store', \n",
        "    dest='batch_size', default=16, type=int)\n",
        "parser.add_argument(\n",
        "    '--use_dev_set', \n",
        "    action = 'store', \n",
        "    dest='use_dev_set', default=False, type=bool)\n",
        "# parser_opt = parser.parse_args()\n",
        "parser_opt, unknown = parser.parse_known_args()\n",
        "print(f'Experiment:')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ0FXPFfs_Mt"
      },
      "source": [
        "from importlib import reload as rl\n",
        "import utils\n",
        "rl(utils)\n",
        "\n",
        "if parser_opt.dataset not in ['ontonotes', 'conll', 'aposentadoria']:\n",
        "    raise ValueError(\n",
        "        'Dataset not recognized. Options are: conll, ontonotes and aposentadoria'\n",
        "    )\n",
        "\n",
        "emb, train_path, test_path, data_format = utils.load_embedding(parser_opt, DATA_PATH)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HLiyYcw6-I3",
        "outputId": "545926a8-b5bf-4909-ee2e-82dceda4116d"
      },
      "source": [
        "rl(utils)\n",
        "\n",
        "collate_object = utils.new_custom_collate_fn(\n",
        "    pad_idx=emb.key_to_index['<PAD>'], \n",
        "    unk_idx=emb.key_to_index['<UNK>']\n",
        "    # pad_idx=emb.vocab['<PAD>'].index, \n",
        "    # unk_idx=emb.vocab['<UNK>'].index\n",
        ")\n",
        "\n",
        "print('\\nGenerating text2idx dictionaries (word, char, tag)')\n",
        "word2idx = utils.create_word2idx_dict(emb, train_path)\n",
        "char2idx = utils.create_char2idx_dict(train_path=train_path)\n",
        "tag2idx  = utils.create_tag2idx_dict(train_path=train_path)\n",
        "\n",
        "print('\\nCreating training dataset')\n",
        "train_set = active_dataset(\n",
        "    data=train_path, \n",
        "    word2idx_dic=word2idx, \n",
        "    char2idx_dic=char2idx, tag2idx_dic=tag2idx, data_format=data_format)\n",
        "# Putting all sentences into the labeled set for training\n",
        "train_set.flag_labeled = False\n",
        "train_set.label_data([i for i in range(len(train_set))])\n",
        "train_set.flag_labeled = True\n",
        "\n",
        "print('\\nCreating test dataset')\n",
        "test_set  = active_dataset(\n",
        "    data=test_path, \n",
        "    word2idx_dic=word2idx, \n",
        "    char2idx_dic=char2idx, tag2idx_dic=tag2idx, data_format=data_format)\n",
        "# Putting all sentences into the labeled set for testing\n",
        "test_set.flag_labeled = False\n",
        "test_set.label_data([i for i in range(len(test_set))])\n",
        "test_set.flag_labeled = True\n",
        "test_dataloader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate_object)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generating text2idx dictionaries (word, char, tag)\n",
            "train_path: drive/MyDrive/knedle_data/NER_dataset/aposentadoria/aposentadoria_train.txt\n",
            "\n",
            "Creating training dataset\n",
            "\n",
            "Creating test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry4WE2zn6-Gu",
        "outputId": "72823986-1b9c-4311-dd15-e3a08b72023e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_opt = parser_opt.model\n",
        "                     \n",
        "# CNN-CNN-LSTM (greedy decoding) model\n",
        "if model_opt == 'CNN-CNN-LSTM':\n",
        "    model = CNN_CNN_LSTM(char_vocab_size=len(char2idx),\n",
        "                                char_embedding_dim=25,\n",
        "                                char_out_channels=50,\n",
        "                                pretrained_word_emb=emb,\n",
        "                                word2idx = word2idx,\n",
        "                                word_out_channels=400,\n",
        "                                word_conv_layers = 1,\n",
        "                                num_classes=len(tag2idx),\n",
        "                                decoder_layers = 1,\n",
        "                                decoder_hidden_size = 128,\n",
        "                                device=device)\n",
        "    lrate = 0.010\n",
        "    new_lrate = 0.010\n",
        "    momentum = 0.9\n",
        "    clipping_value = 5.0\n",
        "    flag_adjust_lrate = False\n",
        "    \n",
        "elif model_opt == 'CNN-biLSTM-CRF':\n",
        "    model = cnn_bilstm_crf(char_vocab_size=len(char2idx), \n",
        "                   char_embedding_dim=30, \n",
        "                   char_out_channels=30, \n",
        "                   pretrained_word_emb=emb, \n",
        "                   num_classes=len(tag2idx), \n",
        "                   device=device, \n",
        "                   lstm_hidden_size=parser_opt.lstm_hidden_size)\n",
        "    lrate = 0.0025\n",
        "    clipping_value = 5.0\n",
        "    momentum = 0.9\n",
        "    flag_adjust_lrate = False\n",
        "\n",
        "model.to(device)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_CNN_LSTM(\n",
              "  (char_encoder): char_cnn(\n",
              "    (embedding): Embedding(65, 25, padding_idx=0)\n",
              "    (conv): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (relu): ReLU()\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (word_encoder): word_cnn(\n",
              "    (embedding): Embedding(117, 50)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (convnet): Sequential(\n",
              "      (0): Conv1d(100, 400, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): decoder(\n",
              "    (lstm): LSTM(542, 128)\n",
              "    (linear): Linear(in_features=128, out_features=42, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (loss_fn): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVHsnw8F6-Eg",
        "outputId": "c78932e0-f79d-4cc4-d747-0e6c33ec7e6a"
      },
      "source": [
        "supervised_epochs = parser_opt.epochs\n",
        "optim = torch.optim.SGD(model.parameters(), lr=lrate, momentum=momentum)\n",
        "\n",
        "\n",
        "# ==============================================================================================\n",
        "# ==============================================================================================\n",
        "# ============================= Supervised learning algorithm ==================================\n",
        "# ==============================================================================================\n",
        "# ==============================================================================================\n",
        "print(f'\\nInitiating supervised training\\n\\n')\n",
        "f1_history = []\n",
        "\n",
        "train_set.flag_labeled = True\n",
        "batch_size = parser_opt.batch_size\n",
        "dataloader = DataLoader(train_set, batch_size=batch_size, pin_memory=True, collate_fn = collate_object, shuffle=False)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initiating supervised training\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9UFyccl6-CD",
        "outputId": "f2c96df6-4114-4581-ff62-ad6ca9d2b241"
      },
      "source": [
        "for epoch in range(10):\n",
        "# for epoch in range(supervised_epochs):\n",
        "    print(f'Epoch: {epoch}')\n",
        "    model.train()        \n",
        "    for sent, tag, word, mask in dataloader:\n",
        "        sent = sent.to(device)\n",
        "        tag = tag.to(device)\n",
        "        word = word.to(device)\n",
        "        mask = mask.to(device)\n",
        "        optim.zero_grad()\n",
        "        loss = model(sent, word, tag, mask)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
        "        optim.step()\n",
        "    \n",
        "    # Verify performance on test set after supervised training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions, targets = preprocess_pred_targ(model, test_dataloader, device)\n",
        "        predictions = IOBES_tags(predictions, tag2idx)\n",
        "        targets = IOBES_tags(targets, tag2idx)\n",
        "        micro_f1 = f1_score(targets, predictions, mode='strict', scheme=IOBES)\n",
        "        f1_history.append(0 if np.isnan(micro_f1) else micro_f1)\n",
        "        print(f'micro f1-score: {micro_f1}\\n')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "micro f1-score: 0.08015478164731896\n",
            "\n",
            "Epoch: 1\n",
            "micro f1-score: 0.5501415285078851\n",
            "\n",
            "Epoch: 2\n",
            "micro f1-score: 0.7710636838947458\n",
            "\n",
            "Epoch: 3\n",
            "micro f1-score: 0.8986074166797058\n",
            "\n",
            "Epoch: 4\n",
            "micro f1-score: 0.9639034472576954\n",
            "\n",
            "Epoch: 5\n",
            "micro f1-score: 0.9647163251719801\n",
            "\n",
            "Epoch: 6\n",
            "micro f1-score: 0.9731474688187822\n",
            "\n",
            "Epoch: 7\n",
            "micro f1-score: 0.9761505360659325\n",
            "\n",
            "Epoch: 8\n",
            "micro f1-score: 0.9849056603773584\n",
            "\n",
            "Epoch: 9\n",
            "micro f1-score: 0.9845596230518304\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXBldSks69_v",
        "outputId": "b6294cde-bd26-4b73-a675-7b849ddc28e2"
      },
      "source": [
        "hyperparams = {'model': str(model), 'LR': lrate, 'momentum': momentum, 'clipping': clipping_value}\n",
        "dic = {'f1_hist': f1_history, 'hyperparams': hyperparams}\n",
        "path = parser_opt.save_path.as_posix()\n",
        "\n",
        "from glob import glob\n",
        "cnt = 1\n",
        "for name in glob(path+'/*.pkl'):\n",
        "    if model_opt in name:\n",
        "        cnt += 1\n",
        "f_name = model_opt + '_' + str(cnt) + '.pkl'\n",
        "\n",
        "joblib.dump(dic, path + f_name)\n",
        "print(f'Training saved in: {path + f_name}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training saved in: drive/MyDrive/knedle_data/NER_resultsCNN-CNN-LSTM_1.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADkkHRzK698X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz1A_SLb695z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQh1XvSms-5x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNtAZeJ5sT53"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADylvbI_OrrS",
        "outputId": "3a5b7d08-4faa-46d1-f451-9a2b6fcdeda1"
      },
      "source": [
        "!ls drive/MyDrive/knedle_data/NER_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aposentadoria  conll03\tREADME.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaisrOjCzv8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from math import sqrt\n",
        "# from torchcrf import CRF\n",
        "# from drive.MyDrive.NER_code.crf import CRF\n",
        "from crf import CRF\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class char_cnn(nn.Module):\n",
        "    \"\"\"single layer CNN with: filters=50, kernel_size=3, dropout=0.5\n",
        "    CHANGES:\n",
        "    - kaiming_uniform initialization of convolution weights\n",
        "    - Embedding dropout with p=0.25\n",
        "    - dropout now only applied on output of convolution layers (after activation function)\n",
        "    - Added weight_norm to conv layers\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, embedding_dim, char_out_channels):\n",
        "        super(char_cnn, self).__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=embedding_size, \n",
        "            embedding_dim=embedding_dim, \n",
        "            padding_idx=0)\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=embedding_dim, \n",
        "            out_channels=char_out_channels,\n",
        "            kernel_size=3, \n",
        "            stride=1, \n",
        "            padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.emb_dropout = nn.Dropout(p=0.5)\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        bias = sqrt(3/self.embedding.embedding_dim)\n",
        "        nn.init.uniform_(self.embedding.weight, -bias, bias)\n",
        "        # nn.init.kaiming_uniform_(self.conv.weight.data, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb_dropout(self.embedding(x))\n",
        "        shape = x.shape\n",
        "        x = self.conv(\n",
        "            x.reshape([shape[0]*shape[1], shape[2], shape[3]]).permute(0, 2, 1)\n",
        "        )\n",
        "        # x = self.relu(x)\n",
        "        # x = self.dropout(self.relu(x))\n",
        "        x = torch.nn.functional.max_pool1d(x, kernel_size=x.shape[2]).squeeze(2)\n",
        "        return x.reshape([shape[0], shape[1], -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32JHSQ4STKY"
      },
      "source": [
        "\n",
        "class bilstm_crf(nn.Module):\n",
        "    def __init__(self, feature_size, num_classes, device, lstm_hidden_size=256):\n",
        "        super(bilstm_crf, self).__init__()\n",
        "        self.bilstm = torch.nn.LSTM(\n",
        "            input_size=feature_size, \n",
        "            hidden_size=lstm_hidden_size, \n",
        "            num_layers=1, \n",
        "            batch_first=True, \n",
        "            bidirectional=True)\n",
        "        self.linear = torch.nn.Linear(\n",
        "            in_features=lstm_hidden_size*2, \n",
        "            out_features=num_classes)\n",
        "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "        self.weight_init()\n",
        "\n",
        "    def weight_init(self):\n",
        "        # Initialize linear layer\n",
        "        bias = sqrt(6/(self.linear.weight.shape[0]+self.linear.weight.shape[1]))\n",
        "        nn.init.uniform_(self.linear.weight, -bias, bias)\n",
        "        nn.init.constant_(self.linear.bias, 0.0)\n",
        "        # Initialize LSTM layer\n",
        "        for name, params in self.bilstm.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(params, 0.0)\n",
        "                nn.init.constant_(params[self.bilstm.hidden_size:2*self.bilstm.hidden_size], 1.0)\n",
        "            else:\n",
        "                bias = sqrt(6/(params.shape[0]+params.shape[1]))\n",
        "                nn.init.uniform_(params, -bias, bias)\n",
        "        \n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        x = self.dropout(x)\n",
        "        x, (_, _) = self.bilstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.crf(x, y, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, mask):\n",
        "        x, (_, _) = self.bilstm(x)\n",
        "        x = self.linear(x)\n",
        "        pred, prob = self.crf.decode(x, mask=mask)\n",
        "        return pred, prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3od6n-DfSOTi"
      },
      "source": [
        "\n",
        "class cnn_bilstm_crf(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, char_out_channels, pretrained_word_emb, num_classes, device, lstm_hidden_size):\n",
        "        super(cnn_bilstm_crf, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "        self.char_encoder = char_cnn(\n",
        "            embedding_size=char_vocab_size, \n",
        "            embedding_dim=char_embedding_dim, \n",
        "            char_out_channels=char_out_channels)\n",
        "        self.word_embedder= nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_word_emb.vectors))\n",
        "        self.decoder      = bilstm_crf(\n",
        "            feature_size=char_out_channels+pretrained_word_emb.vector_size, \n",
        "            num_classes=num_classes, \n",
        "            device=device, \n",
        "            lstm_hidden_size=lstm_hidden_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, sent, word, tag, mask):\n",
        "        char_emb = self.char_encoder(word)\n",
        "        word_emb = self.word_embedder(sent)\n",
        "        x = torch.cat((word_emb, char_emb), dim=2)\n",
        "        x = self.decoder(x, tag, mask)\n",
        "        return -x\n",
        "\n",
        "    def decode(self, sent, word, mask, return_token_log_probabilities = False):\n",
        "        \"\"\"\n",
        "        return_token_log_probabilities not implemented\n",
        "        \"\"\"\n",
        "        char_emb = self.char_encoder(word)\n",
        "        word_emb = self.word_embedder(sent)\n",
        "        x = torch.cat((word_emb, char_emb), dim=2)\n",
        "        x, prob = self.decoder.decode(x, mask=mask)\n",
        "        x = [torch.LongTensor(aux) for aux in x]\n",
        "        predictions = pad_sequence(x, batch_first = True, padding_value = 0)\n",
        "        return predictions, prob\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}