{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-cnn-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig3nqBkZOQGq",
        "outputId": "f5b36f02-36fb-473c-ef9a-152885be8b9f"
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "P = 'drive/MyDrive/knedle-data/NER_dataset'\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbt3cCQBC5kx",
        "outputId": "bd35d777-0aaa-4ac1-fd2d-36cdb4578909"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJYnQMjEszNX",
        "outputId": "fbd348ba-da61-4efc-f722-98e0d84d754e"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8xS8XLsrfi"
      },
      "source": [
        "# Basic packages\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from gensim.models import KeyedVectors\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "import joblib\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import math\n",
        "from random import sample\n",
        "# NER open packages\n",
        "from seqeval.scheme import IOBES\n",
        "from seqeval.metrics import f1_score\n",
        "# my NER packages\n",
        "from data3 import active_dataset\n",
        "from utils import create_char2idx_dict, create_tag2idx_dict, create_word2idx_dict, new_custom_collate_fn, budget_limit, find_iobes_entities, find_iobes_entities2\n",
        "from metrics import exact_f1_score, preprocess_pred_targ, IOBES_tags\n",
        "from CNN_biLSTM_CRF import cnn_bilstm_crf\n",
        "from CNN_CNN_LSTM2 import CNN_CNN_LSTM\n",
        "\n",
        "import CNN_biLSTM_CRF as CBC\n",
        "import CNN_CNN_LSTM2 as CCL\n",
        "import CNN_CNN_CRF as CCC \n",
        "\n",
        "try:\n",
        "  import utils\n",
        "except:\n",
        "  import utils_colab as utils\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6dLVjfps_PS",
        "outputId": "6dbbb112-84af-4515-cb3e-1aeb7e82f987"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_PATH=Path('drive/MyDrive/knedle_data/NER_dataset')\n",
        "SAVE_PATH=Path('drive/MyDrive/knedle_data/NER_results')\n",
        "\n",
        "args = dict(\n",
        "    save_path=SAVE_PATH,\n",
        "    epochs=50,\n",
        "    dataset='aposentadoria',\n",
        "    model='CNN-CNN-LSTM',\n",
        "    lstm_hidden_size=128,\n",
        "    batch_size=16,\n",
        "    use_dev_set=False\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--save_path', \n",
        "    action='store', dest='save_path', \n",
        "    default=SAVE_PATH, type=str,\n",
        "    # default='experiments/Supervised/', type=str\n",
        "    )\n",
        "parser.add_argument(\n",
        "    '--epochs', \n",
        "    action='store', \n",
        "    dest='epochs', default=50, type=int)\n",
        "parser.add_argument(\n",
        "    '--dataset', \n",
        "    action='store', \n",
        "    dest='dataset', default='aposentadoria', type=str)\n",
        "parser.add_argument(\n",
        "    '--model', \n",
        "    action='store', \n",
        "    dest='model', default = 'CNN-CNN-LSTM', type=str)\n",
        "    # dest='model', default = 'CNN-CNN-CRF', type=str)\n",
        "parser.add_argument(\n",
        "    '--lstm_hidden_size', \n",
        "    action='store', \n",
        "    dest='lstm_hidden_size', default=128, type=int)\n",
        "parser.add_argument(\n",
        "    '--batch_size', \n",
        "    action='store', \n",
        "    dest='batch_size', default=16, type=int)\n",
        "parser.add_argument(\n",
        "    '--use_dev_set', \n",
        "    action = 'store', \n",
        "    dest='use_dev_set', default=False, type=bool)\n",
        "# parser_opt = parser.parse_args()\n",
        "parser_opt, unknown = parser.parse_known_args()\n",
        "print(f'Experiment:')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ0FXPFfs_Mt",
        "outputId": "a2722329-aa4a-4ae8-e28d-abf755788f94"
      },
      "source": [
        "from importlib import reload as rl\n",
        "import utils\n",
        "rl(utils)\n",
        "\n",
        "if parser_opt.dataset not in ['ontonotes', 'conll', 'aposentadoria']:\n",
        "    raise ValueError(\n",
        "        'Dataset not recognized. Options are: conll, ontonotes and aposentadoria'\n",
        "    )\n",
        "\n",
        "emb, train_path, test_path, data_format = utils.load_embedding(parser_opt, DATA_PATH)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special token <START> not found, it's being added now\n",
            "Special token <END> not found, it's being added now\n",
            "Special token <UNK> not found, it's being added now\n",
            "Special token <PAD> not found, it's being added now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HLiyYcw6-I3",
        "outputId": "4c5b5b5c-853d-469b-8e00-eae37ea699e9"
      },
      "source": [
        "rl(utils)\n",
        "\n",
        "collate_object = utils.new_custom_collate_fn(\n",
        "    pad_idx=emb.key_to_index['<PAD>'], \n",
        "    unk_idx=emb.key_to_index['<UNK>']\n",
        "    # pad_idx=emb.vocab['<PAD>'].index, \n",
        "    # unk_idx=emb.vocab['<UNK>'].index\n",
        ")\n",
        "\n",
        "print('\\nGenerating text2idx dictionaries (word, char, tag)')\n",
        "word2idx = utils.create_word2idx_dict(emb, train_path)\n",
        "char2idx = utils.create_char2idx_dict(train_path=train_path)\n",
        "tag2idx  = utils.create_tag2idx_dict(train_path=train_path)\n",
        "\n",
        "print('\\nCreating training dataset')\n",
        "train_set = active_dataset(path=train_path, word2idx_dic=word2idx, char2idx_dic=char2idx, tag2idx_dic=tag2idx, data_format=data_format)\n",
        "# Putting all sentences into the labeled set for training\n",
        "train_set.flag_labeled = False\n",
        "train_set.label_data([i for i in range(len(train_set))])\n",
        "train_set.flag_labeled = True\n",
        "\n",
        "print('\\nCreating test dataset')\n",
        "test_set  = active_dataset(path=test_path, word2idx_dic=word2idx, char2idx_dic=char2idx, tag2idx_dic=tag2idx, data_format=data_format)\n",
        "# Putting all sentences into the labeled set for testing\n",
        "test_set.flag_labeled = False\n",
        "test_set.label_data([i for i in range(len(test_set))])\n",
        "test_set.flag_labeled = True\n",
        "test_dataloader = DataLoader(test_set, batch_size=128, shuffle=False, collate_fn=collate_object)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generating text2idx dictionaries (word, char, tag)\n",
            "\n",
            "Creating training dataset\n",
            "\n",
            "Creating test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry4WE2zn6-Gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8b29b2-6961-4d2d-cb94-bccc82b7d593"
      },
      "source": [
        "\n",
        "model_opt = parser_opt.model\n",
        "\n",
        "rl(CCC)\n",
        "rl(CCL)\n",
        "rl(CBC)\n",
        "\n",
        "if model_opt == 'CNN-CNN-LSTM':\n",
        "    model = CCL.CNN_CNN_LSTM(char_vocab_size=len(char2idx),\n",
        "                                char_embedding_dim=25,\n",
        "                                char_out_channels=50,\n",
        "                                pretrained_word_emb=emb,\n",
        "                                word2idx = word2idx,\n",
        "                                word_out_channels=400,\n",
        "                                word_conv_layers = 1,\n",
        "                                num_classes=len(tag2idx),\n",
        "                                decoder_layers = 1,\n",
        "                                decoder_hidden_size = 128,\n",
        "                                device=device)\n",
        "    lrate = 0.010\n",
        "    new_lrate = 0.010\n",
        "    momentum = 0.9\n",
        "    clipping_value = 5.0\n",
        "    flag_adjust_lrate = False\n",
        "    \n",
        "elif model_opt == 'CNN-biLSTM-CRF':\n",
        "    model = CBC.cnn_bilstm_crf(char_vocab_size=len(char2idx), \n",
        "                   char_embedding_dim=30, \n",
        "                   char_out_channels=30, \n",
        "                   pretrained_word_emb=emb, \n",
        "                   num_classes=len(tag2idx), \n",
        "                   device=device, \n",
        "                   lstm_hidden_size=parser_opt.lstm_hidden_size)\n",
        "    lrate = 0.0025\n",
        "    clipping_value = 5.0\n",
        "    momentum = 0.9\n",
        "    flag_adjust_lrate = False\n",
        "elif model_opt == 'CNN-CNN-CRF':\n",
        "    model = CCC.CNN_CNN_CRF(char_vocab_size=len(char2idx),\n",
        "                                char_embedding_dim=25,\n",
        "                                char_out_channels=50,\n",
        "                                pretrained_word_emb=emb,\n",
        "                                word2idx = word2idx,\n",
        "                                word_out_channels=400,\n",
        "                                word_conv_layers = 1,\n",
        "                                num_classes=len(tag2idx),\n",
        "                                decoder_layers = 1,\n",
        "                                decoder_hidden_size = 128,\n",
        "                                device=device)\n",
        "    lrate = 0.010\n",
        "    new_lrate = 0.010\n",
        "    momentum = 0.9\n",
        "    clipping_value = 5.0\n",
        "    flag_adjust_lrate = False\n",
        "\n",
        "\n",
        "model.to(device)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_CNN_LSTM(\n",
              "  (char_encoder): char_cnn(\n",
              "    (embedding): Embedding(65, 25, padding_idx=0)\n",
              "    (conv): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (relu): ReLU()\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (word_encoder): word_cnn(\n",
              "    (embedding): Embedding(117, 50)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (convnet): Sequential(\n",
              "      (0): Conv1d(100, 400, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): decoder(\n",
              "    (lstm): LSTM(542, 128)\n",
              "    (linear): Linear(in_features=128, out_features=42, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (loss_fn): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVHsnw8F6-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56ac7c3-6dc6-4a57-b0ac-6818e4ab9e56"
      },
      "source": [
        "supervised_epochs = parser_opt.epochs\n",
        "optim = torch.optim.SGD(model.parameters(), lr=lrate, momentum=momentum)\n",
        "\n",
        "\n",
        "# ==============================================================================================\n",
        "# ==============================================================================================\n",
        "# ============================= Supervised learning algorithm ==================================\n",
        "# ==============================================================================================\n",
        "# ==============================================================================================\n",
        "print(f'\\nInitiating supervised training\\n\\n')\n",
        "f1_history = []\n",
        "\n",
        "train_set.flag_labeled = True\n",
        "batch_size = parser_opt.batch_size\n",
        "dataloader = DataLoader(\n",
        "    train_set, \n",
        "    batch_size=batch_size, \n",
        "    pin_memory=True, collate_fn = collate_object, shuffle=False)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initiating supervised training\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9UFyccl6-CD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec597fa7-0087-4616-f958-21c4d4ed71b0"
      },
      "source": [
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):\n",
        "# for epoch in range(supervised_epochs):\n",
        "    print(f'Epoch: {epoch}')\n",
        "    model.train()        \n",
        "    for sent, tag, word, mask in dataloader:\n",
        "        sent = sent.to(device)\n",
        "        tag = tag.to(device)\n",
        "        word = word.to(device)\n",
        "        mask = mask.to(device)\n",
        "        optim.zero_grad()\n",
        "        loss = model(sent, word, tag, mask)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
        "        optim.step()\n",
        "    \n",
        "    # Verify performance on test set after supervised training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions, targets = preprocess_pred_targ(model, test_dataloader, device)\n",
        "        predictions = IOBES_tags(predictions, tag2idx)\n",
        "        targets = IOBES_tags(targets, tag2idx)\n",
        "        micro_f1 = f1_score(targets, predictions, mode='strict', scheme=IOBES)\n",
        "        f1_history.append(0 if np.isnan(micro_f1) else micro_f1)\n",
        "        print(f'micro f1-score: {micro_f1}\\n')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "micro f1-score: 0.11352525801195001\n",
            "\n",
            "Epoch: 1\n",
            "micro f1-score: 0.6651562352496933\n",
            "\n",
            "Epoch: 2\n",
            "micro f1-score: 0.8486312399355878\n",
            "\n",
            "Epoch: 3\n",
            "micro f1-score: 0.8506179913235655\n",
            "\n",
            "Epoch: 4\n",
            "micro f1-score: 0.9588131089459698\n",
            "\n",
            "Epoch: 5\n",
            "micro f1-score: 0.9750000000000001\n",
            "\n",
            "Epoch: 6\n",
            "micro f1-score: 0.9501646213708471\n",
            "\n",
            "Epoch: 7\n",
            "micro f1-score: 0.9700537120153042\n",
            "\n",
            "Epoch: 8\n",
            "micro f1-score: 0.9550612123021798\n",
            "\n",
            "Epoch: 9\n",
            "micro f1-score: 0.9832109891707246\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXBldSks69_v"
      },
      "source": [
        "hyperparams = {'model': str(model), 'LR': lrate, 'momentum': momentum, 'clipping': clipping_value}\n",
        "dic = {'f1_hist': f1_history, 'hyperparams': hyperparams}\n",
        "path = parser_opt.save_path.as_posix()\n",
        "\n",
        "from glob import glob\n",
        "cnt = 1\n",
        "for name in glob(path+'/*.pkl'):\n",
        "    if model_opt in name:\n",
        "        cnt += 1\n",
        "f_name = model_opt + '_' + str(cnt) + '.pkl'\n",
        "\n",
        "joblib.dump(dic, path + f_name)\n",
        "print(f'Training saved in: {path + f_name}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}