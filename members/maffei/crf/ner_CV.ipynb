{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Extraindo trechos anotados dos XMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Adiquirindo a raiz de cada XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path = 'xml_batch1/*.xml' # Caminho até os XMLs\n",
    "roots = []\n",
    "for xml in glob.glob(glob_path):\n",
    "    tree = ET.parse(xml)\n",
    "    roots.append(tree.getroot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse bloco deve demorar de alguns segundos a 1 minuto\n",
    "atos_csv_dict = {}\n",
    "\n",
    "for root in roots: # Intera sobre as raizes\n",
    "    for relation in root.findall(\".//relation\"):                            # Intera sobre as relações.\n",
    "        row_act = {}\n",
    "        type_relation = relation.find('.//infon[@key=\"type\"]').text\n",
    "        for node in relation.findall('node'):                               # Intera sobre os nós(anotações) da relação.\n",
    "            ref_id = node.get('refid')\n",
    "            annotation = root.find(f'.//annotation[@id=\"{ref_id}\"]')        # Encontra anotação.\n",
    "            type_annotation = annotation.find('.//infon[@key=\"type\"]').text # Encontra tipo da anotação.\n",
    "            text_annotation = annotation.find('text').text                  # Encontra texto da anotação.\n",
    "            row_act[type_annotation] = text_annotation\n",
    "        \n",
    "        if type_relation not in atos_csv_dict:                              # Checa se a tabela já existe, caso contrário, cria uma.\n",
    "            atos_csv_dict[type_relation] = pd.DataFrame()\n",
    "            \n",
    "        atos_csv_dict[type_relation] = atos_csv_dict[type_relation].append(row_act, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomeacao_df = atos_csv_dict['Ato_Nomeacao_Comissionado']\n",
    "\n",
    "nomeacao_fields = ['nome', 'cargo_efetivo', 'matricula', 'matricula_SIAPE', 'simbolo', 'cargo_comissionado', 'hierarquia_lotacao', 'orgao', 'Ato_Nomeacao_Comissionado']\n",
    "\n",
    "# Nessa linha todas as colunas que não pertencem a nomecao são extraidas para uma segunda lista. (Compreensão de listas).\n",
    "nomeacao_non_fields = [column for column in nomeacao_df.columns if column not in nomeacao_fields]\n",
    "\n",
    "# Exclusão de todas as linhas que possuam algum valor nos campos que não pertecem a nomeacao.\n",
    "nomeacao_df = nomeacao_df[nomeacao_df[nomeacao_non_fields].isna().any(axis=1)]\n",
    "\n",
    "# Exclusão de todas as colunas que não pertecem a nomeação.\n",
    "nomeacao_df = nomeacao_df.drop(columns=nomeacao_non_fields)\n",
    "\n",
    "# Exclusão das linhas que não possuem anotação de atos.\n",
    "nomeacao_df = nomeacao_df.dropna(subset=['Ato_Nomeacao_Comissionado'])\n",
    "\n",
    "atos_csv_dict['Ato_Nomeacao_Comissionado'] = nomeacao_df\n",
    "atos_csv_dict['Ato_Nomeacao_Comissionado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in atos_csv_dict.keys():\n",
    "    atos_csv_dict[key] = atos_csv_dict[key].dropna(subset=[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "def tokenize(sentence):\n",
    "    try:\n",
    "        new_words = _tokenizer.tokenize(sentence)\n",
    "        return new_words    \n",
    "    except:\n",
    "        print(\"SENTENCE:\", sentence)\n",
    "        input()\n",
    "    return ''\n",
    "\n",
    "\n",
    "def find_entity(row, token):\n",
    "    \"\"\" ...\n",
    "    \n",
    "    Assumes `row` has the whole text on the first collumn\n",
    "    and the remaining contain entities.\n",
    "    \"\"\"\n",
    "    for column in row.keys()[1:]:\n",
    "        if (row[column] is not np.nan \n",
    "            and token == tokenize(row[column])[0]):\n",
    "            return column\n",
    "    return None\n",
    "\n",
    "# Atualizar no futuro para qualquer ato.\n",
    "# Aparentemente esse algoritmo está O(n*m) onde n é a quantidade de tokens e m a quantidade de colunas do df.\n",
    "def generate_IOB_labels(row):\n",
    "    \"\"\"Generates IOB-labeling for whole text and entities.\n",
    "\n",
    "    Assumes `row` has the whole text on the first collumn\n",
    "    and the remaining contain entities.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    entity_started = False\n",
    "    text = row.iloc[0]\n",
    "    for token in tokenize(text):                         # Intera sobre cada token da anotação do ato.\n",
    "        if not entity_started:                               # Caso uma entidade ainda n tenha sido identificada nos tokens.\n",
    "            entity = find_entity(row, token)                 # Busca o token atual no primeiro token de todos os campos do df.\n",
    "            if entity is not None:                           # Se foi encontrado o token no inicio de alguma entidade ele inicia a comparação token a token com a entidade.\n",
    "                entity_started = True\n",
    "                token_index = 1\n",
    "                labels.append('B-' + entity)\n",
    "            else:\n",
    "                labels.append('O')\n",
    "        else:                                                # Caso uma entidade já tenha sido identificada\n",
    "            if token_index < len(tokenize(row[entity])) and token == tokenize(row[entity])[token_index]: # Checa se o próximo token pertence à entidade e se o tamanho da entidade chegou ao fim.\n",
    "                labels.append('I-' + entity)                 # Se a entidade ainda possui tokens e a comparação foi bem sucedida adicione o label I.\n",
    "                token_index += 1\n",
    "                if token_index >= len(tokenize(row[entity])):\n",
    "                    entity_started = False\n",
    "            else:                                            # Se o token n for igual ou a entidade chegou ao fim.\n",
    "                entity_started = False\n",
    "                labels.append('O')\n",
    "                \n",
    "    return labels\n",
    "\n",
    "\n",
    "def find_entity(row, token, ignore_idx=0):\n",
    "    \"\"\"Searches for named entities on columns, except by ignore_idx-columns.\n",
    "    \n",
    "    ignore_idx: int indicating which column has\n",
    "                the TEXT where the named were extracted from\n",
    "    \"\"\"\n",
    "    for idx, column in enumerate(row.keys()):\n",
    "        if idx == ignore_idx:\n",
    "            continue\n",
    "        if row[column] is not np.nan and token == tokenize(row[column])[0]:\n",
    "            return column\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Atualizar no futuro para qualquer ato.\n",
    "# Complexidade: O(n*m)\n",
    "# n: quantidade de tokens\n",
    "# m: quantidade de colunas do df.\n",
    "def generate_IOB_labels(row, idx=0):\n",
    "    \"\"\"Generate IOB-labels for idx-column.\"\"\"\n",
    "    labels = []\n",
    "    entity_started = False\n",
    "    text = row.iloc[idx]\n",
    "    for token in tokenize(text):                         # Intera sobre cada token da anotação do ato.\n",
    "        if not entity_started:                               # Caso uma entidade ainda n tenha sido identificada nos tokens.\n",
    "            entity = find_entity(row, token)                 # Busca o token atual no primeiro token de todos os campos do df.\n",
    "            if entity is not None:                           # Se foi encontrado o token no inicio de alguma entidade ele inicia a comparação token a token com a entidade.\n",
    "                entity_started = True\n",
    "                token_index = 1\n",
    "                labels.append('B-' + entity)\n",
    "            else:\n",
    "                labels.append('O')\n",
    "        else:                                                # Caso uma entidade já tenha sido identificada\n",
    "            if token_index < len(tokenize(row[entity])) and token == tokenize(row[entity])[token_index]: # Checa se o próximo token pertence à entidade e se o tamanho da entidade chegou ao fim.\n",
    "                labels.append('I-' + entity)                 # Se a entidade ainda possui tokens e a comparação foi bem sucedida adicione o label I.\n",
    "                token_index += 1\n",
    "                if token_index >= len(tokenize(row[entity])):\n",
    "                    entity_started = False\n",
    "            else:                                            # Se o token n for igual ou a entidade chegou ao fim.\n",
    "                entity_started = False\n",
    "                labels.append('O')\n",
    "                \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Agora, com as funções de geração label prontas iremos criar uma lista de strings representando os labels para adicionar ao df de cada ato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Criação das features e treinamento do CRF à moda José"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "def extract_features(sentence):\n",
    "    sentence_features = []\n",
    "    for j, sent in enumerate(sentence):\n",
    "        word_feat = {\n",
    "                'word': sent.lower(),\n",
    "                'capital_letter': sent[0].isupper(),\n",
    "                'all_capital': sent.isupper(),\n",
    "                'isdigit': sent.isdigit(),\n",
    "                'word_before': sent.lower() if j==0 else sentence[j-1].lower(),\n",
    "                'word_after:': sent.lower() if j+1>=len(sentence) else sentence[j+1].lower(),\n",
    "                'BOS': j==0,\n",
    "                'EOS': j==len(sentence)-1\n",
    "        }\n",
    "        sentence_features.append(word_feat)\n",
    "    return sentence_features\n",
    "\n",
    "\n",
    "def extract_rows_features(arq, idx=0):\n",
    "    \"\"\"\n",
    "    Tokenizes then extract features from idx-column.\n",
    "    \"\"\"\n",
    "    return arq.iloc[:, idx].map(tokenize).map(extract_features)\n",
    "\n",
    "\n",
    "\n",
    "class IOB_Transformer:\n",
    "    def __init__(self, idx=''):\n",
    "#         self.idx = idx\n",
    "        pass\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, df):\n",
    "        labels_row = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                labels_row.append(' '.join(generate_IOB_labels(row)))\n",
    "            except Exception as e:\n",
    "                print(row)\n",
    "                raise e\n",
    "\n",
    "        return pd.Series(labels_row).str.split()\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureTransformer:\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, df):        \n",
    "        return extract_rows_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atos_csv_dict['Ato_Cessao'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=atos_csv_dict['Ato_Cessao'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('cessao_exemplo.csv', 'w').write(ss.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss.values), len(ss.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[ss.values], columns=ss.index\n",
    ")\n",
    "df.to_csv('cessao_exemplo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for v in df.iloc[0]:\n",
    "    print(json.dumps(v), end=',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('cessao_exemplo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in atos_csv_dict.items():\n",
    "    if 'labels_IOB' in v.columns:\n",
    "        v.pop('labels_IOB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atos_csv_dict['Ato_Cessao'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# mlflow.set_registry_uri(\"\")\n",
    "mlflow.set_tracking_uri(\"localhost:5000\")\n",
    "training_ratio = 0.7\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "run = mlflow.start_run()\n",
    "print(\"RUN:\", run.info.run_id)\n",
    "\n",
    "params = dict(\n",
    "    algorithm = 'l2sgd', \n",
    "    c2=1,\n",
    "    max_iterations=10, \n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "\n",
    "model = sklearn_crfsuite.CRF(\n",
    "    **params,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('featurizer', FeatureTransformer()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "for key in atos_csv_dict.keys():\n",
    "    sz = len(atos_csv_dict[key])\n",
    "    limiar = math.floor(training_ratio*sz)\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print(\"Ato:\" + key)\n",
    "    print(\"Tamanho: \" + str(sz))\n",
    "    df = atos_csv_dict[key].copy()\n",
    "    \n",
    "    if sz < 10:\n",
    "        df_train, df_test = df.iloc[:limiar, :], df.iloc[limiar:, :]\n",
    "        pipe.fit(\n",
    "            df_train,\n",
    "            IOB_Transformer().transform(df_train),\n",
    "        );\n",
    "        y_pred = pipe.predict(df_test)\n",
    "        test_y = IOB_Transformer().transform(df_test)\n",
    "\n",
    "        labels = list(pipe.classes_)\n",
    "        labels.remove('O')\n",
    "\n",
    "        f1 = metrics.flat_f1_score(test_y, y_pred, \n",
    "                              average='weighted', labels=labels)\n",
    "    \n",
    "\n",
    "        print(f1)\n",
    "        print(metrics.flat_classification_report(\n",
    "            test_y, y_pred, labels=labels, digits=3\n",
    "        ))\n",
    "        model = pipe\n",
    "\n",
    "    else:\n",
    "        X, y = df, IOB_Transformer().transform(df)\n",
    "        res = cross_validate(\n",
    "            pipe, X, y,\n",
    "            cv = 3, return_estimator=True\n",
    "        )\n",
    "        model = res['estimator'][np.argmax(res['test_score'])]\n",
    "        print(res)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=f\"model-{key}\",\n",
    "        registered_model_name=key,\n",
    "    )\n",
    "    mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
