{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpwowf_F0Wk7"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn_crfsuite\n",
        "!pip install -U 'scikit-learn<0.24'\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "e02HRRPzDiX-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6fy0XFU-zo26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d78f88-aa1e-4f88-ce3f-4ad0f9676f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import sklearn_crfsuite\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import sklearn\n",
        "import joblib\n",
        "import nltk\n",
        "\n",
        "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score\n",
        "from sklearn.metrics import classification_report, make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn_crfsuite import scorers\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ1zU9lkEnd3"
      },
      "source": [
        "***Data From https://github.com/UnB-KnEDLe/experiments/tree/master/members/ian/atos_licitacao_contratos/Corpus3***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Aditamento***"
      ],
      "metadata": {
        "id": "y_OpvBxaQuJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_aditamento = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/hyperparameter_test/main/aditamento.csv')"
      ],
      "metadata": {
        "id": "TJfJ-GRrPe7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Anulação***"
      ],
      "metadata": {
        "id": "p30ZBGsMQwlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_anulacao = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/data_crf_training/main/anulacao_revogacao.csv')"
      ],
      "metadata": {
        "id": "ejMPQioRQEjf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Contrato***"
      ],
      "metadata": {
        "id": "fZhmulaNQ2Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_contrato = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/hyperparameter_test/main/contrato.csv')"
      ],
      "metadata": {
        "id": "XV19fAnBQFdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Licitação***"
      ],
      "metadata": {
        "id": "I1p7U9H3Q4NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_licitacao = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/hyperparameter_test/main/licitacao.csv')"
      ],
      "metadata": {
        "id": "JN5259O2QFov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Suspensao***"
      ],
      "metadata": {
        "id": "ye-GxvtCQ688"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_suspensao = pd.read_csv('https://raw.githubusercontent.com/brunoedcf/hyperparameter_test/main/suspensao.csv')"
      ],
      "metadata": {
        "id": "8HDUDbInQGRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Classe CRF***\n",
        "\n"
      ],
      "metadata": {
        "id": "UtmMOrHvA5W_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qljULwbBzZgQ"
      },
      "outputs": [],
      "source": [
        "class CRF_Flow():\n",
        "\n",
        "  def __init__(self, tipo):\n",
        "\n",
        "    self.tipo = tipo\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "    self.x_train = None\n",
        "    self.y_train = None\n",
        "    self.x_test = None\n",
        "    self.y_test = None\n",
        "    self.x_validation = None\n",
        "    self.y_validation = None\n",
        "    self.labels = None\n",
        "    self.metrics = None\n",
        "\n",
        "\n",
        "    self.crf = sklearn_crfsuite.CRF(\n",
        "      algorithm = 'lbfgs',\n",
        "      c1=0.17,\n",
        "      c2=0.17,\n",
        "      max_iterations=70,\n",
        "      all_possible_transitions=True\n",
        "    )\n",
        "\n",
        "  def get_features(self, sentence):\n",
        "        \n",
        "        sent_features = []\n",
        "        for i in range(len(sentence)):\n",
        "            # print(sentence[i])\n",
        "            word_feat = {\n",
        "                # Palavra atual\n",
        "                'word': sentence[i].lower(),\n",
        "                'capital_letter': sentence[i][0].isupper(),\n",
        "                'all_capital': sentence[i].isupper(),\n",
        "                'isdigit': sentence[i].isdigit(),\n",
        "                # Uma palavra antes\n",
        "                'word_before': '' if i == 0 else sentence[i-1].lower(),\n",
        "                'word_before_isdigit': '' if i == 0 else sentence[i-1].isdigit(),\n",
        "                'word_before_isupper': '' if i == 0 else sentence[i-1].isupper(),\n",
        "                'word_before_istitle': '' if i == 0 else sentence[i-1].istitle(),\n",
        "                # Uma palavra depois\n",
        "                'word_after': '' if i+1 >= len(sentence) else sentence[i+1].lower(),\n",
        "                'word_after_isdigit': '' if i+1 >= len(sentence) else sentence[i+1].isdigit(),\n",
        "                'word_after_isupper': '' if i+1 >= len(sentence) else sentence[i+1].isupper(),\n",
        "                'word_after_istitle': '' if i+1 >= len(sentence) else sentence[i+1].istitle(),\n",
        "\n",
        "                'BOS': i == 0,\n",
        "                'EOS': i == len(sentence)-1\n",
        "            }\n",
        "            sent_features.append(word_feat)\n",
        "        return sent_features\n",
        "\n",
        "  def load(self, data_frame):\n",
        "    if self.tipo != 'anulacao' and self.tipo != 'suspensao':\n",
        "\n",
        "      self.x = []\n",
        "      self.y = []\n",
        "\n",
        "      for i, row in enumerate(data_frame['treated_text']):\n",
        "        self.x.append(word_tokenize(data_frame['treated_text'][i]))\n",
        "        self.y.append(data_frame['IOB'][i].split())\n",
        "\n",
        "      for i in range(len(self.x)):\n",
        "        self.x[i] = self.get_features(self.x[i])\n",
        "\n",
        "      self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=0.4, random_state=42)\n",
        "      self.x_test, self.x_validation, self.y_test, self.y_validation = train_test_split(self.x_test, self.y_test, test_size=0.3, random_state=42)\n",
        "\n",
        "    elif self.tipo == 'anulacao':\n",
        "\n",
        "      self.x = []\n",
        "      self.y = []\n",
        "      \n",
        "      for i, row in enumerate(data_frame['treated_text']):\n",
        "        self.x.append(word_tokenize(data_frame['treated_text'][i]))\n",
        "        self.y.append(data_frame['IOB'][i].split())\n",
        "\n",
        "      for i in range(len(self.x)):\n",
        "        self.x[i] = self.get_features(self.x[i])\n",
        "\n",
        "      DATA_ESCRITO = []\n",
        "\n",
        "      for i in range(len(data_frame)):\n",
        "        if str(data_frame['DATA_ESCRITO'][i]) != \"nan\":\n",
        "          DATA_ESCRITO.append(i)\n",
        "      \n",
        "      DATA_ESCRITO_1 = DATA_ESCRITO[:(len(DATA_ESCRITO)*70)//100]\n",
        "      DATA_ESCRITO_2 = DATA_ESCRITO[(len(DATA_ESCRITO)*70)//100:]\n",
        "\n",
        "      lic = []\n",
        "      lic.extend(DATA_ESCRITO_1)\n",
        "      lic = sorted(list(set(lic)))\n",
        "\n",
        "      not_lic = []\n",
        "      not_lic.extend(DATA_ESCRITO_2)\n",
        "      not_lic = sorted(list(set(not_lic)))\n",
        "\n",
        "      for i in lic:\n",
        "        for j in not_lic:\n",
        "          if i == j:\n",
        "            not_lic.pop(not_lic.index(j))\n",
        "\n",
        "      df_lic = data_frame.iloc[lic]\n",
        "      df_not_lic = data_frame.iloc[not_lic]\n",
        "\n",
        "      df_real = data_frame.drop(df_lic.index)\n",
        "      df_real = df_real.drop(df_not_lic.index)\n",
        "\n",
        "      train = pd.concat([df_lic, df_real[:int(len(data_frame)*0.7) - len(df_lic)]])\n",
        "      test = pd.concat([df_real[int(len(data_frame)*0.7) - len(df_lic):], df_not_lic])\n",
        "\n",
        "      self.x_train = []\n",
        "      self.y_train = []\n",
        "      self.x_test = []\n",
        "      self.y_test = []\n",
        "\n",
        "      tt_test = test['treated_text']\n",
        "      iob_test = test['IOB']\n",
        "      \n",
        "      for i, row in enumerate(train['treated_text']):\n",
        "        self.x_train.append(word_tokenize(row))\n",
        "\n",
        "      for i in range(len(self.x_train)):\n",
        "        self.x_train[i] = self.get_features(self.x_train[i])\n",
        "\n",
        "      for i, row in enumerate(train['IOB']):\n",
        "        self.y_train.append(row.split())\n",
        "\n",
        "      for i, row in enumerate(test['treated_text']):\n",
        "        self.x_test.append(word_tokenize(row))\n",
        "\n",
        "      for i in range(len(self.x_test)):\n",
        "        self.x_test[i] = self.get_features(self.x_test[i])\n",
        "\n",
        "      for i, row in enumerate(test['IOB']):\n",
        "        self.y_test.append(row.split())\n",
        "\n",
        "    elif self.tipo == 'suspensao':\n",
        "\n",
        "      self.x = []\n",
        "      self.y = []\n",
        "\n",
        "      DECISAO_TCDF = []\n",
        "\n",
        "      for i, row in enumerate(data_frame['treated_text']):\n",
        "        self.x.append(word_tokenize(data_frame['treated_text'][i]))\n",
        "        self.y.append(data_frame['IOB'][i].split())\n",
        "\n",
        "      for i in range(len(self.x)):\n",
        "        self.x[i] = self.get_features(self.x[i])\n",
        "\n",
        "      for i in range(len(data_frame)):\n",
        "        if str(data_frame['DECISAO_TCDF'][i]) != \"nan\":\n",
        "          DECISAO_TCDF.append(i)\n",
        "      \n",
        "      DECISAO_TCDF_1 = DECISAO_TCDF[:(len(DECISAO_TCDF)*70)//100]\n",
        "      DECISAO_TCDF_2 = DECISAO_TCDF[(len(DECISAO_TCDF)*70)//100:]\n",
        "\n",
        "      lic = []\n",
        "      lic.extend(DECISAO_TCDF_1)\n",
        "      lic = sorted(list(set(lic)))\n",
        "\n",
        "      not_lic = []\n",
        "      not_lic.extend(DECISAO_TCDF_2)\n",
        "      not_lic = sorted(list(set(not_lic)))\n",
        "\n",
        "      for i in lic:\n",
        "        for j in not_lic:\n",
        "          if i == j:\n",
        "            not_lic.pop(not_lic.index(j))\n",
        "\n",
        "      df_lic = data_frame.iloc[lic]\n",
        "      df_not_lic = data_frame.iloc[not_lic]\n",
        "\n",
        "      df_real = data_frame.drop(df_lic.index)\n",
        "      df_real = df_real.drop(df_not_lic.index)\n",
        "\n",
        "      train = pd.concat([df_lic, df_real[:int(len(data_frame)*0.7) - len(df_lic)]])\n",
        "      test = pd.concat([df_real[int(len(data_frame)*0.7) - len(df_lic):], df_not_lic])\n",
        "\n",
        "      self.x_train = []\n",
        "      self.y_train = []\n",
        "      self.x_test = []\n",
        "      self.y_test = []\n",
        "\n",
        "      tt_test = test['treated_text']\n",
        "      iob_test = test['IOB']\n",
        "      \n",
        "      for i, row in enumerate(train['treated_text']):\n",
        "        self.x_train.append(word_tokenize(row))\n",
        "\n",
        "      for i in range(len(self.x_train)):\n",
        "        self.x_train[i] = self.get_features(self.x_train[i])\n",
        "\n",
        "      for i, row in enumerate(train['IOB']):\n",
        "        self.y_train.append(row.split())\n",
        "\n",
        "      for i, row in enumerate(test['treated_text']):\n",
        "        self.x_test.append(word_tokenize(row))\n",
        "\n",
        "      for i in range(len(self.x_test)):\n",
        "        self.x_test[i] = self.get_features(self.x_test[i])\n",
        "\n",
        "      for i, row in enumerate(test['IOB']):\n",
        "        self.y_test.append(row.split())\n",
        "\n",
        "    else:\n",
        "      print(\"Tipo Invalido\")\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    if self.tipo != 'anulacao' and self.tipo != 'suspensao':\n",
        "      self.crf.fit(self.x_train, self.y_train, X_dev=self.x_validation, y_dev=self.y_validation)\n",
        "    else:\n",
        "      self.crf.fit(self.x_train, self.y_train)\n",
        "\n",
        "    classes = list(self.crf.classes_)\n",
        "    classes.remove('O')\n",
        "    self.labels = classes\n",
        "\n",
        "  def optimize(self):\n",
        "\n",
        "    crf_optimizer = sklearn_crfsuite.CRF(\n",
        "      algorithm='lbfgs',\n",
        "      max_iterations=70,\n",
        "      all_possible_transitions=True\n",
        "    )\n",
        "\n",
        "    params_space = {\n",
        "      'c1': [0.231363],\n",
        "      'c2': [0.001591],\n",
        "      # 'c1': scipy.stats.expon(scale=0.1),\n",
        "      # 'c2': scipy.stats.expon(scale=0.1),\n",
        "    }\n",
        "    f1_scorer = make_scorer(\n",
        "        flat_f1_score,\n",
        "        average='weighted', \n",
        "        labels=self.labels\n",
        "    )\n",
        "\n",
        "    rs = RandomizedSearchCV(\n",
        "        crf_optimizer,\n",
        "        params_space,\n",
        "        cv=10,\n",
        "        verbose=1,\n",
        "        n_jobs=-1,\n",
        "        n_iter=50,\n",
        "        scoring=f1_scorer\n",
        "    )\n",
        "\n",
        "    rs.fit(self.x_train, self.y_train)\n",
        "    \n",
        "    print('best params:', rs.best_params_)\n",
        "    print('best CV score:', rs.best_score_)\n",
        "\n",
        "    self.crf = rs.best_estimator_\n",
        "    joblib.dump(self.crf, f\"{self.tipo}.pkl\")\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "\n",
        "    y_pred = self.crf.predict(self.x_test)\n",
        "\n",
        "    self.metrics = flat_classification_report(self.y_test, y_pred, labels=self.labels, digits=3)\n",
        "    print(self.metrics)\n",
        "\n",
        "\n",
        "  def save(self):\n",
        "    with open('metrics.txt', 'w') as file:\n",
        "      file.write(self.metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Métricas***"
      ],
      "metadata": {
        "id": "uSRsN0cbQ_OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_anulacao = CRF_Flow('anulacao')\n",
        "model_anulacao.load(df_anulacao)\n",
        "model_anulacao.train()\n",
        "# model_anulacao.validate()\n",
        "model_anulacao.optimize()\n",
        "model_anulacao.validate()\n",
        "# model_anulacao.save()"
      ],
      "metadata": {
        "id": "A1_w-8nhVd-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb1fc3d-62b1-42b7-837f-f162a212b8e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    8.8s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params: {'c2': 0.001591, 'c1': 0.231363}\n",
            "best CV score: 0.5863195742544856\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "    B-MODALIDADE_LICITACAO      0.667     0.769     0.714        13\n",
            "           B-NUM_LICITACAO      1.000     0.444     0.615        18\n",
            "           I-NUM_LICITACAO      0.000     0.000     0.000         0\n",
            "B-IDENTIFICACAO_OCORRENCIA      0.929     0.684     0.788        19\n",
            "                B-PROCESSO      0.750     0.545     0.632        11\n",
            "            B-DATA_ESCRITO      1.000     0.800     0.889         5\n",
            "            I-DATA_ESCRITO      1.000     1.000     1.000        16\n",
            "        B-NOME_RESPONSAVEL      1.000     1.000     1.000        13\n",
            "        I-NOME_RESPONSAVEL      0.925     1.000     0.961        37\n",
            "    I-MODALIDADE_LICITACAO      0.643     0.818     0.720        11\n",
            "         B-ORGAO_LICITANTE      1.000     0.429     0.600        14\n",
            "         I-ORGAO_LICITANTE      0.654     0.425     0.515        40\n",
            "                I-PROCESSO      0.000     0.000     0.000         0\n",
            "\n",
            "                 micro avg      0.751     0.706     0.728       197\n",
            "                 macro avg      0.736     0.609     0.649       197\n",
            "              weighted avg      0.853     0.706     0.752       197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_aditamento = CRF_Flow('aditamento')\n",
        "# model_aditamento.load(df_aditamento)\n",
        "# model_aditamento.train()\n",
        "# model_aditamento.validate()\n",
        "# model_aditamento.optimize()\n",
        "# model_aditamento.validate()\n",
        "# model_aditamento.save()"
      ],
      "metadata": {
        "id": "iiZS-syqSwUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_contrato = CRF_Flow('contrato')\n",
        "# model_contrato.load(df_contrato)\n",
        "# model_contrato.train()\n",
        "# model_contrato.validate()\n",
        "# model_contrato.optimize()\n",
        "# model_contrato.validate()\n",
        "# model_contrato.save()"
      ],
      "metadata": {
        "id": "QR-qt-LuVeer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_licitacao = CRF_Flow('licitacao')\n",
        "# model_licitacao.load(df_licitacao)\n",
        "# model_licitacao.train()\n",
        "# model_licitacao.validate()\n",
        "# model_licitacao.optimize()\n",
        "# model_licitacao.validate()\n",
        "# model_licitacao.save()"
      ],
      "metadata": {
        "id": "H1nHORM-VeoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_suspensao = CRF_Flow('suspensao')\n",
        "# model_suspensao.load(df_suspensao)\n",
        "# model_suspensao.train()\n",
        "# model_suspensao.validate()\n",
        "# model_suspensao.optimize()\n",
        "# model_suspensao.validate()\n",
        "# model_suspensao.save()"
      ],
      "metadata": {
        "id": "spxFQDiOVexj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}