{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_block(text):\n",
    "    # Transformar texto em linhas de folha de documento em texto corrido em parágrafo\n",
    "\n",
    "    a = \"\\n\".join([l for l in text.split(\"\\n\") if l != \"\"])\n",
    "    words = a.replace(\"\\n\", \" \").split(\" \")\n",
    "    words = [w for w in words if w != \"\"]\n",
    "\n",
    "    m_words = []\n",
    "    dash_cut = False\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if (word[-1] == \"-\") and (i+1)<len(words):\n",
    "            word = word[:-1] + words[i+1]\n",
    "            i += 1\n",
    "\n",
    "        m_words.append(word)\n",
    "\n",
    "    return \" \".join(m_words)\n",
    "\n",
    "def clean_text(text, filter_patterns, rep=\"\"):\n",
    "    filter_patterns = \"|\".join(filter_patterns)\n",
    "\n",
    "    text = pd.Series(text).str.replace(filter_patterns, rep, regex=True)[0]\n",
    "\n",
    "    return join_block(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_blocks(df, patterns_clean):\n",
    "    # Separar texto em blocos e limpá-los\n",
    "    \n",
    "    pos_texts = []\n",
    "\n",
    "    # Identificação de blocos é feita com as tags xxbob e xxeob\n",
    "    for number, text in df[[\"number\", \"text\"]].itertuples(index=False):\n",
    "        begin = list(re.finditer(\"xxbob\", text))\n",
    "        end = list(re.finditer(\"xxeob\", text))\n",
    "        pos_texts.append([number, text, begin, end])\n",
    "        \n",
    "    full_blocks = []\n",
    "\n",
    "    for p in pos_texts:\n",
    "        blocks = []\n",
    "\n",
    "        number = p[0]\n",
    "        text = p[1]\n",
    "        begin = p[2]\n",
    "        end = p[3]\n",
    "\n",
    "        if (number % 50) == 0:\n",
    "            print(number)\n",
    "\n",
    "        for i in range(len(begin)):\n",
    "            block = text[begin[i].start():end[i].end()]\n",
    "\n",
    "            for i in range(len(patterns_clean[\"patterns\"])):\n",
    "                block = clean_text(text=block, filter_patterns=patterns_clean[\"patterns\"][i], rep=patterns_clean[\"rep\"][i])\n",
    "\n",
    "            blocks.append(block)\n",
    "\n",
    "        full_blocks.append([number, blocks])\n",
    "    \n",
    "    return full_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching(full_blocks, editais):\n",
    "    # Busca de matchings entre SIS Editais e os blocos encontrados e filtrados\n",
    "    \n",
    "    encontrados = []\n",
    "\n",
    "    for f in full_blocks:\n",
    "        number = f[0]\n",
    "        blocks = f[1]\n",
    "\n",
    "        blocks_mod = pd.Series(blocks).str.lower()\n",
    "        \n",
    "        ementas_obj = editais[editais[\"n_dodf\"] == number][\"Ementa Objeto\"].str.lower()\n",
    "        processos_obj = editais[editais[\"n_dodf\"] == number][\"ProcessoGDF\"].astype(str)\n",
    "        editais_obj = editais[editais[\"n_dodf\"] == number][\"Edital\"].astype(str)\n",
    "        \n",
    "        for ementa, processo, edital in zip(ementas_obj, processos_obj, editais_obj):\n",
    "            main_rule = re.escape(ementa)\n",
    "            sec_rule = re.escape(processo)\n",
    "            third_rule = re.escape(edital)\n",
    "            \n",
    "            # A prioridade é dada a Ementa Objeto, depois ProcessoGDF e, por fim, Edital\n",
    "            if blocks_mod.str.contains(main_rule).sum() == 1:\n",
    "                encontrados.append([number, \"ementa\", ementa, blocks_mod[blocks_mod.str.contains(main_rule)].iloc[0]])\n",
    "            elif blocks_mod.str.contains(sec_rule).sum() == 1:\n",
    "                encontrados.append([number, \"processo\", processo, blocks_mod[blocks_mod.str.contains(sec_rule)].iloc[0]])\n",
    "            elif blocks_mod.str.contains(third_rule).sum() == 1:\n",
    "                encontrados.append([number, \"edital\", edital, blocks_mod[blocks_mod.str.contains(third_rule)].iloc[0]])\n",
    "                \n",
    "    return encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data):\n",
    "    # Concatenar os dados do DODF, juntando todas as páginas em uma linha\n",
    "    \n",
    "    return data.sort_values(by=[\"file_name\", \"page\"]).groupby([\"file_name\", \"number\", \"day\", \"month\", \"year\"], as_index=False).agg({'text': '\\n'.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(row, editais):\n",
    "    # Pegar index correspondente ao ato nos SIS Editais\n",
    "\n",
    "    if row[\"selection_type\"] == \"ementa\":\n",
    "        return editais[(row[\"year\"] == editais[\"year\"]) & (row[\"n_dodf\"] == editais[\"n_dodf\"]) & (row[\"selection_rule\"] == editais[\"Ementa Objeto\"].str.lower())].index.values\n",
    "    elif row[\"selection_type\"] == \"processo\":\n",
    "        return editais[(row[\"year\"] == editais[\"year\"]) & (row[\"n_dodf\"] == editais[\"n_dodf\"]) & (row[\"selection_rule\"] == editais[\"ProcessoGDF\"])].index.values\n",
    "    elif row[\"selection_type\"] == \"edital\":\n",
    "        return editais[(row[\"year\"] == editais[\"year\"]) & (row[\"n_dodf\"] == editais[\"n_dodf\"]) & (row[\"selection_rule\"] == editais[\"Edital\"])].index.values\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regras de limpeza, separadas por tipo de substituição a fazer\n",
    "patterns_clean = json.loads(r'{\"patterns\": [[\"\\\\nP\\u00c1GINA\\\\s([0-9]{1,5})\", \"\\\\nDI\\u00c1RIO\\\\sOFICIAL\\\\sDO\\\\sDISTRITO\\\\sFEDERAL\", \"\\\\nN\\u00ba(.+?)2([0-9]{3})\", \"\\\\nxx([a-z]{0,10}) Di\\u00e1rio Oficial do Distrito Federal xx([a-z]{0,10})\", \"\\\\nDi\\u00e1rio Oficial do Distrito Federal\", \"Documento assinado digitalmente conforme MP n\\u00ba 2.200-2 de 24/08/2001, que institui a\", \"Infraestrutura de Chaves P\\u00fablicas Brasileira ICP-Brasil\", \"Este documento pode ser verificado no endere\\u00e7o eletr\\u00f4nico\", \"http://wwwin.gov.br/autenticidade.html\", \"pelo c\\u00f3digo ([0-9]{15,18})\", \"\\\\nDocumento assinado digitalmente, original em https://www.dodf.df.gov.br\", \"http:/Awwwin.gov.br/autenticidade.html\", \"Documento assinado digitalmente conforme MP n\\u00ba 2.200-2 de 24/08/2001,\", \"\\\\nque institui a\\\\n\", \"\\\\nhttp://www.in.gov.br/autenticidade.html\", \"\\\\nhttp://www.in.gov.brautenticidade html\", \"Documento assinado digitalmente conforme MP n 2.200-2 de 24/08/2001, que institui a .\", \"http://www.in.gov.brautenticidade html,\", \"xx([a-z]{1,10}) \", \" xx([a-z]{1,10})\", \"xx([a-z]{1,10})\"], [\"\\\\n-\\\\n\", \"\\\\n- -\\\\n\", \"\\\\n- - -\\\\n\", \"\\\\n[\\\\.\\\\,\\\\-\\\\\\u2014]\\\\n\", \"\\u2014 -\", \". -\", \"\\\\r[\\\\.\\\\,\\\\-\\\\\\u2014]\\\\r\", \"\\\\n-\\\\r\", \"\\\\r\"]], \"rep\": [\" \", \"\\n\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis_editais = pd.read_csv(\"siseditais_alterado.csv\", encoding = 'latin1', sep=\";\")\n",
    "sis_editais[\"Modalidade\"] = sis_editais[\"Modalidade\"].str.replace(\"PREG?O ELETR?NICO\", \"PREGÃO ELETRÔNICO\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis_editais_mod = sis_editais.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegar data dos DODFs de cada linha\n",
    "sis_editais_mod[\"day\"] = sis_editais_mod[\"Publicação\"].str.split(\"/\").str[0]\n",
    "sis_editais_mod[\"month\"] = sis_editais_mod[\"Publicação\"].str.split(\"/\").str[1]\n",
    "sis_editais_mod[\"year\"] = sis_editais_mod[\"Publicação\"].str.split(\"/\").str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas mal-formatadas\n",
    "sis_editais_mod = sis_editais_mod[(sis_editais_mod[\"Dodf\"].str.strip() != \"\") & (~sis_editais_mod[\"Dodf\"].isnull())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegar número do DODF por linha\n",
    "sis_editais_mod[\"n_dodf\"] = sis_editais_mod[\"Dodf\"].str.split(\"/\").str[0].str.strip().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só há dados consistentes e confiáveis entre 2011 e 2021\n",
    "sis_editais_mod = sis_editais_mod[~sis_editais_mod[\"year\"].isnull()]\n",
    "sis_editais_mod = sis_editais_mod[(sis_editais_mod[\"year\"].astype(int) > 2010) & (sis_editais_mod[\"year\"].astype(int) < 2022)].reset_index(drop=True)\n",
    "sis_editais_mod[\"year\"] = sis_editais_mod[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n",
      "50\n",
      "100\n",
      "150\n",
      "\n",
      "2020\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "\n",
      "2019\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "\n",
      "2018\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "\n",
      "2017\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "\n",
      "2016\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "\n",
      "2015\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "\n",
      "2014\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "\n",
      "2013\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "\n",
      "2012\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_matchings = []\n",
    "\n",
    "for year_sis in sis_editais_mod[\"year\"].unique():\n",
    "    print(year_sis)\n",
    "    \n",
    "    # data_dodf contém a base de DODFs do Fabrício, segmentada por ano\n",
    "    data_dodf = pd.read_parquet(\"/home/ciri/Desktop/sofia/unb/knedle/anotacoes_manuais_tcdf/dodfs_parquet/df_{}.parquet.gzip\".format(str(year_sis)))\n",
    "    data_dodf = merge_data(data_dodf)\n",
    "    \n",
    "    sis_editais_temp = sis_editais_mod[sis_editais_mod[\"year\"] == year_sis]\n",
    "    \n",
    "    full_blocks_data = get_filtered_blocks(data_dodf, patterns_clean)\n",
    "    matchings_temp = find_matching(full_blocks_data, sis_editais_temp)\n",
    "    \n",
    "    full_matchings.append([year_sis, matchings_temp])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_matchings_v2_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(full_matchings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_matchings_v2_2.pickle', 'rb') as handle:\n",
    "    full_matchings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n",
      "2020\n",
      "2019\n",
      "2018\n",
      "2017\n",
      "2016\n",
      "2015\n",
      "2014\n",
      "2013\n",
      "2012\n"
     ]
    }
   ],
   "source": [
    "matchings_year_dfs = []\n",
    "\n",
    "for matchings_year in full_matchings:\n",
    "    n_dodf = [i[0] for i in matchings_year[1]]\n",
    "    year = matchings_year[0]\n",
    "    selection_type = [i[1] for i in matchings_year[1]]\n",
    "    selection_rule = [i[2] for i in matchings_year[1]]\n",
    "    act = [i[3] for i in matchings_year[1]]\n",
    "    \n",
    "    matchings_year_df = pd.DataFrame({\n",
    "        \"n_dodf\": n_dodf,\n",
    "        \"year\": year,\n",
    "        \"selection_rule\": selection_rule,\n",
    "        \"selection_type\": selection_type,\n",
    "        \"text\": act\n",
    "    })\n",
    "    \n",
    "    matchings_year_df[\"loc_index\"] = matchings_year_df.apply(lambda row: get_index(row, sis_editais_mod), axis=1)\n",
    "    matchings_year_dfs.append(matchings_year_df)\n",
    "    \n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchings_year_total = pd.concat(matchings_year_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover casos de ambiguidade, com repetição de identificador\n",
    "matchings_year_total = matchings_year_total[matchings_year_total[\"loc_index\"].str.len() == 1].reset_index(drop=True)\n",
    "matchings_year_total[\"loc_index\"] = matchings_year_total[\"loc_index\"].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sis_editais_mod[\"loc_index\"] = sis_editais_mod.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchings_year_total_mod = matchings_year_total.merge(sis_editais_mod[[\"loc_index\", \"Tipo Objeto\", \"EspObjeto\", \"Licitante\", \"Interessado\", \"Responsável\", \"Modalidade\"]], how=\"left\", on=[\"loc_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchings_year_total_mod.to_csv(\"matchings_sis_dodf.csv\", index=False)\n",
    "sis_editais_mod.to_csv(\"sis_editais.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7168220414464699"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matchings_year_total_mod)/len(sis_editais_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
